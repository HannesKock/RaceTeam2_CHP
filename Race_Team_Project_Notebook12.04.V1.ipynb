{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannesKock/RaceTeam2_CHP/blob/main/Race_Team_Project_Notebook12.04.V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycXWv5-jYpIC"
      },
      "source": [
        "# **Race Team Project Notebook**\n",
        "Names:\n",
        "\n",
        "Matr.:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUOq9_tORt3e"
      },
      "source": [
        "Please write down the Name of the Group member you worked on each section of code. This is necessary for grading by Studienbüro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8aiLYHdaX9P"
      },
      "source": [
        "## **Analytics for Race 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuIPnDlucl2q"
      },
      "source": [
        "- For each race day built one section with the code that you built for this race\n",
        "- If you keep the same code during the next races copy the code over to the next section\n",
        "- Describe why you built your analytics the way you do and interprete after each race what went well and what you want to improve on\n",
        "- During motivation and interpretation try to cite online sources that you read to make better decisions (e.g., scientific articles on machine learning, blog posts, github pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhtO3nrmQL5S"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Paula, Cedric, Hannes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we read the CSV file\"simulator_data.csv\" into a pandas DataFrame for analysis. We displays the first few rows, lists the column names, provides summary statistics, and shows a concise overview of the dataset's structure.\n"
      ],
      "metadata": {
        "id": "1eAN599PnjCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading Data and summarizing contens\n",
        "#https://www.datacamp.com/tutorial/pandas-read-csv\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(df.describe())\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "id": "rQ78TqIW1vi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see what the Data looks like we create scatter plots showing the relationship between each parameter in the dataset and lap time."
      ],
      "metadata": {
        "id": "MBL5sfS2n9Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplots (Parameter vs. Lap Time)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "param_cols = df.columns.drop(\"Lap Time\")\n",
        "\n",
        "for col in param_cols:\n",
        "    plt.figure()\n",
        "    sns.scatterplot(x=df[col], y=df[\"Lap Time\"], alpha=0.3)\n",
        "    plt.title(f\"{col} vs. Lap Time\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "L1t83oN149ZJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We noticed that the Laps within the \"Simulator_Data.csv\" have different lengths, therefore we divide \"Lap Distance\" by \"Lap_time\" and multiply by 3600 to get \"Avg. Speed\" in km/h.\n",
        "\n",
        "Then we again generate scatter plots to visualize the relationship between each parameter (excluding \"Lap Time\" and \"Avg. Speed\") and the calculated average speed."
      ],
      "metadata": {
        "id": "Zsu4G1XLoMDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplots (Parameter vs. Avg. Speed)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Durchschnittsgeschwindigkeit berechnen (falls noch nicht vorhanden)\n",
        "if \"Avg. Speed\" not in df.columns:\n",
        "    df[\"Avg. Speed\"] = df[\"Lap Distance\"] / (df[\"Lap Time\"] / 3600)\n",
        "\n",
        "# Alle Spalten außer \"Lap Time\" und \"Avg. Speed\" verwenden\n",
        "param_cols = df.columns.drop([\"Lap Time\", \"Avg. Speed\"])\n",
        "\n",
        "# Scatterplots erzeugen\n",
        "for col in param_cols:\n",
        "    plt.figure()\n",
        "    sns.scatterplot(x=df[col], y=df[\"Avg. Speed\"], alpha=0.3)\n",
        "    plt.title(f\"{col} vs. Avg. Speed\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Avg. Speed (km/h)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "83ZAwsoP_YTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the scatter plots did not lead to much insights, we wanted to se how the different parameters correlate with each other. We hoped to be able to use logic to figure out how to set the car paramters.\n",
        "\n",
        "We calculated the correlation matrix to explore how the variables relate to each other. The correlations are visualized using a heatmap to easily identify weak or strong linear relationships between parameters."
      ],
      "metadata": {
        "id": "cmwkBmr2q5CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load your CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(\"simulator_data.csv\")  # replace with your file path\n",
        "\n",
        "# Step 2: Calculate the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Step 3: Plot the correlation matrix\n",
        "plt.figure(figsize=(20, 20))  # adjust size as needed\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, vmin=-0.05, vmax=0.05)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p2M4Y2nX41BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next four code blocks are first trys at using ML Models to predict and further understand the Parameters.\n",
        "\n",
        "These attempts did not lead to much, they are still in here because they are an important step on the way to our Final Solution for this weeks race analytics.\n",
        "\n",
        "The four Codes are:\n",
        "1. first try to use a randomForrest Model and optuna to find best car parameters\n",
        "2. To further understand the model parameters, we used a ML Model to create a SHAP-Diagram\n",
        "3. Using the Model from the SHAP-Diagram, we try to find optimal Carparameters using optuna\n",
        "4. to improve the ML Model-Parameters, we do a Grid-Search which took forever and did not lead to noticably better results."
      ],
      "metadata": {
        "id": "HPCv-8V_r0yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import optuna\n",
        "\n",
        "# === 1. Daten einlesen ===\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Ziel- und Feature-Spalten\n",
        "target_col = \"Lap Time\"\n",
        "feature_cols = df.columns.drop(target_col)\n",
        "\n",
        "# === 2. Modell trainieren ===\n",
        "X = df[feature_cols]\n",
        "y = df[target_col]\n",
        "\n",
        "# Optional: Skalieren\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Training/Test-Split (z. B. für Validierung)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Regressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# === 3. Bayesian Optimization Setup (Optuna) ===\n",
        "\n",
        "# Gegeben: Umgebungsbedingungen\n",
        "fixed_conditions = {\n",
        "    'Lap Distance': 3.7,\n",
        "    'Cornering': 6,\n",
        "    'Inclines': 20,\n",
        "    'Camber': 44,\n",
        "    'Grip': 1,\n",
        "    'Wind (Avg. Speed)': 97,\n",
        "    'Temperature': 29,\n",
        "    'Humidity': 23,\n",
        "    'Air Density': 70,\n",
        "    'Air Pressure': 98,\n",
        "    'Wind (Gusts)': 49,\n",
        "    'Altitude': 31,\n",
        "    'Roughness': 49,\n",
        "    'Width': 29\n",
        "}\n",
        "\n",
        "def objective(trial):\n",
        "    # Optimierbare Fahrzeugparameter\n",
        "    params = {\n",
        "        'Rear Wing': trial.suggest_float('Rear Wing', 0.0, 500),\n",
        "        'Engine': trial.suggest_float('Engine', 0.0, 500),\n",
        "        'Front Wing': trial.suggest_float('Front Wing', 0.0, 500),\n",
        "        'Brake Balance': trial.suggest_float('Brake Balance', 0.0, 500),\n",
        "        'Differential': trial.suggest_float('Differential', 0.0, 500),\n",
        "        'Suspension': trial.suggest_float('Suspension', 0.0, 500),\n",
        "    }\n",
        "\n",
        "    # Kombinieren mit festen Bedingungen\n",
        "    full_input = {**fixed_conditions, **params}\n",
        "    X_input = pd.DataFrame([full_input])\n",
        "    X_input_scaled = scaler.transform(X_input)\n",
        "\n",
        "    # Vorhersage durch Modell\n",
        "    lap_time = model.predict(X_input_scaled)[0]\n",
        "    return lap_time\n",
        "\n",
        "# Optuna-Studie starten\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Ergebnisse\n",
        "print(\"Beste Parameterkombination:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "print(f\"Erwartete Rundenzeit: {study.best_value:.4f} Sekunden\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a9HOY9CO106A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy  as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Falls noch nicht geschehen: Durchschnittsgeschwindigkeit berechnen\n",
        "if \"Avg. Speed\" not in df.columns:\n",
        "    df[\"Avg. Speed\"] = df[\"Lap Distance\"] / (df[\"Lap Time\"] / 3600)\n",
        "\n",
        "# 2. Features & Ziel definieren\n",
        "feature_cols = [\n",
        "    'Lap Distance', 'Cornering', 'Inclines', 'Camber', 'Grip',\n",
        "    'Wind (Avg. Speed)', 'Temperature', 'Humidity', 'Air Density',\n",
        "    'Air Pressure', 'Wind (Gusts)', 'Altitude', 'Roughness', 'Width',\n",
        "    'Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential',\n",
        "    'Suspension'\n",
        "]\n",
        "\n",
        "X = df[feature_cols]\n",
        "y = df[\"Avg. Speed\"]\n",
        "\n",
        "# 3. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# 4. Modell trainieren (XGBoost)\n",
        "#model = xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
        "model = xgb.XGBRegressor(colsample_bytree=1.0, learning_rate = 0.05, max_depth = 6, min_child_weight = 1, n_estimators = 300, subsample = 0.8)\n",
        "model.fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "error = np.mean(np.abs(y_test - y_hat))\n",
        "print(error)\n",
        "\n",
        "# # 5. SHAP-Werte berechnen\n",
        "explainer = shap.Explainer(model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# # 6. SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, X_test)\n"
      ],
      "metadata": {
        "id": "_4C7T1BhGyGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# You can get this from df.mean().values or pick a sample row\n",
        "base_input = df[feature_cols].mean().values\n",
        "\n",
        "for fc in fixed_conditions:\n",
        "    base_input[feature_cols.index(fc)] = fixed_conditions[fc]\n",
        "\n",
        "# Indices of the features we want to optimize\n",
        "optim_features = ['Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential', 'Suspension']\n",
        "optim_indices = [feature_cols.index(f) for f in optim_features]\n",
        "\n",
        "# Define bounds from your dataset (here we use min/max)\n",
        "feature_bounds = {\n",
        "    'Rear Wing': (1, 500),\n",
        "    'Engine': (1, 500),\n",
        "    'Front Wing': (1, 500),\n",
        "    'Brake Balance': (1, 500),\n",
        "    'Differential': (1, 500),\n",
        "    'Suspension': (1, 500),\n",
        "}\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective(trial):\n",
        "    x = base_input.copy()\n",
        "\n",
        "    for f in optim_features:\n",
        "        val = trial.suggest_int(f, int(feature_bounds[f][0]), int(feature_bounds[f][1]))\n",
        "        x[feature_cols.index(f)] = val\n",
        "\n",
        "    pred = model.predict(np.array([x]))[0]\n",
        "    return pred  # Optuna will maximize if we tell it to\n",
        "\n",
        "# Run Optuna study\n",
        "optuna.logging.disable_default_handler()\n",
        "study = optuna.create_study(direction='maximize')  # use 'minimize' if lower lap time is better\n",
        "study.optimize(objective, n_trials=255)\n",
        "\n",
        "\n",
        "# Show results\n",
        "print(\"Best params:\")\n",
        "for k, v in study.best_params.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "print(f\"Max predicted avg speed: {study.best_value}\")\n"
      ],
      "metadata": {
        "id": "6v_W-wPAUbrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grid Search for best Model Parameters\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Create the target variable: average speed\n",
        "df['Avg Speed'] = df['Lap Distance'] / df['Lap Time']\n",
        "\n",
        "# Drop unused columns\n",
        "X = df.drop(columns=['Lap Distance', 'Lap Time', 'Avg Speed'])\n",
        "y = df['Avg Speed']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define XGBoost Regressor\n",
        "model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define grid search parameters\n",
        "param_grid = {\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 300],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "    'min_child_weight': [1, 5]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "jJHJ_frRk-Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determining, which Parameters have the biggest impacts on Lap Times / Speeds.\n",
        "\n",
        "\n",
        "\n",
        "This code trains an XGBoost Regressor model to predict the average speed of a car based on various track and environmental features. First, it loads the data and calculates the average speed by dividing the lap distance by the lap time. The code then preprocesses the data by removing unnecessary columns and splits it into training and testing sets. After training the model, it evaluates the feature importances to determine which variables most strongly influence the predicted average speed. **Finally, it plots a bar chart to visually display the feature importances, providing insights into the relative impact of each feature.**"
      ],
      "metadata": {
        "id": "CL-Q4Cb9qo6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = df[\"Lap Distance\"] / df[\"Lap Time\"]\n",
        "\n",
        "X = df.drop(columns=[\"Lap Distance\", \"Lap Time\", \"Avg Speed\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "importances = model.feature_importances_\n",
        "feat_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "feat_importance.plot(kind=\"bar\", figsize=(12, 6), title=\"Feature Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Lgr-Ysmx0tl9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determining, which Track/Weather Parameters have the biggest impacts on each car parameter\n",
        "\n",
        "This code performs feature importance analysis for various car setup parameters based on environmental and track/weather conditions. It uses the XGBoost Regressor to model the relationship between the track/weather variables and each car setup parameter. The data is split into training and test sets, and the model is evaluated using the R² score to measure prediction accuracy. After training, the code extracts and plots the feature importances for each car parameter, helping to identify which track/weather variables have the most significant impact on each car setup."
      ],
      "metadata": {
        "id": "jhlF9wcwouWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Define inputs (track/weather features) and targets (car parameters)\n",
        "track_weather_features = [\n",
        "    'Lap Distance', 'Cornering', 'Inclines', 'Camber', 'Grip',\n",
        "    'Wind (Avg. Speed)', 'Temperature', 'Humidity', 'Air Density',\n",
        "    'Air Pressure', 'Wind (Gusts)', 'Altitude', 'Roughness', 'Width'\n",
        "]\n",
        "\n",
        "car_setup_params = ['Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential', 'Suspension']\n",
        "\n",
        "# Store feature importances for each car parameter\n",
        "feature_importance_dict = {}\n",
        "\n",
        "# Loop through car parameters\n",
        "for param in car_setup_params:\n",
        "    X = df[track_weather_features]\n",
        "    y = df[param]\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate R² score\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = r2_score(y_test, y_pred)\n",
        "    print(f\"\\n{param} - R²: {score:.3f}\")\n",
        "\n",
        "    # Get feature importances and filter by importance > 0.1\n",
        "    importances = model.feature_importances_\n",
        "    importance_series = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "    # Store importances\n",
        "    feature_importance_dict[param] = importance_series\n",
        "\n",
        "    # Display the features with importance > 0.1\n",
        "    print(f\"\\nFor {param}, track/weather parameters with importance > 0.1:\")\n",
        "    for feature, importance in importance_series.items():\n",
        "        if importance > 0.075:\n",
        "            print(f\" - {feature}: {importance:.3f}\")\n",
        "\n",
        "    # Optional: Plot feature importances\n",
        "    importance_series.plot(kind='barh', title=f\"Feature Importance for {param}\", figsize=(8, 5))\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "piDIG5ojfAUz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basiclly the same as the code before, but the other car parameters are also included in the models to determine the parameter importance\n",
        "#This is done to see, how different car parameters might influence each other\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Define inputs (track/weather features) and targets (car parameters)\n",
        "track_weather_features = [\n",
        "    'Lap Distance', 'Cornering', 'Inclines', 'Camber', 'Grip',\n",
        "    'Wind (Avg. Speed)', 'Temperature', 'Humidity', 'Air Density',\n",
        "    'Air Pressure', 'Wind (Gusts)', 'Altitude', 'Roughness', 'Width'\n",
        "]\n",
        "\n",
        "car_setup_params = ['Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential', 'Suspension']\n",
        "\n",
        "# Store feature importances for each car parameter\n",
        "feature_importance_dict = {}\n",
        "\n",
        "# Loop through car parameters\n",
        "for param in car_setup_params:\n",
        "    # Include track/weather features and other car parameters (excluding the target parameter itself)\n",
        "    other_car_params = [p for p in car_setup_params if p != param]\n",
        "    X = df[track_weather_features + other_car_params]\n",
        "    y = df[param]\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate R² score\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = r2_score(y_test, y_pred)\n",
        "    print(f\"\\n{param} - R²: {score:.3f}\")\n",
        "\n",
        "    # Get feature importances and sort them\n",
        "    importances = model.feature_importances_\n",
        "    importance_series = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "    # Store importances\n",
        "    feature_importance_dict[param] = importance_series\n",
        "\n",
        "    # Display the features with importance > 0.075\n",
        "    print(f\"\\nFor {param}, all parameters with importance > 0.075:\")\n",
        "    for feature, importance in importance_series.items():\n",
        "        if importance > 0.075:\n",
        "            print(f\" - {feature}: {importance:.3f}\")\n",
        "\n",
        "    # Optional: Plot feature importances\n",
        "    importance_series.plot(kind='barh', title=f\"Feature Importance for {param}\", figsize=(8, 5))\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "D3kuj_jRFkln",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FR-MJQxykiEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sequential optimization of car-parameters. **\n",
        "\n",
        "We optimize each Car-parameter in a predefined order, ensuring that each optimization builds upon the previous ones. (Order: Feature Importance (highest to lowest)(see above))\n",
        "\n",
        "For each car parameter, we first train a predictive model for average speed using relevant track/weather variables and already optimized parameters. Then, using Optuna, we search for the optimal value of the current car parameter that maximizes the predicted average speed.\n",
        "\n",
        "What track and wheather parameters are important for each Car-Parameter was determined before (see above)\n",
        "\n",
        "Order of optimization and relevant track/weather parameters:\n",
        "\n",
        "1. Engine: Grip, Altitude, Humidity, Air Density, Temperature, Air Pressure, Inclines\n",
        "2. Differetial: Cornering, Width, Inclines, Grip, Temprature, Air Density\n",
        "3. Rear Wing: Air Pressure, Air Density, Cornering, Inclines, Wind (Avg. Speed), Humidity, Roughness\n",
        "4. Break Balance: Width, Cornering, Roughness, Temperature\n",
        "5. Front Wing: Air Pressure, Cornering, Air Density, Inclines, Wind (Avg. Speed), Humidity, Wind (Gusts)\n",
        "6. Suspension: Grip, Inclines, Cornering, Camber, Width, Roughness\n"
      ],
      "metadata": {
        "id": "1HSUU_WOlmll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Fixed track & weather settings — user-defined!\n",
        "fixed_conditions = {\n",
        "    \"Cornering\": 6,\n",
        "    \"Inclines\": 20,\n",
        "    \"Camber\": 44,\n",
        "    \"Grip\": 1,\n",
        "    \"Altitude\": 31,\n",
        "    \"Roughness\": 49,\n",
        "    \"Width\": 29,\n",
        "    \"Temperature\": 29,\n",
        "    \"Humidity\": 23,\n",
        "    \"Wind (Avg. Speed)\": 97,\n",
        "    \"Wind (Gusts)\": 49,\n",
        "    \"Air Density\": 70,\n",
        "    \"Air Pressure\": 98,\n",
        "}\n",
        "\n",
        "# Order of optimization and relevant features\n",
        "optimization_order = [\n",
        "    (\"Engine\", [\"Grip\", \"Altitude\", \"Humidity\", \"Air Density\", \"Temperature\", \"Air Pressure\", \"Inclines\"]),\n",
        "    (\"Differential\", [\"Cornering\", \"Width\", \"Inclines\", \"Grip\", \"Temperature\", \"Air Density\"]),\n",
        "    (\"Rear Wing\", [\"Air Pressure\", \"Air Density\", \"Cornering\", \"Inclines\", \"Wind (Avg. Speed)\", \"Humidity\", \"Roughness\"]),\n",
        "    (\"Brake Balance\", [\"Width\", \"Cornering\", \"Roughness\", \"Temperature\"]),\n",
        "    (\"Front Wing\", [\"Air Pressure\", \"Cornering\", \"Air Density\", \"Inclines\", \"Wind (Avg. Speed)\", \"Humidity\", \"Wind (Gusts)\"]),\n",
        "    (\"Suspension\", [\"Grip\", \"Inclines\", \"Cornering\", \"Camber\", \"Width\", \"Roughness\"]),\n",
        "]\n",
        "\n",
        "# Storage for optimized parameters\n",
        "optimized_params = {}\n",
        "\n",
        "\n",
        "# Optimization loop\n",
        "for param, relevant_features in optimization_order:\n",
        "    print(f\"\\n Optimizing {param}...\")\n",
        "\n",
        "    # Features for model = track/weather + already optimized + current param\n",
        "    model_features = relevant_features + list(optimized_params.keys()) + [param]\n",
        "    model_df = df[model_features + [\"Avg Speed\"]].dropna()\n",
        "\n",
        "    X = model_df[model_features]\n",
        "    y = model_df[\"Avg Speed\"]\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Prepare fixed input for this stage\n",
        "    input_row = {f: fixed_conditions[f] for f in relevant_features}\n",
        "    input_row.update(optimized_params)  # Include already optimized parameters\n",
        "\n",
        "    def objective(trial):\n",
        "        trial_value = trial.suggest_int(param, 1, 500)\n",
        "        row = input_row.copy()\n",
        "        row[param] = trial_value\n",
        "        df_input = pd.DataFrame([row])\n",
        "        pred = model.predict(df_input)[0]\n",
        "        return pred  # Maximizing Avg Speed\n",
        "\n",
        "    optuna.logging.disable_default_handler()\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    print(f\"Max predicted avg speed: {study.best_value}\")\n",
        "\n",
        "    best_val = study.best_params[param]\n",
        "    optimized_params[param] = best_val\n",
        "\n",
        "    print(f\" Best {param}: {best_val}\")\n",
        "\n",
        "# Final output\n",
        "print(\"\\n All optimized parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "Lu2xXtIKRJMF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "All optimized parameters:\n",
        "Engine: 30\n",
        "Differential: 1\n",
        "Rear Wing: 48\n",
        "Brake Balance: 3\n",
        "Front Wing: 37\n",
        "Suspension: 124"
      ],
      "metadata": {
        "id": "-U8GQaEz_Gao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Same code as above, but the optimization order of the Car-Parameters is flipped.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Fixed track & weather settings — user-defined!\n",
        "fixed_conditions = {\n",
        "    \"Cornering\": 6,\n",
        "    \"Inclines\": 20,\n",
        "    \"Camber\": 44,\n",
        "    \"Grip\": 1,\n",
        "    \"Altitude\": 31,\n",
        "    \"Roughness\": 49,\n",
        "    \"Width\": 29,\n",
        "    \"Temperature\": 29,\n",
        "    \"Humidity\": 23,\n",
        "    \"Wind (Avg. Speed)\": 97,\n",
        "    \"Wind (Gusts)\": 49,\n",
        "    \"Air Density\": 70,\n",
        "    \"Air Pressure\": 98,\n",
        "}\n",
        "\n",
        "# Order of optimization and relevant features\n",
        "optimization_order = [\n",
        "    (\"Suspension\", [\"Grip\", \"Inclines\", \"Cornering\", \"Camber\", \"Width\", \"Roughness\"]),\n",
        "    (\"Front Wing\", [\"Air Pressure\", \"Cornering\", \"Air Density\", \"Inclines\", \"Wind (Avg. Speed)\", \"Humidity\", \"Wind (Gusts)\"]),\n",
        "    (\"Brake Balance\", [\"Width\", \"Cornering\", \"Roughness\", \"Temperature\"]),\n",
        "    (\"Rear Wing\", [\"Air Pressure\", \"Air Density\", \"Cornering\", \"Inclines\", \"Wind (Avg. Speed)\", \"Humidity\", \"Roughness\"]),\n",
        "    (\"Differential\", [\"Cornering\", \"Width\", \"Inclines\", \"Grip\", \"Temperature\", \"Air Density\"]),\n",
        "    (\"Engine\", [\"Grip\", \"Altitude\", \"Humidity\", \"Air Density\", \"Temperature\", \"Air Pressure\", \"Inclines\"])\n",
        "]\n",
        "\n",
        "# Storage for optimized parameters\n",
        "optimized_params = {}\n",
        "\n",
        "\n",
        "# Optimization loop\n",
        "for param, relevant_features in optimization_order:\n",
        "    print(f\"\\n Optimizing {param}...\")\n",
        "\n",
        "    # Features for model = track/weather + already optimized + current param\n",
        "    model_features = relevant_features + list(optimized_params.keys()) + [param]\n",
        "    model_df = df[model_features + [\"Avg Speed\"]].dropna()\n",
        "\n",
        "    X = model_df[model_features]\n",
        "    y = model_df[\"Avg Speed\"]\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Prepare fixed input for this stage\n",
        "    input_row = {f: fixed_conditions[f] for f in relevant_features}\n",
        "    input_row.update(optimized_params)  # Include already optimized parameters\n",
        "\n",
        "    def objective(trial):\n",
        "        trial_value = trial.suggest_int(param, 1, 500)\n",
        "        row = input_row.copy()\n",
        "        row[param] = trial_value\n",
        "        df_input = pd.DataFrame([row])\n",
        "        pred = model.predict(df_input)[0]\n",
        "        return pred  # Maximizing Avg Speed\n",
        "\n",
        "    #optuna.logging.disable_default_handler()\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=1000)\n",
        "    print(f\"Max predicted avg speed: {study.best_value}\")\n",
        "\n",
        "    best_val = study.best_params[param]\n",
        "    optimized_params[param] = best_val\n",
        "\n",
        "    print(f\" Best {param}: {best_val}\")\n",
        "\n",
        "# Final output\n",
        "print(\"\\n All optimized parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "MnVIytdH9tnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Race Strategy"
      ],
      "metadata": {
        "id": "br4U1GyguFO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the tire parameters and their lap time formulas\n",
        "def lap_time_super_soft(X):\n",
        "    return (69.269-0.141812499999999) + 0.141812499999999 * X\n",
        "\n",
        "def lap_time_soft(X):\n",
        "    return (70.53-0.0667741935483869) + 0.0667741935483869 * X\n",
        "\n",
        "def lap_time_medium(X):\n",
        "    return (70.927-0.0670857142857143) + 0.0670857142857143 * X\n",
        "\n",
        "def lap_time_hard(X):\n",
        "    return (70.257-0.109609756097561) + 0.109609756097561 * X\n",
        "\n",
        "# Tire data with their lifespan\n",
        "tire_lifespan = {\n",
        "    \"super_soft\": 16,\n",
        "    \"soft\": 30,\n",
        "    \"medium\": 35,\n",
        "    \"hard\": 41\n",
        "}\n",
        "\n",
        "# Pit stop penalty\n",
        "pit_stop_time = 30  # seconds\n",
        "\n",
        "# Function to calculate the total race time for a given strategy\n",
        "def calculate_race_time(laps, strategy):\n",
        "    total_time = 0\n",
        "    total_pit_stops = 0\n",
        "    lap_index = 0\n",
        "    lap_counter = 0\n",
        "\n",
        "    while lap_counter < laps:\n",
        "        tire, stint_laps = strategy[lap_index]\n",
        "\n",
        "        # Ensure we don't exceed the total laps\n",
        "        if lap_counter + stint_laps > laps:\n",
        "            stint_laps = laps - lap_counter\n",
        "\n",
        "        # Calculate the lap times for this stint\n",
        "        lap_times = []\n",
        "        for i in range(stint_laps):\n",
        "            if tire == \"super_soft\":\n",
        "                lap_times.append(lap_time_super_soft(i + 1))\n",
        "            elif tire == \"soft\":\n",
        "                lap_times.append(lap_time_soft(i + 1))\n",
        "            elif tire == \"medium\":\n",
        "                lap_times.append(lap_time_medium(i + 1))\n",
        "            elif tire == \"hard\":\n",
        "                lap_times.append(lap_time_hard(i + 1))\n",
        "\n",
        "        total_time += sum(lap_times)  # Add the lap times of this stint\n",
        "        lap_counter += stint_laps\n",
        "\n",
        "        # If we are not at the last stint, account for a pit stop\n",
        "        if lap_counter < laps:\n",
        "            total_time += pit_stop_time  # Pit stop penalty\n",
        "            total_pit_stops += 1\n",
        "\n",
        "        lap_index += 1\n",
        "        if lap_index >= len(strategy):\n",
        "            break\n",
        "\n",
        "    return total_time, total_pit_stops\n",
        "\n",
        "# Function to generate possible strategies dynamically\n",
        "def generate_strategies(laps):\n",
        "    strategies = []\n",
        "    tire_choices = [\"super_soft\", \"soft\", \"medium\", \"hard\"]\n",
        "\n",
        "    # Generate strategies by breaking the laps into multiple stints\n",
        "    for tire1 in tire_choices:\n",
        "        for tire2 in tire_choices:\n",
        "            for tire3 in tire_choices:\n",
        "                #for tire4 in tire_choices:\n",
        "                  #for tire5 in tire_choices:\n",
        "                    strategy = []\n",
        "                    remaining_laps = laps\n",
        "\n",
        "                    # Create dynamic stints for each tire\n",
        "                    #for tire in [tire1, tire2, tire3, tire4, tire5]:\n",
        "                    #for tire in [tire1, tire2, tire3, tire4]:\n",
        "                    for tire in [tire1, tire2, tire3]:\n",
        "                    #for tire in [tire1, tire2]:\n",
        "                        stint_laps = tire_lifespan[tire]\n",
        "\n",
        "                        if remaining_laps > stint_laps:\n",
        "                            strategy.append((tire, stint_laps))\n",
        "                            remaining_laps -= stint_laps\n",
        "                        else:\n",
        "                            strategy.append((tire, remaining_laps))\n",
        "                            break\n",
        "\n",
        "                    if sum([stint[1] for stint in strategy]) == laps:\n",
        "                        strategies.append(strategy)\n",
        "\n",
        "    return strategies\n",
        "\n",
        "# Function to find the best strategy\n",
        "def optimize_strategy(laps):\n",
        "    best_time = math.inf\n",
        "    best_strategy = None\n",
        "\n",
        "    strategies = generate_strategies(laps)\n",
        "\n",
        "    for strategy in strategies:\n",
        "        total_time, pit_stops = calculate_race_time(laps, strategy)\n",
        "        if total_time < best_time:\n",
        "            best_time = total_time\n",
        "            best_strategy = strategy\n",
        "            best_pit_stops = pit_stops\n",
        "\n",
        "    return best_strategy, best_time, best_pit_stops\n",
        "\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    race_laps = 83\n",
        "    best_strategy, best_time, total_pit_stops = optimize_strategy(race_laps)\n",
        "    print(f\"Best Strategy: {best_strategy}\")\n",
        "    print(f\"Best Total Time: {best_time} seconds\")\n",
        "    print(f\"Total Pit Stops: {total_pit_stops}\")\n"
      ],
      "metadata": {
        "id": "VQEz50zWfvhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best Strategy: [('super_soft', 16), ('super_soft', 16), ('super_soft', 16), ('super_soft', 16), ('soft', 19)]\n",
        "Best Total Time: 5972.774387096774 seconds\n",
        "\n",
        "Best Strategy: [('super_soft', 16), ('super_soft', 16), ('soft', 30), ('soft', 21)]\n",
        "Best Total Time: 5980.742354838709 seconds\n",
        "\n",
        "Best Strategy: [('soft', 30), ('soft', 30), ('soft', 23)]\n",
        "Best Total Time: 5988.977419354838 seconds"
      ],
      "metadata": {
        "id": "jMfNBV63wjmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Car-Parameters, to see, if optimized values are actually better than other values."
      ],
      "metadata": {
        "id": "TqLe2liv8T-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "print(f'Rear Wing {randint(1, 500)}')\n",
        "print(f'Engine {randint(1, 500)}')\n",
        "print(f'Front Wing {randint(1, 500)}')\n",
        "print(f'Brakebalance {randint(1, 500)}')\n",
        "print(f'Differential {randint(1, 500)}')\n",
        "print(f'Suspension {randint(1, 500)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "WMpm0v-d5b_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1LiOt7yQL5T"
      },
      "source": [
        "## **Analytics for Race 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZcWkDUrQL5T"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Names.."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "sSk6H0gbDmEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    X = df.drop(columns=['Lap Distance', 'Lap Time', 'Avg Speed'])\n",
        "    y = df[\"Avg Speed\"]\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=1000,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        #rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gfu5XKfCDNOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Define X and y\n",
        "X = df.drop(columns=['Lap Distance', 'Lap Time', 'Avg Speed'])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Set the hyperparameters\n",
        "params = {\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "train_data = lgb.Dataset(X, label=y)\n",
        "model = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=100)\n",
        "\n",
        "# Get feature importance by 'split' and 'gain'\n",
        "importance_split = model.feature_importance(importance_type='split')\n",
        "importance_gain = model.feature_importance(importance_type='gain')\n",
        "\n",
        "# Sort feature importance by 'split'\n",
        "sorted_split_idx = importance_split.argsort()[::1]\n",
        "sorted_split_importance = importance_split[sorted_split_idx]\n",
        "sorted_split_features = X.columns[sorted_split_idx]\n",
        "\n",
        "# Sort feature importance by 'gain'\n",
        "sorted_gain_idx = importance_gain.argsort()[::1]\n",
        "sorted_gain_importance = importance_gain[sorted_gain_idx]\n",
        "sorted_gain_features = X.columns[sorted_gain_idx]\n",
        "\n",
        "# Plot feature importance by 'split'\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(sorted_split_features, sorted_split_importance , color='red')\n",
        "plt.title('Feature Importance by Split')\n",
        "plt.xlabel('Number of Splits')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importance by 'gain'\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(sorted_gain_features, sorted_gain_importance, color='blue')\n",
        "plt.title('Feature Importance by Gain')\n",
        "plt.xlabel('Gain')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "msqqrwxsOWzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZneIB62QL5T"
      },
      "source": [
        "## **Analytics for Race 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdHKUysbQL5T"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Names.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFETMTcZQL5T"
      },
      "source": [
        "## **Analytics for Race 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS0Om6gBQL5T"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Names.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ajs8np_QL5T"
      },
      "source": [
        "## **Analytics for Race 5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfLSArfnQL5T"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Names.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFvVYCufQL5T"
      },
      "source": [
        "## **Debrief Race Calendar before Final Race**\n",
        "\n",
        "*Write a longer text (200-500 words) reflecting on what were the main ideas you started the seminar with, how you improved your models to achieve better performance and what strategy and analytics you want to use for your final race during seminar day*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1t67p2kQL5T"
      },
      "source": [
        "## **Analytics for Final Race**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVrrOFguQL5T"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Names.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kulv8cpKcq9_"
      },
      "source": [
        "## **References**\n",
        "- Cite all references you need according to chair guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OLrMDxWc8oM"
      },
      "source": [
        "Liu, Xuan; Shi, Savannah Wei; Teixeira, Thales; Wedel, Michel (2018): Video Content Marketing: The Making of Clips, Journal of Marketing, Vol. 82, 86-101."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}