{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannesKock/RaceTeam2_CHP/blob/main/Race_Team_Project_Notebook_12-06_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycXWv5-jYpIC"
      },
      "source": [
        "# **Race Team Project Notebook**\n",
        "Names: Paula Kussauer, Cedric Schwandt, Hannes Kock\n",
        "\n",
        "Matr.: Eintragen, 7410658, 7421447"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUOq9_tORt3e"
      },
      "source": [
        "Please write down the Name of the Group member you worked on each section of code. This is necessary for grading by Studienbüro."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preface**"
      ],
      "metadata": {
        "id": "qFTxSBPhbMXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following texts and code snippets relate to our work for the seminar: \"Driven by Data: Leveraging Artificial Intelligence in NOVA Business School’s Racing Simulation.\"\n",
        "\n",
        "At the beginning of each Analytics section, we provided an overview of our motivations, goals, and procedures for that particular week. The corresponding code used to implement the described activities can be found within the respective structure.\n",
        "\n",
        "Throughout the texts and within comments in the code, we distinguish parameters into two categories: Car Parameters and Track/Weather Parameters.  \n",
        "Car Parameters include: Rear Wing, Front Wing, Engine, Differential, Brake Balance, and Suspension.  \n",
        "Track/Weather Parameters include: Cornering, Inclines, Camber, Grip, Wind (Average Speed), Temperature, Humidity, Air Density, Air Pressure, Wind (Gusts), Altitude, Roughness, and Width.\n",
        "\n",
        "Early in the analysis for Race 1, we observed that it is more effective to use the average speed as a target variable rather than lap time. We compute the average speed in km/h by dividing the lap distance by the lap time, then multiplying the result by 3,600. Although this approach is not explicitly mentioned in the texts, it is consistently applied throughout the analysis.\n",
        "\n",
        "Our optimization approaches can be categorized into two types: \"All-In-One Optimization\" and \"Sequential Optimization.\"\n",
        "\n",
        "- \"All-In-One Optimization\" refers to an approach in which all six car-parameters are optimized simultaneously using a single machine learning (ML) model.\n",
        "- \"Sequential Optimization\" refers to an approach where the car-parameters are optimized one after another, potentially using a single or multiple ML models.\n",
        "\n",
        "These two approaches are discussed in more detail within the respective sections. After their initial explanation, they are not reiterated each time they are referenced.\n",
        "\n",
        "Within the code, various types of datasets are used. Files that include the term \"newnames\" in their filenames indicate that the column names have been modified to ensure compatibility with our code. Specifically, the column names were changed to be consistent throughout the files and to eliminate blank spaces within the car-parameter columns.\n",
        "\n",
        "Additionally, the Practice_Data files are labeled with identifiers such as \"V3_2.\" The \"V3\" indicates that the data pertains to Race 3, and the \"2\" signifies that it is the second practice data file exported for that race.\n",
        "\n",
        "Sources are cited at the beginning of code cells whenever new and relevant components, such as libraries, code logic, or concepts are introduced. After that, the contents and logic are assumed to be understood.\n",
        "\n",
        "Whenever code utilizes Optuna or Selenium, the corresponding library needs to be installed once per Google Colab runtime. This can be done by running the following commands in a code cell:\n",
        "\n",
        "!pip install optuna\n",
        "\n",
        "!pip install selenium\n",
        "\n",
        "\n",
        "For code cleanliness, these installation cells are included only once, immediately before the section “Analytics for Race 1.”\n",
        "\n",
        "At certain points in our code, it is required to execute previous code cells to train and save a model, or to generate a merged dataset.\n"
      ],
      "metadata": {
        "id": "tPLidxPcbS3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install selenium"
      ],
      "metadata": {
        "id": "Jc0ssAMEuYH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8aiLYHdaX9P"
      },
      "source": [
        "# **Analytics for Race 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The activities from week one can be divided into four main categories:\n",
        "\n",
        "1. Data Visualization and Understanding the Simulation  \n",
        "\n",
        "\n",
        "2. Initial Machine Learning Experiments  \n",
        "Next, we made our first attempts at training and applying machine learning (ML) models. These models helped us visualize the relationships in the data more clearly and served as preliminary steps toward optimizing car parameters.\n",
        "\n",
        "3. Exploring Sequential Optimization  \n",
        "\n",
        "\n",
        "4. Developing a Better Strategy Optimization Tool  \n",
        "Finally, we realized that the existing Excel-based \"Race Simulator\" was insufficient for strategic optimization. Therefore, we developed a new code that optimizes strategies based on tire endurance, total laps, and a linear relationship between lap time and fuel consumption. The code evaluates all possible strategies, calculates their resulting lap times, and identifies the best one.\n",
        "\n",
        "**Data Visualization**\n",
        "\n",
        "We began by analyzing the dataset to gain insights into the simulation environment. First, we read the CSV file \"simulator_data.csv\" into a pandas DataFrame. We displayed the first few rows, listed the column names, provided summary statistics, and gave a concise overview of the dataset's structure.  \n",
        "\n",
        "To better understand the data, we created scatter plots to visualize the relationship between each parameter and lap time. During this process, we noticed that the lap distances in \"simulator_data.csv\" varied. To account for this, we calculated the average speed by dividing \"Lap Distance\" by \"Lap Time\" and multiplying by 3600 to convert it to km/h.\n",
        "\n",
        "Next, we generated scatter plots to examine the relationship between each parameter (excluding \"Lap Time\" and \"Avg. Speed\") and the calculated average speed.  \n",
        "\n",
        "Since these scatter plots did not provide many clear insights, we decided to explore the correlations among the parameters. We computed the correlation matrix and visualized it with a heatmap, which allowed us to easily identify the strength of linear relationships between variables. The analysis revealed that the simulation does not appear to be based on real physics—for example, parameters like air density and air pressure showed little to no correlation.\n",
        "\n",
        "\n",
        "**Sequential Optimization Process**\n",
        "\n",
        "After reading the Gordy interview on the Team-Analytics website, we were inspired by his statement: **\"We always optimized each decision separately, ensuring that each part of the car was at its best without unnecessary compromises.\"** This motivated us to try optimizing each car parameter individually, focusing only on the parameters relevant to each specific aspect. This marked the beginning of our pursuit of sequential optimization.\n",
        "\n",
        "1. Feature Importance Analysis for Overall Influence  \n",
        "A model using an XGBoost Regressor is trained to predict the average speed based on various environmental and track features. After training, it evaluates feature importances and plots a bar chart illustrating the relative impact of each feature. This helps us understand which variables most strongly influence the vehicle's average speed.\n",
        "\n",
        "2. Feature Importance Analysis for Car Setup Parameters  \n",
        "Next, we perform a similar analysis for each individual car setup parameter. Using an XGBoost Regressor, it models the relationship between track/weather parameters and each car parameter. The feature importances are extracted and visualized as bar charts. These insights help identify which track and weather variables most significantly affect each setup parameter.\n",
        "\n",
        "3. Sequential Optimization of Car Parameters  \n",
        "\n",
        "- Order of Optimization:  \n",
        "  Based on the feature importance analyses, we predefined an order for optimizing the car parameters, from most to least influential:\n",
        "  \n",
        "  - Engine: Grip, Altitude, Humidity, Air Density, Temperature, Air Pressure, Inclines\n",
        "  - Differential: Cornering, Width, Inclines, Grip, Temperature, Air Density\n",
        "  - Rear Wing: Air Pressure, Air Density, Cornering, Inclines, Wind (Average Speed), Humidity, Roughness\n",
        "  - Brake Balance: Width, Cornering, Roughness, Temperature\n",
        "  - Front Wing: Air Pressure, Cornering, Air Density, Inclines, Wind (Average Speed), Humidity, Wind (Gusts)\n",
        "  - Suspension: Grip, Inclines, Cornering, Camber, Width, Roughness  \n",
        "\n",
        "- Inclusion of Previously Optimized Parameters:  \n",
        "  In each step of the optimization, we include all parameters that have already been optimized in previous steps. This ensures that the models account for their influence when predicting the impact of the current parameter.\n",
        "\n",
        "- Optimization Procedure:  \n",
        "  For each car parameter:\n",
        "  1. We train a predictive model (using an XGBoost Regressor) to estimate the average speed, utilizing relevant track/weather variables and previously optimized parameters.\n",
        "  2. Using Optuna, we search for the optimal value of the current parameter to maximize the predicted average speed.\n",
        "\n",
        "- Testing Different Optimization Orders:  \n",
        "  We also experimented with reversing the order—starting from the features with the highest importance to the lowest—but this did not improve the results. The original highest-to-lowest order proved to be more effective.\n",
        "\n",
        "\n",
        "\n",
        "**Race Strategy**  \n",
        "After working with the Racing_Simulation Excel for a while, we realized that there must be a better way to optimize the strategy, as the Excel approach seemed somewhat imprecise. We thought about this for some time and decided that the most straightforward solution would be to brute-force simulate or calculate every possible strategy and then select the best one.  \n",
        "\n",
        "To do this, we needed the following data:  \n",
        "- Total number of race laps  \n",
        "- Tire durability (number of laps each tire can last)  \n",
        "- Pit stop time (duration of each pit stop)  \n",
        "- The relationship between the amount of fuel in the car and lap times (which we assumed to be linear)  \n",
        "\n",
        "We also assumed that tire condition has no effect on lap times. We made this assumption because there was no way to test or accurately model this before the first race. After the race, we confirmed this assumption by comparing race data with practice data.\n",
        "\n",
        "In summary, the strategy optimizer systematically explores every feasible combination of tire, fuel, and pit strategies by modeling how fuel load impacts lap times, simulating each scenario, and selecting the most efficient plan. This comprehensive brute-force approach ensures that the optimal strategy is identified based on the parameters and assumptions we have established.\n",
        "\n",
        "This provides a more precise description of how the process works:\n",
        "\n",
        "\n",
        "1. Modeling Fuel Load and Its Effect on Lap Times:\n",
        "  - The relationship between fuel in the car and lap times was established through practice laps.  \n",
        "  - During these practice runs, data was collected with both minimal and maximal fuel loads:  \n",
        "    - The minimal fuel load (corresponding to the start of a stint) produces the fastest lap times.  \n",
        "    - The maximal fuel load (the highest amount of fuel needed to complete the entire stint or race) results in the slowest lap times.  \n",
        "  - The lap times from these runs were used to fit linear functions for each tire compound, where:  \n",
        "    - The constant term (intercept) represents the lap time when the car has minimal fuel.  \n",
        "    - The variable coefficient reflects how lap times increase proportionally with the amount of fuel carried.  \n",
        "    - If `x = 1`, the function estimates the lap time when the car has one lap of fuel remaining; if `x = 30`, it corresponds to the lap time when the car has 30 laps of fuel remaining.  \n",
        "    - By summing the estimated lap times for `x` values from 1 to the total number of laps in a stint, the total time for that stint can be approximated.  \n",
        "\n",
        "2. Generating Feasible Strategies:\n",
        "   - The algorithm considers all possible ways to split the total race laps into multiple stints, each with a specific fuel load and tire choice.\n",
        "   - Each strategy consists of a sequence of stints, with each stint:\n",
        "     - Using a specific tire compound (soft, medium, or hard).\n",
        "     - Running for a number of laps determined based on tire lifespan and strategic considerations.\n",
        "   - Constraints ensure no stint exceeds the maximum number of laps the tire can sustain (its durability), and the total combined laps cover the entire race.\n",
        "\n",
        "3. Simulating Each Strategy:\n",
        "   - For every candidate strategy, the code simulates the race by:\n",
        "     - Calculating lap times for each stint using the fuel load linear functions, reflecting the impact of fuel on speed.\n",
        "     - Summing all laps within each stint to get the total time for that segment.\n",
        "     - Adding pit stop times (penalties) between stints, except after the last to reflect real race conditions.\n",
        "   - This process estimates the total race duration associated with each specific combination of tires and fuel loads.\n",
        "\n",
        "4. Evaluating and Selecting the Optimal Strategy:\n",
        "   - After simulating all possible strategies, the code compares their total race times.\n",
        "   - It identifies the strategy with the shortest overall race time as the optimal plan.\n",
        "   - The output includes:\n",
        "     - The sequence of tire choices and stint lengths.\n",
        "     - The planned pit stops.\n",
        "     - The expected total race time.\n"
      ],
      "metadata": {
        "id": "MmYfuPLZp2Tl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhtO3nrmQL5S"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Paula Kussauer, Cedric Schwandt, Hannes Kock"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Data"
      ],
      "metadata": {
        "id": "zjq1CbI1o5Wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we read the CSV file\"simulator_data.csv\" into a pandas DataFrame for analysis. We displays the first few rows, lists the column names, provides summary statistics, and shows a concise overview of the dataset's structure.\n"
      ],
      "metadata": {
        "id": "1eAN599PnjCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading Data and summarizing contens\n",
        "#https://www.datacamp.com/tutorial/pandas-read-csv\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(df.describe())\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "id": "rQ78TqIW1vi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see what the Data looks like we create scatter plots showing the\n",
        "relationship between each parameter in the dataset and lap time."
      ],
      "metadata": {
        "id": "MBL5sfS2n9Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplots (Parameter vs. Lap Time)\n",
        "#https://www.w3schools.com/python/python_ml_scatterplot.asp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "param_cols = df.columns.drop(\"Lap Time\")\n",
        "\n",
        "for col in param_cols:\n",
        "    plt.figure()\n",
        "    sns.scatterplot(x=df[col], y=df[\"Lap Time\"], alpha=0.3)\n",
        "    plt.title(f\"{col} vs. Lap Time\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "L1t83oN149ZJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We noticed that the Laps within the \"Simulator_Data.csv\" have different lengths, therefore we divide \"Lap Distance\" by \"Lap_time\" and multiply by 3600 to get \"Avg. Speed\" in km/h.\n",
        "\n",
        "Then we again generate scatter plots to visualize the relationship between each parameter (excluding \"Lap Time\" and \"Avg. Speed\") and the calculated average speed."
      ],
      "metadata": {
        "id": "Zsu4G1XLoMDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplots (Parameter vs. Avg. Speed)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# calculating avg. speed (falls noch nicht vorhanden)\n",
        "if \"Avg. Speed\" not in df.columns:\n",
        "    df[\"Avg. Speed\"] = df[\"Lap Distance\"] / (df[\"Lap Time\"] / 3600)\n",
        "\n",
        "# Drop coulums exept \"Lap Time\" and \"Avg. Speed\"\n",
        "param_cols = df.columns.drop([\"Lap Time\", \"Avg. Speed\"])\n",
        "\n",
        "# create scatterplots.\n",
        "for col in param_cols:\n",
        "    plt.figure()\n",
        "    sns.scatterplot(x=df[col], y=df[\"Avg. Speed\"], alpha=0.3)\n",
        "    plt.title(f\"{col} vs. Avg. Speed\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Avg. Speed (km/h)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "83ZAwsoP_YTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the scatter plots did not lead to much insights, we wanted to see how the different parameters correlate with each other. We hoped to be able to use logic to figure out how to set the car paramters.\n",
        "\n",
        "We calculated the correlation matrix to explore how the variables relate to each other. The correlations are visualized using a heatmap to easily identify weak or strong linear relationships between parameters."
      ],
      "metadata": {
        "id": "cmwkBmr2q5CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.geeksforgeeks.org/how-to-create-a-seaborn-correlation-heatmap-in-python/\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load your CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(\"simulator_data.csv\")  # replace with your file path\n",
        "\n",
        "# Step 2: Calculate the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Step 3: Plot the correlation matrix\n",
        "plt.figure(figsize=(20, 20))  # adjust size as needed\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, vmin=-0.05, vmax=0.05)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20, 20))  # adjust size as needed\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, vmin=-0.5, vmax=0.5)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2M4Y2nX41BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Tries at using ML Models"
      ],
      "metadata": {
        "id": "Rl07Ob4mpJhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next four code blocks are first trys at using ML Models to predict and further understand the Parameters.\n",
        "\n",
        "These attempts did not lead to much, they are still in here because they are an important step on the way to our Final Solution for this weeks race analytics.\n",
        "\n",
        "The four Codes are:\n",
        "1. first try to use a randomForrest Model and optuna to find best car parameters\n",
        "2. To further understand the model parameters, we used a ML Model to create a SHAP-Diagram\n",
        "3. Using the Model from the SHAP-Diagram, we try to find optimal Carparameters using optuna\n",
        "4. to improve the ML Model-Parameters, we do a Grid-Search which took forever and did not lead to noticably better results."
      ],
      "metadata": {
        "id": "HPCv-8V_r0yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import optuna\n",
        "\n",
        "# === 1. Daten einlesen ===\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Ziel- und Feature-Spalten\n",
        "target_col = \"Lap Time\"\n",
        "feature_cols = df.columns.drop(target_col)\n",
        "\n",
        "# === 2. Modell trainieren ===\n",
        "X = df[feature_cols]\n",
        "y = df[target_col]\n",
        "\n",
        "# Optional: Skalieren\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Training/Test-Split (z. B. für Validierung)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Regressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# === 3. Bayesian Optimization Setup (Optuna) ===\n",
        "\n",
        "# Gegeben: Umgebungsbedingungen\n",
        "fixed_conditions = {\n",
        "    'Lap Distance': 3.7,\n",
        "    'Cornering': 6,\n",
        "    'Inclines': 20,\n",
        "    'Camber': 44,\n",
        "    'Grip': 1,\n",
        "    'Wind (Avg. Speed)': 97,\n",
        "    'Temperature': 29,\n",
        "    'Humidity': 23,\n",
        "    'Air Density': 70,\n",
        "    'Air Pressure': 98,\n",
        "    'Wind (Gusts)': 49,\n",
        "    'Altitude': 31,\n",
        "    'Roughness': 49,\n",
        "    'Width': 29\n",
        "}\n",
        "\n",
        "def objective(trial):\n",
        "    # Optimierbare Fahrzeugparameter\n",
        "    params = {\n",
        "        'Rear Wing': trial.suggest_float('Rear Wing', 0.0, 500),\n",
        "        'Engine': trial.suggest_float('Engine', 0.0, 500),\n",
        "        'Front Wing': trial.suggest_float('Front Wing', 0.0, 500),\n",
        "        'Brake Balance': trial.suggest_float('Brake Balance', 0.0, 500),\n",
        "        'Differential': trial.suggest_float('Differential', 0.0, 500),\n",
        "        'Suspension': trial.suggest_float('Suspension', 0.0, 500),\n",
        "    }\n",
        "\n",
        "    # Kombinieren mit festen Bedingungen\n",
        "    full_input = {**fixed_conditions, **params}\n",
        "    X_input = pd.DataFrame([full_input])\n",
        "    X_input_scaled = scaler.transform(X_input)\n",
        "\n",
        "    # Vorhersage durch Modell\n",
        "    lap_time = model.predict(X_input_scaled)[0]\n",
        "    return lap_time\n",
        "\n",
        "# Optuna-Studie starten\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Ergebnisse\n",
        "print(\"Beste Parameterkombination:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "print(f\"Erwartete Rundenzeit: {study.best_value:.4f} Sekunden\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a9HOY9CO106A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy  as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Falls noch nicht geschehen: Durchschnittsgeschwindigkeit berechnen\n",
        "if \"Avg. Speed\" not in df.columns:\n",
        "    df[\"Avg. Speed\"] = df[\"Lap Distance\"] / (df[\"Lap Time\"] / 3600)\n",
        "\n",
        "# 2. Features & Ziel definieren\n",
        "feature_cols = [\n",
        "    'Lap Distance', 'Cornering', 'Inclines', 'Camber', 'Grip',\n",
        "    'Wind (Avg. Speed)', 'Temperature', 'Humidity', 'Air Density',\n",
        "    'Air Pressure', 'Wind (Gusts)', 'Altitude', 'Roughness', 'Width',\n",
        "    'Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential',\n",
        "    'Suspension'\n",
        "]\n",
        "\n",
        "X = df[feature_cols]\n",
        "y = df[\"Avg. Speed\"]\n",
        "\n",
        "# 3. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# 4. Modell trainieren (XGBoost)\n",
        "#model = xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
        "model = xgb.XGBRegressor(colsample_bytree=1.0, learning_rate = 0.05, max_depth = 6, min_child_weight = 1, n_estimators = 300, subsample = 0.8)\n",
        "model.fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "error = np.mean(np.abs(y_test - y_hat))\n",
        "print(error)\n",
        "\n",
        "# # 5. SHAP-Werte berechnen\n",
        "explainer = shap.Explainer(model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# # 6. SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, X_test)\n"
      ],
      "metadata": {
        "id": "_4C7T1BhGyGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# You can get this from df.mean().values or pick a sample row\n",
        "base_input = df[feature_cols].mean().values\n",
        "\n",
        "for fc in fixed_conditions:\n",
        "    base_input[feature_cols.index(fc)] = fixed_conditions[fc]\n",
        "\n",
        "# Indices of the features we want to optimize\n",
        "optim_features = ['Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential', 'Suspension']\n",
        "optim_indices = [feature_cols.index(f) for f in optim_features]\n",
        "\n",
        "# Define bounds from your dataset (here we use min/max)\n",
        "feature_bounds = {\n",
        "    'Rear Wing': (1, 500),\n",
        "    'Engine': (1, 500),\n",
        "    'Front Wing': (1, 500),\n",
        "    'Brake Balance': (1, 500),\n",
        "    'Differential': (1, 500),\n",
        "    'Suspension': (1, 500),\n",
        "}\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective(trial):\n",
        "    x = base_input.copy()\n",
        "\n",
        "    for f in optim_features:\n",
        "        val = trial.suggest_int(f, int(feature_bounds[f][0]), int(feature_bounds[f][1]))\n",
        "        x[feature_cols.index(f)] = val\n",
        "\n",
        "    pred = model.predict(np.array([x]))[0]\n",
        "    return pred  # Optuna will maximize if we tell it to\n",
        "\n",
        "# Run Optuna study\n",
        "optuna.logging.disable_default_handler()\n",
        "study = optuna.create_study(direction='maximize')  # use 'minimize' if lower lap time is better\n",
        "study.optimize(objective, n_trials=255)\n",
        "\n",
        "\n",
        "# Show results\n",
        "print(\"Best params:\")\n",
        "for k, v in study.best_params.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "print(f\"Max predicted avg speed: {study.best_value}\")\n"
      ],
      "metadata": {
        "id": "6v_W-wPAUbrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grid Search for best Model Parameters\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Create the target variable: average speed\n",
        "df['Avg Speed'] = df['Lap Distance'] / df['Lap Time']\n",
        "\n",
        "# Drop unused columns\n",
        "X = df.drop(columns=['Lap Distance', 'Lap Time', 'Avg Speed'])\n",
        "y = df['Avg Speed']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define XGBoost Regressor\n",
        "model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define grid search parameters\n",
        "param_grid = {\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 300],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "    'min_child_weight': [1, 5]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "jJHJ_frRk-Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting the work on the sequential Optimizer"
      ],
      "metadata": {
        "id": "Z5B7ILoZpfSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining, which Parameters have the biggest impacts on Lap Times / Speeds.\n",
        "\n",
        "\n",
        "\n",
        "This code trains an XGBoost Regressor model to predict the average speed of a car based on various track and environmental features. First, it loads the data and calculates the average speed by dividing the lap distance by the lap time. The code then preprocesses the data by removing unnecessary columns and splits it into training and testing sets. After training the model, it evaluates the feature importances to determine which variables most strongly influence the predicted average speed. **Finally, it plots a bar chart to visually display the feature importances, providing insights into the relative impact of each feature.**"
      ],
      "metadata": {
        "id": "CL-Q4Cb9qo6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackabuse.com/bytes/get-feature-importance-from-xgbregressor-with-xgboost/\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = df[\"Lap Distance\"] / df[\"Lap Time\"]\n",
        "\n",
        "X = df.drop(columns=[\"Lap Distance\", \"Lap Time\", \"Avg Speed\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "importances = model.feature_importances_\n",
        "feat_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "feat_importance.plot(kind=\"bar\", figsize=(12, 6), title=\"Feature Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Lgr-Ysmx0tl9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining, which Track/Weather Parameters have the biggest impacts on each car parameter\n",
        "\n",
        "This code performs feature importance analysis for various car setup parameters based on environmental and track/weather conditions. It uses the XGBoost Regressor to model the relationship between the track/weather variables and each car setup parameter. The data is split into training and test sets, and the model is evaluated using the R² score to measure prediction accuracy. After training, the code extracts and plots the feature importances for each car parameter, helping to identify which track/weather variables have the most significant impact on each car setup."
      ],
      "metadata": {
        "id": "jhlF9wcwouWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Define inputs (track/weather features) and targets (car parameters)\n",
        "track_weather_features = [\n",
        "    'Lap Distance', 'Cornering', 'Inclines', 'Camber', 'Grip',\n",
        "    'Wind (Avg. Speed)', 'Temperature', 'Humidity', 'Air Density',\n",
        "    'Air Pressure', 'Wind (Gusts)', 'Altitude', 'Roughness', 'Width'\n",
        "]\n",
        "\n",
        "car_setup_params = ['Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential', 'Suspension']\n",
        "\n",
        "# Store feature importances for each car parameter\n",
        "feature_importance_dict = {}\n",
        "\n",
        "# Loop through car parameters\n",
        "for param in car_setup_params:\n",
        "    X = df[track_weather_features]\n",
        "    y = df[param]\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate R² score\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = r2_score(y_test, y_pred)\n",
        "    print(f\"\\n{param} - R²: {score:.3f}\")\n",
        "\n",
        "    # Get feature importances and filter by importance > 0.1\n",
        "    importances = model.feature_importances_\n",
        "    importance_series = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "    # Store importances\n",
        "    feature_importance_dict[param] = importance_series\n",
        "\n",
        "    # Display the features with importance > 0.1\n",
        "    print(f\"\\nFor {param}, track/weather parameters with importance > 0.1:\")\n",
        "    for feature, importance in importance_series.items():\n",
        "        if importance > 0.075:\n",
        "            print(f\" - {feature}: {importance:.3f}\")\n",
        "\n",
        "    # Optional: Plot feature importances\n",
        "    importance_series.plot(kind='barh', title=f\"Feature Importance for {param}\", figsize=(8, 5))\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "piDIG5ojfAUz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basiclly the same as the code before, but the other car parameters are also included in the models to determine the parameter importance\n",
        "#This is done to see, how different car parameters might influence each other\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Define inputs (track/weather features) and targets (car parameters)\n",
        "track_weather_features = [\n",
        "    'Lap Distance', 'Cornering', 'Inclines', 'Camber', 'Grip',\n",
        "    'Wind (Avg. Speed)', 'Temperature', 'Humidity', 'Air Density',\n",
        "    'Air Pressure', 'Wind (Gusts)', 'Altitude', 'Roughness', 'Width'\n",
        "]\n",
        "\n",
        "car_setup_params = ['Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential', 'Suspension']\n",
        "\n",
        "# Store feature importances for each car parameter\n",
        "feature_importance_dict = {}\n",
        "\n",
        "# Loop through car parameters\n",
        "for param in car_setup_params:\n",
        "    # Include track/weather features and other car parameters (excluding the target parameter itself)\n",
        "    other_car_params = [p for p in car_setup_params if p != param]\n",
        "    X = df[track_weather_features + other_car_params]\n",
        "    y = df[param]\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate R² score\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = r2_score(y_test, y_pred)\n",
        "    print(f\"\\n{param} - R²: {score:.3f}\")\n",
        "\n",
        "    # Get feature importances and sort them\n",
        "    importances = model.feature_importances_\n",
        "    importance_series = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "    # Store importances\n",
        "    feature_importance_dict[param] = importance_series\n",
        "\n",
        "    # Display the features with importance > 0.075\n",
        "    print(f\"\\nFor {param}, all parameters with importance > 0.075:\")\n",
        "    for feature, importance in importance_series.items():\n",
        "        if importance > 0.075:\n",
        "            print(f\" - {feature}: {importance:.3f}\")\n",
        "\n",
        "    # Optional: Plot feature importances\n",
        "    importance_series.plot(kind='barh', title=f\"Feature Importance for {param}\", figsize=(8, 5))\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "D3kuj_jRFkln",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential optimization of car-parameters.\n",
        "\n",
        "We optimize each Car-parameter in a predefined order, ensuring that each optimization builds upon the previous ones. (Order: Feature Importance (highest to lowest)(see above))\n",
        "\n",
        "For each car parameter, we first train a predictive model for average speed using relevant track/weather variables and already optimized parameters. Then, using Optuna, we search for the optimal value of the current car parameter that maximizes the predicted average speed.\n",
        "\n",
        "What track and wheather parameters are important for each Car-Parameter was determined before (see above)\n",
        "\n",
        "Order of optimization and relevant track/weather parameters:\n",
        "\n",
        "1. Engine: Grip, Altitude, Humidity, Air Density, Temperature, Air Pressure, Inclines\n",
        "2. Differetial: Cornering, Width, Inclines, Grip, Temprature, Air Density\n",
        "3. Rear Wing: Air Pressure, Air Density, Cornering, Inclines, Wind (Avg. Speed), Humidity, Roughness\n",
        "4. Break Balance: Width, Cornering, Roughness, Temperature\n",
        "5. Front Wing: Air Pressure, Cornering, Air Density, Inclines, Wind (Avg. Speed), Humidity, Wind (Gusts)\n",
        "6. Suspension: Grip, Inclines, Cornering, Camber, Width, Roughness\n"
      ],
      "metadata": {
        "id": "1HSUU_WOlmll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Fixed track & weather settings — user-defined!\n",
        "fixed_conditions = {\n",
        "    \"Cornering\": 6,\n",
        "    \"Inclines\": 20,\n",
        "    \"Camber\": 44,\n",
        "    \"Grip\": 1,\n",
        "    \"Altitude\": 31,\n",
        "    \"Roughness\": 49,\n",
        "    \"Width\": 29,\n",
        "    \"Temperature\": 29,\n",
        "    \"Humidity\": 23,\n",
        "    \"Wind (Avg. Speed)\": 97,\n",
        "    \"Wind (Gusts)\": 49,\n",
        "    \"Air Density\": 70,\n",
        "    \"Air Pressure\": 98,\n",
        "}\n",
        "\n",
        "# Order of optimization and relevant features\n",
        "optimization_order = [\n",
        "    (\"Engine\", [\"Grip\", \"Altitude\", \"Humidity\", \"Air Density\", \"Temperature\", \"Air Pressure\", \"Inclines\"]),\n",
        "    (\"Differential\", [\"Cornering\", \"Width\", \"Inclines\", \"Grip\", \"Temperature\", \"Air Density\"]),\n",
        "    (\"Rear Wing\", [\"Air Pressure\", \"Air Density\", \"Cornering\", \"Inclines\", \"Wind (Avg. Speed)\", \"Humidity\", \"Roughness\"]),\n",
        "    (\"Brake Balance\", [\"Width\", \"Cornering\", \"Roughness\", \"Temperature\"]),\n",
        "    (\"Front Wing\", [\"Air Pressure\", \"Cornering\", \"Air Density\", \"Inclines\", \"Wind (Avg. Speed)\", \"Humidity\", \"Wind (Gusts)\"]),\n",
        "    (\"Suspension\", [\"Grip\", \"Inclines\", \"Cornering\", \"Camber\", \"Width\", \"Roughness\"]),\n",
        "]\n",
        "\n",
        "# Storage for optimized parameters\n",
        "optimized_params = {}\n",
        "\n",
        "\n",
        "# Optimization loop\n",
        "for param, relevant_features in optimization_order:\n",
        "    print(f\"\\n Optimizing {param}...\")\n",
        "\n",
        "    # Features for model = track/weather + already optimized + current param\n",
        "    model_features = relevant_features + list(optimized_params.keys()) + [param]\n",
        "    model_df = df[model_features + [\"Avg Speed\"]].dropna()\n",
        "\n",
        "    X = model_df[model_features]\n",
        "    y = model_df[\"Avg Speed\"]\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Prepare fixed input for this stage\n",
        "    input_row = {f: fixed_conditions[f] for f in relevant_features}\n",
        "    input_row.update(optimized_params)  # Include already optimized parameters\n",
        "\n",
        "    def objective(trial):\n",
        "        trial_value = trial.suggest_int(param, 1, 500)\n",
        "        row = input_row.copy()\n",
        "        row[param] = trial_value\n",
        "        df_input = pd.DataFrame([row])\n",
        "        pred = model.predict(df_input)[0]\n",
        "        return pred  # Maximizing Avg Speed\n",
        "\n",
        "    optuna.logging.disable_default_handler()\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    print(f\"Max predicted avg speed: {study.best_value}\")\n",
        "\n",
        "    best_val = study.best_params[param]\n",
        "    optimized_params[param] = best_val\n",
        "\n",
        "    print(f\" Best {param}: {best_val}\")\n",
        "\n",
        "# Final output\n",
        "print(\"\\n All optimized parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "Lu2xXtIKRJMF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "All optimized parameters:\n",
        "Engine: 30\n",
        "Differential: 1\n",
        "Rear Wing: 48\n",
        "Brake Balance: 3\n",
        "Front Wing: 37\n",
        "Suspension: 124"
      ],
      "metadata": {
        "id": "-U8GQaEz_Gao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Same code as above, but the optimization order of the Car-Parameters is flipped.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Fixed track & weather settings — user-defined!\n",
        "fixed_conditions = {\n",
        "    \"Cornering\": 6,\n",
        "    \"Inclines\": 20,\n",
        "    \"Camber\": 44,\n",
        "    \"Grip\": 1,\n",
        "    \"Altitude\": 31,\n",
        "    \"Roughness\": 49,\n",
        "    \"Width\": 29,\n",
        "    \"Temperature\": 29,\n",
        "    \"Humidity\": 23,\n",
        "    \"Wind (Avg. Speed)\": 97,\n",
        "    \"Wind (Gusts)\": 49,\n",
        "    \"Air Density\": 70,\n",
        "    \"Air Pressure\": 98,\n",
        "}\n",
        "\n",
        "# Order of optimization and relevant features\n",
        "optimization_order = [\n",
        "    (\"Suspension\", [\"Grip\", \"Inclines\", \"Cornering\", \"Camber\", \"Width\", \"Roughness\"]),\n",
        "    (\"Front Wing\", [\"Air Pressure\", \"Cornering\", \"Air Density\", \"Inclines\", \"Wind (Avg. Speed)\", \"Humidity\", \"Wind (Gusts)\"]),\n",
        "    (\"Brake Balance\", [\"Width\", \"Cornering\", \"Roughness\", \"Temperature\"]),\n",
        "    (\"Rear Wing\", [\"Air Pressure\", \"Air Density\", \"Cornering\", \"Inclines\", \"Wind (Avg. Speed)\", \"Humidity\", \"Roughness\"]),\n",
        "    (\"Differential\", [\"Cornering\", \"Width\", \"Inclines\", \"Grip\", \"Temperature\", \"Air Density\"]),\n",
        "    (\"Engine\", [\"Grip\", \"Altitude\", \"Humidity\", \"Air Density\", \"Temperature\", \"Air Pressure\", \"Inclines\"])\n",
        "]\n",
        "\n",
        "# Storage for optimized parameters\n",
        "optimized_params = {}\n",
        "\n",
        "\n",
        "# Optimization loop\n",
        "for param, relevant_features in optimization_order:\n",
        "    print(f\"\\n Optimizing {param}...\")\n",
        "\n",
        "    # Features for model = track/weather + already optimized + current param\n",
        "    model_features = relevant_features + list(optimized_params.keys()) + [param]\n",
        "    model_df = df[model_features + [\"Avg Speed\"]].dropna()\n",
        "\n",
        "    X = model_df[model_features]\n",
        "    y = model_df[\"Avg Speed\"]\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Prepare fixed input for this stage\n",
        "    input_row = {f: fixed_conditions[f] for f in relevant_features}\n",
        "    input_row.update(optimized_params)  # Include already optimized parameters\n",
        "\n",
        "    def objective(trial):\n",
        "        trial_value = trial.suggest_int(param, 1, 500)\n",
        "        row = input_row.copy()\n",
        "        row[param] = trial_value\n",
        "        df_input = pd.DataFrame([row])\n",
        "        pred = model.predict(df_input)[0]\n",
        "        return pred  # Maximizing Avg Speed\n",
        "\n",
        "    #optuna.logging.disable_default_handler()\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=1000)\n",
        "    print(f\"Max predicted avg speed: {study.best_value}\")\n",
        "\n",
        "    best_val = study.best_params[param]\n",
        "    optimized_params[param] = best_val\n",
        "\n",
        "    print(f\" Best {param}: {best_val}\")\n",
        "\n",
        "# Final output\n",
        "print(\"\\n All optimized parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "MnVIytdH9tnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Race Strategy"
      ],
      "metadata": {
        "id": "br4U1GyguFO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the tire parameters and their lap time formulas\n",
        "def lap_time_super_soft(X):\n",
        "    return (69.269-0.141812499999999) + 0.141812499999999 * X\n",
        "\n",
        "def lap_time_soft(X):\n",
        "    return (70.53-0.0667741935483869) + 0.0667741935483869 * X\n",
        "\n",
        "def lap_time_medium(X):\n",
        "    return (70.927-0.0670857142857143) + 0.0670857142857143 * X\n",
        "\n",
        "def lap_time_hard(X):\n",
        "    return (70.257-0.109609756097561) + 0.109609756097561 * X\n",
        "\n",
        "# Tire data with their lifespan\n",
        "tire_lifespan = {\n",
        "    \"super_soft\": 16,\n",
        "    \"soft\": 30,\n",
        "    \"medium\": 35,\n",
        "    \"hard\": 41\n",
        "}\n",
        "\n",
        "# Pit stop penalty\n",
        "pit_stop_time = 30  # seconds\n",
        "\n",
        "# Function to calculate the total race time for a given strategy\n",
        "def calculate_race_time(laps, strategy):\n",
        "    total_time = 0\n",
        "    total_pit_stops = 0\n",
        "    lap_index = 0\n",
        "    lap_counter = 0\n",
        "\n",
        "    while lap_counter < laps:\n",
        "        tire, stint_laps = strategy[lap_index]\n",
        "\n",
        "        # Ensure we don't exceed the total laps\n",
        "        if lap_counter + stint_laps > laps:\n",
        "            stint_laps = laps - lap_counter\n",
        "\n",
        "        # Calculate the lap times for this stint\n",
        "        lap_times = []\n",
        "        for i in range(stint_laps):\n",
        "            if tire == \"super_soft\":\n",
        "                lap_times.append(lap_time_super_soft(i + 1))\n",
        "            elif tire == \"soft\":\n",
        "                lap_times.append(lap_time_soft(i + 1))\n",
        "            elif tire == \"medium\":\n",
        "                lap_times.append(lap_time_medium(i + 1))\n",
        "            elif tire == \"hard\":\n",
        "                lap_times.append(lap_time_hard(i + 1))\n",
        "\n",
        "        total_time += sum(lap_times)  # Add the lap times of this stint\n",
        "        lap_counter += stint_laps\n",
        "\n",
        "        # If we are not at the last stint, account for a pit stop\n",
        "        if lap_counter < laps:\n",
        "            total_time += pit_stop_time  # Pit stop penalty\n",
        "            total_pit_stops += 1\n",
        "\n",
        "        lap_index += 1\n",
        "        if lap_index >= len(strategy):\n",
        "            break\n",
        "\n",
        "    return total_time, total_pit_stops\n",
        "\n",
        "# Function to generate possible strategies dynamically\n",
        "def generate_strategies(laps):\n",
        "    strategies = []\n",
        "    tire_choices = [\"super_soft\", \"soft\", \"medium\", \"hard\"]\n",
        "\n",
        "    # Generate strategies by breaking the laps into multiple stints\n",
        "    for tire1 in tire_choices:\n",
        "        for tire2 in tire_choices:\n",
        "            for tire3 in tire_choices:\n",
        "                #for tire4 in tire_choices:\n",
        "                  #for tire5 in tire_choices:\n",
        "                    strategy = []\n",
        "                    remaining_laps = laps\n",
        "\n",
        "                    # Create dynamic stints for each tire\n",
        "                    #for tire in [tire1, tire2, tire3, tire4, tire5]:\n",
        "                    #for tire in [tire1, tire2, tire3, tire4]:\n",
        "                    for tire in [tire1, tire2, tire3]:\n",
        "                    #for tire in [tire1, tire2]:\n",
        "                        stint_laps = tire_lifespan[tire]\n",
        "\n",
        "                        if remaining_laps > stint_laps:\n",
        "                            strategy.append((tire, stint_laps))\n",
        "                            remaining_laps -= stint_laps\n",
        "                        else:\n",
        "                            strategy.append((tire, remaining_laps))\n",
        "                            break\n",
        "\n",
        "                    if sum([stint[1] for stint in strategy]) == laps:\n",
        "                        strategies.append(strategy)\n",
        "\n",
        "    return strategies\n",
        "\n",
        "# Function to find the best strategy\n",
        "def optimize_strategy(laps):\n",
        "    best_time = math.inf\n",
        "    best_strategy = None\n",
        "\n",
        "    strategies = generate_strategies(laps)\n",
        "\n",
        "    for strategy in strategies:\n",
        "        total_time, pit_stops = calculate_race_time(laps, strategy)\n",
        "        if total_time < best_time:\n",
        "            best_time = total_time\n",
        "            best_strategy = strategy\n",
        "            best_pit_stops = pit_stops\n",
        "\n",
        "    return best_strategy, best_time, best_pit_stops\n",
        "\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    race_laps = 83\n",
        "    best_strategy, best_time, total_pit_stops = optimize_strategy(race_laps)\n",
        "    print(f\"Best Strategy: {best_strategy}\")\n",
        "    print(f\"Best Total Time: {best_time} seconds\")\n",
        "    print(f\"Total Pit Stops: {total_pit_stops}\")\n"
      ],
      "metadata": {
        "id": "VQEz50zWfvhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best Strategy: [('super_soft', 16), ('super_soft', 16), ('super_soft', 16), ('super_soft', 16), ('soft', 19)]\n",
        "Best Total Time: 5972.774387096774 seconds\n",
        "\n",
        "Best Strategy: [('super_soft', 16), ('super_soft', 16), ('soft', 30), ('soft', 21)]\n",
        "Best Total Time: 5980.742354838709 seconds\n",
        "\n",
        "Best Strategy: [('soft', 30), ('soft', 30), ('soft', 23)]\n",
        "Best Total Time: 5988.977419354838 seconds"
      ],
      "metadata": {
        "id": "jMfNBV63wjmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Car-Parameters, to see, if optimized values are actually better than other values."
      ],
      "metadata": {
        "id": "TqLe2liv8T-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "print(f'Rear Wing {randint(1, 500)}')\n",
        "print(f'Engine {randint(1, 500)}')\n",
        "print(f'Front Wing {randint(1, 500)}')\n",
        "print(f'Brakebalance {randint(1, 500)}')\n",
        "print(f'Differential {randint(1, 500)}')\n",
        "print(f'Suspension {randint(1, 500)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "WMpm0v-d5b_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1LiOt7yQL5T"
      },
      "source": [
        "# **Analytics for Race 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview of Our Approach for Race Two**\n",
        "\n",
        "Our work for Race Two can be divided into three main parts:\n",
        "\n",
        "1. We attempted to build an \"All-In-One\" optimization method again.\n",
        "2. We further developed our sequential optimization approach.\n",
        "3. We applied our race-strategy code from the first race to decide on an optimal strategy.\n",
        "\n",
        "\n",
        "One significant change was switching from XGBoost to LightGBM, based on discussions in class. This switch affected both our \"All-In-One\" and sequential optimization methods. Additionally, instead of training models within the code during runtime, we now train them beforehand and load the pre-trained models as needed.\n",
        "\n",
        "\n",
        "**1: \"All-In-One\"-Optimization:**\n",
        "\n",
        "We learned online that, for LightGBM, hyperparameter tuning plays a crucial role. Therefore, we began by optimizing hyperparameters specifically for our use case and dataset.\n",
        "\n",
        "After tuning the hyperparameters, we evaluated the model's performance using the R² metric, both with and without hyperparameter tuning. Once satisfied with the hyperparameters, we trained a LightGBM model on the full dataset and stored it for later use.\n",
        "\n",
        "We then used this trained model in conjunction with Optuna to optimize car parameters, fixing track and weather conditions. Finally, we developed code to import the trained model and further train it using practice data for improved performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**2: Sequential optimization:**\n",
        "\n",
        "This week, we reflected on how to improve our sequential optimization approach. We devised a plan that not only involved switching to LightGBM but also included six detailed steps:\n",
        "\n",
        "1. Which Car-Parameters should use which track/weather-parameters\n",
        "\n",
        "2. In what order should the Car-Parameters be optimized?\n",
        "\n",
        "3. Write Code, that optimizes each Models Hyperparameters using Optuna and K-Fold-cross-validation.\n",
        "\n",
        "4. Train and Save each Model with full data set (no Train/Test-Split)\n",
        "\n",
        "5. Optimize Models using Practice_Data\n",
        "\n",
        "6. Write Code, that sequentially optimizes Car-Parameters with existing Models\n",
        "\n",
        "The first and second steps of this plan involve using feature importance to determine the order and parameters relevant for optimizing the car parameters.\n",
        "\n",
        "To select which Track and Weather parameters should be used for optimizing each car parameter, we plotted the feature importance for each car parameter. All resulting graphs revealed a clear cutoff point, where the importance of subsequent Track and Weather parameters significantly decreased. We chose to use the parameters above each cutoff point for the optimization.\n",
        "\n",
        "Regarding the order of optimization, we decided to proceed from the most important to the least important parameters. This approach ensures that the parameters with the greatest influence are optimized first, with the lowest possible bias from other already fixed parameters.\n",
        "\n",
        "In addition to the selected Track and Weather parameters for each optimization step, we also include each previously optimized car parameter as part of the parameter set for subsequent optimization steps.\n",
        "\n",
        "As a third step, we developed code to optimize the hyperparameters for each of the six parameter combinations. After completing this process, we trained and saved each model.\n",
        "\n",
        "With the trained models in hand, we used a modified version of our sequential optimizer from week one to initiate the first round of optimization. Following several practice rounds, we employed the code from Step 5 to further train the six models using the practice data.\n",
        "\n",
        "\n",
        "\n",
        "**3: Race Strategy:**\n",
        "\n",
        "We used the same Race Strategy Calculator from week one with updated Racedata.  "
      ],
      "metadata": {
        "id": "JQ_3gOqBVy7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Choice, Parameter Variation, Track/Weather as .csv and optimization procedure:**\n",
        "\n",
        "**Hyperparameter Choice:**\n",
        "\n",
        "We chose these hyperparameters because they have a significant impact on the model’s performance and ability to generalize to unseen data. Controlling model complexity through `num_leaves`, `max_depth`, and `min_data_in_leaf` helps prevent overfitting and underfitting, ensuring the model is sufficiently expressive for the problem at hand. The `learning_rate` was selected to balance training speed and convergence, allowing the model to learn effectively without overshooting optimal solutions. Sampling parameters such as `feature_fraction`, `bagging_fraction`, and `bagging_freq` were included to promote diversity during training, which enhances robustness and reduces overfitting. Finally, `lambda_l1` and `lambda_l2` introduce regularization, which is crucial when working with high-dimensional data, as it helps prevent the model from becoming overly complex. Together, these parameters provide a comprehensive strategy to improve model performance, stability, and generalization capabilities.\n",
        "\n",
        "\n",
        "**Parameter Variation:**\n",
        "We encountered a situation where our sequential optimizer produced the same set of parameters every time it was run. While this consistency indicates that the approach yields low variability in the results— which is not necessarily a problem—it posed a challenge when trying to create practice data for further training of the LightGBM models. Fortunately, our all-in-one optimizer exhibited considerable variability in the parameter sets it generated. To leverage this advantage, we limited the parameter ranges within the all-in-one optimizer to focus around the parameters suggested by the sequential optimizer. This approach aimed to generate parameter combinations that the all-in-one optimizer considered optimal. These combinations were then used to create practice data for further model training. The size of the parameter ranges in the all-in-one optimizer was determined by the sensitivity of the individual car parameters, with higher sensitivity leading to narrower ranges.\n",
        "\n",
        "This method was based on the assumption that our sequential optimizer identified values that were reasonably close to optimal, meaning the parameters it suggested were not entirely incorrect. This assumption allowed us to focus the optimization process and refine the models effectively.\n",
        "\n",
        "**Track/Weather as .csv**\n",
        "Another improvement we made this week was to import the track and weather parameters from a .csv file, instead of setting them directly within the code. This approach not only cleans up the code but also makes it easier to adapt for future races\n",
        "\n",
        "\n",
        "**Optimization Procedure:**\n",
        "We performed several iterations of retraining our models, during which we varied the parameters using the all-in-one optimizer. The resulting data from each iteration was fed back into the models to further refine and improve their performance."
      ],
      "metadata": {
        "id": "BsCownwWraR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozTX4aI8wSqa"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Paula Kussauer, Cedric Schwandt, Hannes Kock"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AllInOne Optimization"
      ],
      "metadata": {
        "id": "gaek7r_g2y4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZcWkDUrQL5T"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Names.."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning for Full LGBM"
      ],
      "metadata": {
        "id": "KEAdErdH2qho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    X = df.drop(columns=['Lap Distance', 'Lap Time', 'Avg Speed'])\n",
        "    y = df[\"Avg Speed\"]\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        #rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gfu5XKfCDNOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Tuning for two datasets\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV2_6.csv\")\n",
        "\n",
        "# Calculate target: Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0  # Adjust this ratio to control importance (10000/40 = 250)\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Prepare features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\",\"Round\", \"Track\", \"Qualifying\", \"Stint\", \"Lap\", \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights.iloc[train_idx], weights.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=1000,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T7NHt2Z04N5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### R^2 Test LightGBM"
      ],
      "metadata": {
        "id": "lSp_Yhl7Dm9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    }\n",
        "\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "r2_scores = []\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # LightGBM Dataset\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "\n",
        "    # Train model\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_set=lgb_train,\n",
        "        num_boost_round=300  )\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = model.predict(X_val)\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    r2_scores.append(r2)\n",
        "    print(f\"Fold R^2 Score: {r2:.4f}\")\n",
        "\n",
        "# Overall R^2\n",
        "print(f\"\\nAverage R^2 Score: {np.mean(r2_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "-BbmAyMtTs13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training LGBM-Model with Simulator_Data and optm. Hyperparameters"
      ],
      "metadata": {
        "id": "fa53UW9MDxas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"best_lgbm_model.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'best_lgbm_model.txt'\")\n"
      ],
      "metadata": {
        "id": "8UuMHCcezFBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Directly trained LGBM-Model with Simulator_Data and Practice_data using optm. Hyperparameters\n",
        "#Hyperparameters optm. for Simulator and Practice_data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV2_6.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0\n",
        "\n",
        "# Make sure both have the same columns in the same order\n",
        "required_cols = [\"Lap Distance\", \"Lap Time\", \"Avg Speed\", \"weight\", \"Rear Wing\", \"Front Wing\", \"Engine\", \"Brake Balance\", \"Differential\", \"Suspension\", \"Cornering\", \"Inclines\", \"Camber\", \"Grip\", \"Altitude\", \"Roughness\", \"Width\", \"Temperature\", \"Humidity\", \"Wind (Avg. Speed)\", \"Wind (Gusts)\", \"Air Density\", \"Air Pressure\"] + \\\n",
        "                [col for col in sim_df.columns if col not in [\"Lap Distance\", \"Lap Time\", \"Avg Speed\", \"weight\", \"Rear Wing\", \"Front Wing\", \"Engine\", \"Brake Balance\", \"Differential\", \"Suspension\", \"Cornering\", \"Inclines\", \"Camber\", \"Grip\", \"Altitude\", \"Roughness\", \"Width\", \"Temperature\", \"Humidity\", \"Wind (Avg. Speed)\", \"Wind (Gusts)\", \"Air Density\", \"Air Pressure\"]]\n",
        "\n",
        "sim_df = sim_df[required_cols]\n",
        "prac_df = prac_df[required_cols]\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Features, target, and weights\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\", \"weight\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (already tuned)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 74,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.028870630893942515,\n",
        "    'min_data_in_leaf': 13,\n",
        "    'feature_fraction': 0.7399238619191133,\n",
        "    'bagging_fraction': 0.9203471068533383,\n",
        "    'bagging_freq': 6,\n",
        "    'lambda_l1': 0.025758523534949052,\n",
        "    'lambda_l2': 0.9073066525703107}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"sim_prac_lgbm_model.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'sim_prac_lgbm_model.txt'\")\n"
      ],
      "metadata": {
        "id": "iIDwVfsI9h59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving previously traind LGBM-Model using Pracice_Data"
      ],
      "metadata": {
        "id": "8JIu4WehEAwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the original model\n",
        "model = lgb.Booster(model_file=\"best_lgbm_model.txt\")\n",
        "\n",
        "# Load the new data\n",
        "new_df = pd.read_csv(\"practice_dataV2_5.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "new_df[\"Avg Speed\"] = (new_df[\"Lap Distance\"] / new_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Prepare features and target\n",
        "X_new = new_df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\",\"Round\", \"Track\", \"Qualifying\", \"Stint\", \"Lap\", \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"])\n",
        "y_new = new_df[\"Avg Speed\"]\n",
        "\n",
        "# Assign higher weights (e.g., 10x more important)\n",
        "sample_weight = [1000] * len(y_new)\n",
        "\n",
        "# Create new LightGBM Dataset\n",
        "new_data = lgb.Dataset(X_new, label=y_new, weight=sample_weight)\n",
        "\n",
        "# Continue training from previous model\n",
        "model = lgb.train(\n",
        "    params={},  # Empty here since model already knows them\n",
        "    train_set=new_data,\n",
        "    init_model=model,\n",
        "    num_boost_round=1000,  # You can increase if needed\n",
        ")\n",
        "\n",
        "# Save updated model\n",
        "model.save_model(\"best_lgbm_model_updated.txt\")\n",
        "\n",
        "print(\"Model updated with new data and saved to 'best_lgbm_model_updated.txt'\")\n"
      ],
      "metadata": {
        "id": "wH98NNhn3Z7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Used to test prediction performance against practice_data"
      ],
      "metadata": {
        "id": "2mSYqbY-EM3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model\n",
        "model = lgb.Booster(model_file=\"best_lgbm_model.txt\")\n",
        "\n",
        "# Load the test data\n",
        "test_df = pd.read_csv(\"practice_data_snd_half.csv\")\n",
        "\n",
        "# Calculate actual average speed\n",
        "test_df[\"Actual Avg Speed\"] = (test_df[\"Lap Distance\"] / test_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Prepare the test features (same feature engineering as training)\n",
        "X_test = test_df.drop(columns=[\"Actual Avg Speed\", \"Lap Distance\", \"Lap Time\",\"Round\", \"Track\", \"Qualifying\", \"Stint\", \"Lap\", \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"])\n",
        "\n",
        "# Predict using the model\n",
        "test_df[\"Predicted Avg Speed\"] = model.predict(X_test)\n",
        "\n",
        "# Calculate difference\n",
        "test_df[\"Difference\"] = test_df[\"Predicted Avg Speed\"] - test_df[\"Actual Avg Speed\"]\n",
        "\n",
        "# Output the results\n",
        "print(test_df[[\"Predicted Avg Speed\", \"Actual Avg Speed\", \"Difference\"]])\n",
        "\n",
        "# Summary statistics\n",
        "mae = np.mean(np.abs(test_df[\"Difference\"]))\n",
        "rmse = np.sqrt(np.mean(test_df[\"Difference\"] ** 2))\n",
        "max_diff = np.max(np.abs(test_df[\"Difference\"]))\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
        "print(f\"Maximum Absolute Difference: {max_diff:.2f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xx99yt_r4Fmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Optimization"
      ],
      "metadata": {
        "id": "JwwlFnR2JJwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "\n",
        "# Load trained model\n",
        "#model = lgb.Booster(model_file=\"best_lgbm_model_updated.txt\")\n",
        "model = lgb.Booster(model_file=\"sim_prac_lgbm_model.txt\")\n",
        "\n",
        "# Load track/weather data (single row)\n",
        "track_weather = pd.read_csv(\"track_weather_germany.csv\")\n",
        "\n",
        "# Define the objective function\n",
        "def objective(trial):\n",
        "    # Suggest car parameters\n",
        "    params = {\n",
        "        \"Engine\": trial.suggest_int(\"Engine\", 1, 500),\n",
        "        \"Rear Wing\": trial.suggest_int(\"Rear Wing\", 1, 500),\n",
        "        \"Front Wing\": trial.suggest_int(\"Front Wing\", 1, 500),\n",
        "        \"Brake Balance\": trial.suggest_int(\"Brake Balance\", 1, 500),\n",
        "        \"Suspension\": trial.suggest_int(\"Suspension\", 1, 500),\n",
        "        \"Differential\": trial.suggest_int(\"Differential\", 1, 500),\n",
        "    }\n",
        "#Engine: 91\n",
        "#Differential: 101\n",
        "#RearWing: 489\n",
        "#FrontWing: 274\n",
        "#Suspension: 45\n",
        "#BrakeBalance: 101\n",
        "\n",
        "    # Combine with static track/weather parameters\n",
        "    input_data = pd.concat([track_weather, pd.DataFrame([params])], axis=1)\n",
        "\n",
        "    # Predict average speed\n",
        "    predicted_avg_speed = model.predict(input_data)[0]\n",
        "\n",
        "    # We want to maximize speed\n",
        "    return -predicted_avg_speed\n",
        "\n",
        "# Create study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Output best results\n",
        "best_trial = study.best_trial\n",
        "print(\"\\nBest Parameters:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print(f\"Predicted Avg Speed: {-best_trial.value:.2f}\")"
      ],
      "metadata": {
        "id": "GZv70c6f9aTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Best Parameters:\n",
        "Engine: 133\n",
        "Rear Wing: 39\n",
        "Front Wing: 67\n",
        "Brake: 217\n",
        "Suspension: 95\n",
        "Differential: 88\n",
        "Predicted Avg Speed: --187.46\n",
        "\n",
        "Engine: 91\n",
        "Differential: 101\n",
        "RearWing: 489\n",
        "FrontWing: 274\n",
        "Suspension: 45\n",
        "BrakeBalance: 101"
      ],
      "metadata": {
        "id": "9dwmXQnPUNM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the intervals around the supposedly optimal parameters\n"
      ],
      "metadata": {
        "id": "tjXh9FR_eX8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "import threading\n",
        "\n",
        "# Load trained model\n",
        "#model = lgb.Booster(model_file=\"best_lgbm_model_updated.txt\")\n",
        "model = lgb.Booster(model_file=\"best_lgbm_model.txt\")\n",
        "\n",
        "# Load track/weather data (single row)\n",
        "track_weather = pd.read_csv(\"track_weather_germany.csv\")\n",
        "\n",
        "# Store all best results\n",
        "all_best_results = []\n",
        "\n",
        "# Optional: Start dashboard for one of the runs\n",
        "def run_dashboard(study):\n",
        "    run_server(study)\n",
        "\n",
        "# Run 20 optimization loops\n",
        "for run in range(1, 11):\n",
        "    print(f\"\\n🔁 Optimization Run {run}/20\")\n",
        "\n",
        "    # Define objective function (wrapped inside loop to keep it clean)\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"Engine\": trial.suggest_int(\"Engine\", 85, 115),\n",
        "            \"Rear Wing\": trial.suggest_int(\"Rear Wing\", 499, 500),\n",
        "            \"Front Wing\": trial.suggest_int(\"Front Wing\", 70, 90),\n",
        "            \"Brake\": trial.suggest_int(\"Brake\", 85, 115),\n",
        "            \"Suspension\": trial.suggest_int(\"Suspension\", 20, 50),\n",
        "            \"Differential\": trial.suggest_int(\"Differential\", 68, 98),\n",
        "        }\n",
        "\n",
        "#Engine:         91  +-30    => 85 - 115\n",
        "#Differential:  101  +-30    => 68 - 98\n",
        "#RearWing:      489  +-125   => 499 - 500\n",
        "#FrontWing:     274  +-125   => 70 - 90\n",
        "#Suspension:     45  +-125  => 20 - 50\n",
        "#BrakeBalance:  101  +-125   => 85 - 115\n",
        "\n",
        "        input_data = pd.concat([track_weather, pd.DataFrame([params])], axis=1)\n",
        "        predicted_avg_speed = model.predict(input_data)[0]\n",
        "        return predicted_avg_speed  # maximize speed\n",
        "\n",
        "    # Create a new study for each run\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "\n",
        "    # Run optimization (adjust trials as needed)\n",
        "    study.optimize(objective, n_trials=100)\n",
        "\n",
        "    # Collect results\n",
        "    best_trial = study.best_trial\n",
        "    result = {\n",
        "        \"Run\": run,\n",
        "        \"Predicted Avg Speed\": best_trial.value,\n",
        "        **best_trial.params\n",
        "    }\n",
        "    all_best_results.append(result)\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(all_best_results)\n",
        "print(\"\\n📊 Summary of All Runs:\")\n",
        "print(results_df)\n",
        "\n",
        "# Optionally save to CSV\n",
        "results_df.to_csv(\"optimization_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "UZyHIfdAA4Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine:         91  +-30    => 61 - 121\n",
        "#Differential:  101  +-30    => 51 - 151\n",
        "#RearWing:      489  +-125   => 364 - 500\n",
        "#FrontWing:     274  +-125   => 149 - 399\n",
        "#Suspension:     45  +-125  => 5 - 170\n",
        "#BrakeBalance:  101  +-125   => 0 - 226"
      ],
      "metadata": {
        "id": "lSkcp0gsWA3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "📊 Summary of All Runs: (ohne update)\n",
        "   Run  Predicted Avg Speed  Engine  Rear Wing  Front Wing  Brake  Suspension  \\\n",
        "0    1           186.038712      89        411         170    100          91\n",
        "1    2           185.970222     111        369         156    210          80\n",
        "2    3           186.081557     111        478         163    101          82\n",
        "3    4           185.960439     105        496         154    176          97\n",
        "4    5           185.998318      93        445         198    121          99\n",
        "5    6           186.093750     109        427         152    107          98\n",
        "6    7           186.047609      89        463         198    116          92\n",
        "7    8           186.013795     110        459         149     99          88\n",
        "8    9           186.048086      94        437         155    101         106\n",
        "9   10           186.066058      90        388         158    114          94\n",
        "\n",
        "   Differential\n",
        "0            86\n",
        "1            82\n",
        "2            83\n",
        "3            94\n",
        "4            82\n",
        "5            82\n",
        "6            83\n",
        "7           100\n",
        "8            91\n",
        "9            85"
      ],
      "metadata": {
        "id": "kCGkj75KXB0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CEjrFYansSCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Optimization LightGBM"
      ],
      "metadata": {
        "id": "M_Txp08_sVwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1. Which Car-Parameters should use which track/weather-parameters\n",
        "\n",
        "2. In what order should the Car-Parameters be optimized?\n",
        "\n",
        "3. Write Code, that optimizes each Models Hyperparameters using Optuna and K-Fold-cross-validation.\n",
        "\n",
        "4. Train and Save each Model with full data set (no Train/Test-Split)\n",
        "\n",
        "5. Optimize Models using Practice_Data\n",
        "\n",
        "6. Write Code, that sequentially optimizes Car-Parameters with existing Models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IW2dhvstsfJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Step: Which Car-Parameters should use which track/weather-parameters?\n"
      ],
      "metadata": {
        "id": "DvpEBv-CuDsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Rear Wing\n",
        " - Air Density\n",
        " - Cornering\n",
        " - Air Pressure\n",
        " - Inclines\n",
        " - Wind (Avg. Speed)\n",
        " - Humidity\n",
        " - Roughness\n",
        "\n",
        "Engine\n",
        " - Grip\n",
        " - Humidity\n",
        " - Air Density\n",
        " - Altitude\n",
        " - Temperature\n",
        " - Inclines\n",
        " - Air Pressure\n",
        " - Cornering\n",
        "\n",
        "Front Wing\n",
        " - Cornering\n",
        " - Air Pressure\n",
        " - Air Density\n",
        " - Inclines\n",
        " - Wind (Avg. Speed)\n",
        " - Humidity\n",
        " - Wind (Gusts)\n",
        "\n",
        "Brake Balance\n",
        " - Cornering\n",
        " - Width\n",
        " - Roughness\n",
        " - Temperature\n",
        "\n",
        "Differential\n",
        " - Cornering\n",
        " - Width\n",
        " - Inclines\n",
        " - Grip\n",
        " - Temperature\n",
        " - Air Density\n",
        "\n",
        "Suspension\n",
        " - Grip\n",
        " - Inclines\n",
        " - Cornering\n",
        " - Camber\n",
        " - Roughness\n",
        " - Width"
      ],
      "metadata": {
        "id": "2EYjgPWkveBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Define inputs (track/weather features) and targets (car parameters)\n",
        "track_weather_features = [\n",
        "    'Cornering', 'Inclines', 'Camber', 'Grip',\n",
        "    'Wind (Avg. Speed)', 'Temperature', 'Humidity', 'Air Density',\n",
        "    'Air Pressure', 'Wind (Gusts)', 'Altitude', 'Roughness', 'Width'\n",
        "]\n",
        "\n",
        "car_setup_params = ['Rear Wing', 'Engine', 'Front Wing', 'Brake Balance', 'Differential', 'Suspension']\n",
        "\n",
        "# Store feature importances for each car parameter\n",
        "feature_importance_dict = {}\n",
        "\n",
        "# Loop through car parameters\n",
        "for param in car_setup_params:\n",
        "    X = df[track_weather_features]\n",
        "    y = df[param]\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    model = LGBMRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate R² score\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = r2_score(y_test, y_pred)\n",
        "    print(f\"\\n{param} - R²: {score:.3f}\")\n",
        "\n",
        "    # Get feature importances and filter by importance > 0.1\n",
        "    importances = model.booster_.feature_importance(importance_type='gain')\n",
        "    importance_series = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "    # Store importances\n",
        "    feature_importance_dict[param] = importance_series\n",
        "\n",
        "    # Display the features with importance > 0.075\n",
        "    print(f\"\\nFor {param}, track/weather parameters with importance > 0.075:\")\n",
        "    for feature, importance in importance_series.items():\n",
        "        if importance > 0.075:\n",
        "            print(f\" - {feature}: {importance:.3f}\")\n",
        "\n",
        "    # Optional: Plot feature importances\n",
        "    importance_series.plot(kind='barh', title=f\"Feature Importance for {param}\", figsize=(8, 5))\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5dN_q0RDuADO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second Step: In what order should the Car-Parameters be optimized?"
      ],
      "metadata": {
        "id": "AihzvPHiv54j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Engine\n",
        "2. Differential\n",
        "3. Rear Wing\n",
        "4. Front Wing\n",
        "5. Suspension\n",
        "6. Break Balance"
      ],
      "metadata": {
        "id": "CAwkPifgwNTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Define X and y\n",
        "X = df.drop(columns=['Lap Distance', 'Lap Time', 'Avg Speed'])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Set the hyperparameters\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "train_data = lgb.Dataset(X, label=y)\n",
        "model = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=100)\n",
        "\n",
        "# Get feature importance by 'split' and 'gain'\n",
        "importance_split = model.feature_importance(importance_type='split')\n",
        "importance_gain = model.feature_importance(importance_type='gain')\n",
        "\n",
        "# Sort feature importance by 'split'\n",
        "sorted_split_idx = importance_split.argsort()[::1]\n",
        "sorted_split_importance = importance_split[sorted_split_idx]\n",
        "sorted_split_features = X.columns[sorted_split_idx]\n",
        "\n",
        "# Sort feature importance by 'gain'\n",
        "sorted_gain_idx = importance_gain.argsort()[::1]\n",
        "sorted_gain_importance = importance_gain[sorted_gain_idx]\n",
        "sorted_gain_features = X.columns[sorted_gain_idx]\n",
        "\n",
        "# Plot feature importance by 'split'\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(sorted_split_features, sorted_split_importance , color='red')\n",
        "plt.title('Feature Importance by Split')\n",
        "plt.xlabel('Number of Splits')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importance by 'gain'\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(sorted_gain_features, sorted_gain_importance, color='blue')\n",
        "plt.title('Feature Importance by Gain')\n",
        "plt.xlabel('Gain')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hn0-UX1vv-Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third Step: Write Code, that optimizes each Models Hyperparameters using Optuna and K-Fold-cross-validation."
      ],
      "metadata": {
        "id": "vCbFxwAXwaIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimized Hyperparameters:\n",
        "Engine:\n",
        "{'num_leaves': 281, 'max_depth': 11, 'learning_rate': 0.04083240077095524, 'min_data_in_leaf': 96, 'feature_fraction': 0.9534759457760267, 'bagging_fraction': 0.6637656869307713, 'bagging_freq': 7, 'lambda_l1': 0.48486124811843856, 'lambda_l2': 1.0469435993282974}\n",
        "Differential:\n",
        "{'num_leaves': 126, 'max_depth': 8, 'learning_rate': 0.05725939263903659, 'min_data_in_leaf': 96, 'feature_fraction': 0.9048274481436259, 'bagging_fraction': 0.6871315558035545, 'bagging_freq': 1, 'lambda_l1': 2.9809700145880162, 'lambda_l2': 2.893530172381254}\n",
        "Rear Wing:\n",
        "{'num_leaves': 41, 'max_depth': 14, 'learning_rate': 0.07379289439034513, 'min_data_in_leaf': 14, 'feature_fraction': 0.9805583874353474, 'bagging_fraction': 0.757494844528942, 'bagging_freq': 1, 'lambda_l1': 3.3280185770912905, 'lambda_l2': 4.668868493827121}\n",
        "Front Wing:\n",
        "{'num_leaves': 228, 'max_depth': 6, 'learning_rate': 0.10991909073754202, 'min_data_in_leaf': 94, 'feature_fraction': 0.954670793846081, 'bagging_fraction': 0.7659843231590522, 'bagging_freq': 1, 'lambda_l1': 3.8616883301361553, 'lambda_l2': 4.896984209036385}\n",
        "Suspension:\n",
        "{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.14051851191638579, 'min_data_in_leaf': 75, 'feature_fraction': 0.999888882811791, 'bagging_fraction': 0.7009625088481276, 'bagging_freq': 1, 'lambda_l1': 4.8851994495913145, 'lambda_l2': 3.6824821872767783}\n",
        "Break Balance:\n",
        "{'num_leaves': 27, 'max_depth': 11, 'learning_rate': 0.18696777761444966, 'min_data_in_leaf': 73, 'feature_fraction': 0.964098272670725, 'bagging_fraction': 0.8852821315081366, 'bagging_freq': 10, 'lambda_l1': 3.486399193949678, 'lambda_l2': 3.7053586457221366}"
      ],
      "metadata": {
        "id": "af_mRuKP2NuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HP-optimization code for each Car-Param."
      ],
      "metadata": {
        "id": "6wPBhGNqDqh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Engine\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        #rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FlBxc04rw-MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Differential\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        #rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UJ7o8EcIzwfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Rear Wing\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    X = df[['Rear Wing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        #rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "G1WltfoG0XDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Front Wing\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    X = df[['Front Wing', 'Rear Wing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        #rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "vf9k6sWp0YYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Suspension\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    X = df[['Suspension', 'Front Wing', 'Rear Wing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        #rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "0peIPcB80ZSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Brake Balance\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"])*3600\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    X = df[['Brake Balance', 'Suspension', 'Front Wing', 'Rear Wing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        #rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "Xdamwe740aPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourth Step: Train and Save each Model with full data set (no Train/Test-Split)"
      ],
      "metadata": {
        "id": "KF_qUNre8BP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Brake Balance: \tX = df[['Brake_Balance', 'Suspension', 'Front_Wing', 'Rear_Wing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "Suspension: \tX = df[['Suspension', 'Front_Wing', 'Rear_Wing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "Front Wing: \tX = df[['Front_Wing', 'Rear_Wing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "Rear Wing: \tX = df[['Rear_Wing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "Differential: \tX = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "Engine: \tX = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]"
      ],
      "metadata": {
        "id": "lDZ_Uj7sAJrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimized Hyperparameters:\n",
        "Engine:\n",
        "{'num_leaves': 281, 'max_depth': 11, 'learning_rate': 0.04083240077095524, 'min_data_in_leaf': 96, 'feature_fraction': 0.9534759457760267, 'bagging_fraction': 0.6637656869307713, 'bagging_freq': 7, 'lambda_l1': 0.48486124811843856, 'lambda_l2': 1.0469435993282974}\n",
        "Differential:\n",
        "{'num_leaves': 126, 'max_depth': 8, 'learning_rate': 0.05725939263903659, 'min_data_in_leaf': 96, 'feature_fraction': 0.9048274481436259, 'bagging_fraction': 0.6871315558035545, 'bagging_freq': 1, 'lambda_l1': 2.9809700145880162, 'lambda_l2': 2.893530172381254}\n",
        "Rear Wing:\n",
        "{'num_leaves': 41, 'max_depth': 14, 'learning_rate': 0.07379289439034513, 'min_data_in_leaf': 14, 'feature_fraction': 0.9805583874353474, 'bagging_fraction': 0.757494844528942, 'bagging_freq': 1, 'lambda_l1': 3.3280185770912905, 'lambda_l2': 4.668868493827121}\n",
        "Front Wing:\n",
        "{'num_leaves': 228, 'max_depth': 6, 'learning_rate': 0.10991909073754202, 'min_data_in_leaf': 94, 'feature_fraction': 0.954670793846081, 'bagging_fraction': 0.7659843231590522, 'bagging_freq': 1, 'lambda_l1': 3.8616883301361553, 'lambda_l2': 4.896984209036385}\n",
        "Suspension:\n",
        "{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.14051851191638579, 'min_data_in_leaf': 75, 'feature_fraction': 0.999888882811791, 'bagging_fraction': 0.7009625088481276, 'bagging_freq': 1, 'lambda_l1': 4.8851994495913145, 'lambda_l2': 3.6824821872767783}\n",
        "Break Balance:\n",
        "{'num_leaves': 27, 'max_depth': 11, 'learning_rate': 0.18696777761444966, 'min_data_in_leaf': 73, 'feature_fraction': 0.964098272670725, 'bagging_fraction': 0.8852821315081366, 'bagging_freq': 10, 'lambda_l1': 3.486399193949678, 'lambda_l2': 3.7053586457221366}"
      ],
      "metadata": {
        "id": "JCrHVq0gAq5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code to build Model for each Car-Param."
      ],
      "metadata": {
        "id": "a-gKI-dEDyTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"_newnames\" explained:**\n",
        "because of how the following code works, the coloums in the Data need to have names, that fit a certain scheme. The coloums cant have blank spaces between words, the blank spaces can also not be replaced with underscores (_).\n",
        "Also the trained Models need to have the exact same name, as their corresponding car-parameter.\n",
        "\n",
        "Rear Wing: RearWing\n",
        "\n",
        "Front Wing: FrontWing\n",
        "\n",
        "Brake: BrakeBalance\n",
        "\n",
        "This needs to be done for Simulator and Practice_Data."
      ],
      "metadata": {
        "id": "N9OSmZoQEtwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 281,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.04083240077095524,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9534759457760267,\n",
        "    'bagging_fraction': 0.6637656869307713,\n",
        "    'bagging_freq': 7,\n",
        "    'lambda_l1': 0.48486124811843856,\n",
        "    'lambda_l2': 1.0469435993282974\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Engine_lgbm_model.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Engine_lgbm_model.txt'\")\n"
      ],
      "metadata": {
        "id": "fzDzzWhV-DKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Differential (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 126,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.05725939263903659,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9048274481436259,\n",
        "    'bagging_fraction': 0.6871315558035545,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 2.9809700145880162,\n",
        "    'lambda_l2': 2.893530172381254\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Differential_lgbm_model.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Differential_lgbm_model.txt'\")\n"
      ],
      "metadata": {
        "id": "R0aao5t5EOOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rear Wing (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 41,\n",
        "    'max_depth': 14,\n",
        "    'learning_rate': 0.07379289439034513,\n",
        "    'min_data_in_leaf': 14,\n",
        "    'feature_fraction': 0.9805583874353474,\n",
        "    'bagging_fraction': 0.757494844528942,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 3.3280185770912905,\n",
        "    'lambda_l2': 4.668868493827121\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"RearWing_lgbm_model.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'RearWing_lgbm_model.txt'\")\n"
      ],
      "metadata": {
        "id": "beEB37hDEOfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Front Wing (hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 228,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.10991909073754202,\n",
        "    'min_data_in_leaf': 94,\n",
        "    'feature_fraction': 0.954670793846081,\n",
        "    'bagging_fraction': 0.7659843231590522,\n",
        "    'bagging_freq': 1, 'lambda_l1': 3.8616883301361553,\n",
        "    'lambda_l2': 4.896984209036385\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"FrontWing_lgbm_model.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'FrontWing_lgbm_model.txt'\")\n"
      ],
      "metadata": {
        "id": "rJwvOAD2EOsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Suspension\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': 10,\n",
        "    'learning_rate': 0.14051851191638579,\n",
        "    'min_data_in_leaf': 75,\n",
        "    'feature_fraction': 0.999888882811791,\n",
        "    'bagging_fraction': 0.7009625088481276,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 4.8851994495913145,\n",
        "    'lambda_l2': 3.6824821872767783\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Suspension_lgbm_model.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Suspension_lgbm_model.txt'\")\n"
      ],
      "metadata": {
        "id": "tjumQ9BPEO8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brake Balance (hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 27,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.18696777761444966,\n",
        "    'min_data_in_leaf': 73,\n",
        "    'feature_fraction': 0.964098272670725,\n",
        "    'bagging_fraction': 0.8852821315081366,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 3.486399193949678,\n",
        "    'lambda_l2': 3.7053586457221366\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"BrakeBalance_lgbm_model.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'BrakeBalance_lgbm_model.txt'\")\n"
      ],
      "metadata": {
        "id": "GWo_8vq7EPNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fifth Step: Optimize Models using Practice_Data"
      ],
      "metadata": {
        "id": "QgJHT3_ZS1m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Model names\n",
        "model_names = [\n",
        "    \"Engine_lgbm_model.txt\",\n",
        "    \"Differential_lgbm_model.txt\",\n",
        "    \"RearWing_lgbm_model.txt\",\n",
        "    \"FrontWing_lgbm_model.txt\",\n",
        "    \"Suspension_lgbm_model.txt\",\n",
        "    \"BrakeBalance_lgbm_model.txt\"\n",
        "]\n",
        "\n",
        "# Corresponding feature sets\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load new data\n",
        "df = pd.read_csv(\"practice_data_newnamesV2_5.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Target variable\n",
        "y_new = df[\"Avg Speed\"]\n",
        "\n",
        "# Sample weight\n",
        "sample_weight = [1000] * len(y_new)\n",
        "\n",
        "# Training each model with its feature set\n",
        "for model_file in model_names:\n",
        "    model_key = model_file.split(\"_\")[0]  # Extract the prefix e.g., \"Engine\"\n",
        "    features = feature_sets[model_key]\n",
        "\n",
        "    print(f\"\\nUpdating {model_file} with features: {features}\")\n",
        "\n",
        "    # Prepare feature matrix\n",
        "    X_new = df[features]\n",
        "\n",
        "    # Load existing model\n",
        "    model = lgb.Booster(model_file=model_file)\n",
        "\n",
        "    # Create dataset\n",
        "    new_data = lgb.Dataset(X_new, label=y_new, weight=sample_weight)\n",
        "\n",
        "    # Continue training\n",
        "    updated_model = lgb.train(\n",
        "        params={},\n",
        "        train_set=new_data,\n",
        "        init_model=model,\n",
        "        num_boost_round=1000\n",
        "    )\n",
        "\n",
        "    # Save updated model\n",
        "    updated_file = model_file.replace(\".txt\", \"_updated.txt\")\n",
        "    updated_model.save_model(updated_file)\n",
        "\n",
        "    print(f\"Model saved to {updated_file}\")\n",
        "\n",
        "print(\"\\nAll models updated successfully.\")\n"
      ],
      "metadata": {
        "id": "3UVmIWEOTC2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sixth Step: Write Code, that sequentially optimizes Car-Parameters with existing Models"
      ],
      "metadata": {
        "id": "vU4q8VKYUcSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "# Ordered list of parameters to optimize\n",
        "params_to_optimize = [\"Engine\", \"Differential\", \"RearWing\", \"FrontWing\", \"Suspension\", \"BrakeBalance\"]\n",
        "\n",
        "# Corresponding feature sets for each model\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load static track & weather data\n",
        "base_data = pd.read_csv(\"track_weather_germany.csv\")\n",
        "\n",
        "# This will hold the best parameter values as we optimize them\n",
        "optimized_params = {}\n",
        "\n",
        "# Begin sequential optimization\n",
        "for car_part in params_to_optimize:\n",
        "    print(f\"\\n🔧 Optimizing {car_part}...\")\n",
        "\n",
        "    model_path = f\"{car_part}_lgbm_model_updated.txt\"\n",
        "    model = lgb.Booster(model_file=model_path)\n",
        "    features = feature_sets[car_part]\n",
        "\n",
        "    def objective(trial):\n",
        "        # Set the current parameter being optimized\n",
        "\n",
        "        params = {\n",
        "            \"Engine\": trial.suggest_int(\"Engine\", 85, 115),\n",
        "            \"RearWing\": trial.suggest_int(\"RearWing\", 499, 500),\n",
        "            \"FrontWing\": trial.suggest_int(\"FrontWing\", 70, 90),\n",
        "            \"BrakeBalance\": trial.suggest_int(\"BrakeBalance\", 85, 115),\n",
        "            \"Suspension\": trial.suggest_int(\"Suspension\", 20, 50),\n",
        "            \"Differential\": trial.suggest_int(\"Differential\", 68, 98),}\n",
        "\n",
        "        current_value = params[car_part]\n",
        "\n",
        "        # Create a single row input with all needed features\n",
        "        input_data = base_data.copy()\n",
        "\n",
        "        # Add previously optimized params\n",
        "        for param, value in optimized_params.items():\n",
        "            input_data[param] = value\n",
        "\n",
        "        # Add current trial value\n",
        "        input_data[car_part] = current_value\n",
        "\n",
        "        # If any missing feature, fill with 0 or a safe default\n",
        "        for col in features:\n",
        "            if col not in input_data.columns:\n",
        "                input_data[col] = 0\n",
        "\n",
        "        # Ensure correct order of features\n",
        "        X = input_data[features]\n",
        "\n",
        "        # Predict Avg Speed\n",
        "        avg_speed = model.predict(X)[0]\n",
        "        return avg_speed  # Optuna maximizes this\n",
        "\n",
        "    # Run Optuna\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=300, show_progress_bar=True)\n",
        "\n",
        "    best_value = study.best_params[car_part]\n",
        "    optimized_params[car_part] = best_value\n",
        "    print(f\"✅ Best {car_part}: {best_value}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n🎯 Final Optimized Parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "iOUiyfYXUaJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "477\t166\t89\t97\t83\t35\n",
        "\n",
        "🎯 Final Optimized Parameters: (model without update)\n",
        "Engine: 91\n",
        "Differential: 101\n",
        "RearWing: 489\n",
        "FrontWing: 274\n",
        "Suspension: 45\n",
        "BrakeBalance: 101\n",
        "\n",
        "🎯 Final Optimized Parameters:(model with update)\n",
        "Engine: 32\n",
        "Differential: 1\n",
        "RearWing: 43\n",
        "FrontWing: 68\n",
        "Suspension: 29\n",
        "BrakeBalance: 268"
      ],
      "metadata": {
        "id": "syK5a_0KipDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Race Strategy"
      ],
      "metadata": {
        "id": "QfAjryBIwS2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the tire parameters and their lap time formulas\n",
        "def lap_time_super_soft(X):\n",
        "    return (67.1830587725926) + 0.18352607407407 * X\n",
        "\n",
        "def lap_time_soft(X):\n",
        "    return (67.5640573104347) + 0.140194202898552 * X\n",
        "\n",
        "def lap_time_medium(X):\n",
        "    return (68.2222646857471) + 0.142527494252874 * X\n",
        "\n",
        "def lap_time_hard(X):\n",
        "    return (68.9335449948148) + 0.108381851851852 * X\n",
        "\n",
        "# Tire data with their lifespan\n",
        "tire_lifespan = {\n",
        "    \"super_soft\": 9,\n",
        "    \"soft\": 23,\n",
        "    \"medium\": 29,\n",
        "    \"hard\": 36\n",
        "}\n",
        "\n",
        "# Pit stop penalty\n",
        "pit_stop_time = 30  # seconds\n",
        "\n",
        "# Function to calculate the total race time for a given strategy\n",
        "def calculate_race_time(laps, strategy):\n",
        "    total_time = 0\n",
        "    total_pit_stops = 0\n",
        "    lap_index = 0\n",
        "    lap_counter = 0\n",
        "\n",
        "    while lap_counter < laps:\n",
        "        tire, stint_laps = strategy[lap_index]\n",
        "\n",
        "        # Ensure we don't exceed the total laps\n",
        "        if lap_counter + stint_laps > laps:\n",
        "            stint_laps = laps - lap_counter\n",
        "\n",
        "        # Calculate the lap times for this stint\n",
        "        lap_times = []\n",
        "        for i in range(stint_laps):\n",
        "            if tire == \"super_soft\":\n",
        "                lap_times.append(lap_time_super_soft(i + 1))\n",
        "            elif tire == \"soft\":\n",
        "                lap_times.append(lap_time_soft(i + 1))\n",
        "            elif tire == \"medium\":\n",
        "                lap_times.append(lap_time_medium(i + 1))\n",
        "            elif tire == \"hard\":\n",
        "                lap_times.append(lap_time_hard(i + 1))\n",
        "\n",
        "        total_time += sum(lap_times)  # Add the lap times of this stint\n",
        "        lap_counter += stint_laps\n",
        "\n",
        "        # If we are not at the last stint, account for a pit stop\n",
        "        if lap_counter < laps:\n",
        "            total_time += pit_stop_time  # Pit stop penalty\n",
        "            total_pit_stops += 1\n",
        "\n",
        "        lap_index += 1\n",
        "        if lap_index >= len(strategy):\n",
        "            break\n",
        "\n",
        "    return total_time, total_pit_stops\n",
        "\n",
        "# Function to generate possible strategies dynamically\n",
        "def generate_strategies(laps):\n",
        "    strategies = []\n",
        "    tire_choices = [\"super_soft\", \"soft\", \"medium\", \"hard\"]\n",
        "\n",
        "    # Generate strategies by breaking the laps into multiple stints\n",
        "    for tire1 in tire_choices:\n",
        "        for tire2 in tire_choices:\n",
        "            for tire3 in tire_choices:\n",
        "                for tire4 in tire_choices:\n",
        "                  for tire5 in tire_choices:\n",
        "                    strategy = []\n",
        "                    remaining_laps = laps\n",
        "\n",
        "                    # Create dynamic stints for each tire\n",
        "                    for tire in [tire1, tire2, tire3, tire4, tire5]:\n",
        "                    #for tire in [tire1, tire2, tire3, tire4]:\n",
        "                    #for tire in [tire1, tire2, tire3]:\n",
        "                    #for tire in [tire1, tire2]:\n",
        "                        stint_laps = tire_lifespan[tire]\n",
        "\n",
        "                        if remaining_laps > stint_laps:\n",
        "                            strategy.append((tire, stint_laps))\n",
        "                            remaining_laps -= stint_laps\n",
        "                        else:\n",
        "                            strategy.append((tire, remaining_laps))\n",
        "                            break\n",
        "\n",
        "                    if sum([stint[1] for stint in strategy]) == laps:\n",
        "                        strategies.append(strategy)\n",
        "\n",
        "    return strategies\n",
        "\n",
        "# Function to find the best strategy\n",
        "def optimize_strategy(laps):\n",
        "    best_time = math.inf\n",
        "    best_strategy = None\n",
        "\n",
        "    strategies = generate_strategies(laps)\n",
        "\n",
        "    for strategy in strategies:\n",
        "        total_time, pit_stops = calculate_race_time(laps, strategy)\n",
        "        if total_time < best_time:\n",
        "            best_time = total_time\n",
        "            best_strategy = strategy\n",
        "            best_pit_stops = pit_stops\n",
        "\n",
        "    return best_strategy, best_time, best_pit_stops\n",
        "\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    race_laps = 85\n",
        "    best_strategy, best_time, total_pit_stops = optimize_strategy(race_laps)\n",
        "    print(f\"Best Strategy: {best_strategy}\")\n",
        "    print(f\"Best Total Time: {best_time} seconds\")\n",
        "    print(f\"Total Pit Stops: {total_pit_stops}\")"
      ],
      "metadata": {
        "id": "Mtb9mV-1wVmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best Strategy: [('soft', 23), ('soft', 23), ('soft', 23), ('soft', 14)]\n",
        "Best Total Time: 5828.617948070432 seconds\n",
        "Total Pit Stops: 3"
      ],
      "metadata": {
        "id": "CS15anDxxnQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZneIB62QL5T"
      },
      "source": [
        "# **Analytics for Race 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Race 3, we aimed to build upon our results from Race 2.  \n",
        "In the analytics for Race 2, we retrained the LightGBM models using data from our practice laps.  \n",
        "\n",
        "We identified a potential issue: retaining these models on this different type of data could lead to changes in the optimal hyperparameters. Therefore, we decided to optimize the hyperparameters of the models by incorporating the practice data.\n",
        "\n",
        "\n",
        "We applied this approach to both our Sequential Optimization Models and the AllInOne Model.\n",
        "\n",
        "\n",
        "To do this, we combined the datasets using `pd.concat` and assigned a weight to each dataset.  \n",
        "The simulation data was given a weight of 1.  \n",
        "To ensure the practice data was sufficiently influential in the model training, we calculated its weight by dividing the number of lines in the simulation data by the number of lines in the practice data (for example, 10000 / 40 = 250).\n",
        "\n",
        "Aside from this, we followed the same process as in Race 2 to optimize the car parameters and race strategy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Foreshadowing:**\n",
        "In the feedback session after Race 3, we learned that other teams used much lower dataset weights and achieved better results. Upon further research, we discovered that assigning excessively high weights to certain datasets—such as the practice data—may not be the most effective strategy. While increasing the weight of practice data highlights its importance during model training, overly high weights can lead to overfitting. This causes the model to become too tailored to that specific data and diminishes its ability to generalize well.\n",
        "\n",
        "As a result, we decided to prioritize a more balanced approach to dataset weighting for Race 4, aiming to improve the robustness and generalization of our models.\n"
      ],
      "metadata": {
        "id": "t4xTOoPfKExa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI1U47bawAhA"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Paula Kussauer, Cedric Schwandt, Hannes Kock"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Optimization"
      ],
      "metadata": {
        "id": "IFBPGD5iF9Qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Optimization Sim Prac Combo"
      ],
      "metadata": {
        "id": "jPfGsw1syZAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine SimPrac Hyperparameter Optimization\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV3_3.csv\")\n",
        "\n",
        "# Calculate target: Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0  # Adjust this ratio to control importance (10000/40 = 250)\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"weight\"]\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights.iloc[train_idx], weights.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=1000,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=250)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "FrYgFIWpyeHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Differential SimPrac Hyperparameter Optimization\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV3_3.csv\")\n",
        "\n",
        "# Calculate target: Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0  # Adjust this ratio to control importance (10000/40 = 250)\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"weight\"]\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights.iloc[train_idx], weights.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=1000,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=250)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "OV_UVw6z0Bx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rear Wing SimPrac Hyperparameter Optimization\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV3_3.csv\")\n",
        "\n",
        "# Calculate target: Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0  # Adjust this ratio to control importance (10000/40 = 250)\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    X = df[['Rear Wing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"weight\"]\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights.iloc[train_idx], weights.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=1000,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=250)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "nLpv6BY21Ie5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Front Wing SimPrac Hyperparameter Optimization\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV3_3.csv\")\n",
        "\n",
        "# Calculate target: Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0  # Adjust this ratio to control importance (10000/40 = 250)\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    X = df[['Front Wing', 'Rear Wing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"weight\"]\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights.iloc[train_idx], weights.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=1000,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=250)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "wBbm3-Vs1az2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Suspension SimPrac Hyperparameter Optimization\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV3_3.csv\")\n",
        "\n",
        "# Calculate target: Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0  # Adjust this ratio to control importance (10000/40 = 250)\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    X = df[['Suspension', 'Front Wing', 'Rear Wing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"weight\"]\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights.iloc[train_idx], weights.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=1000,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=250)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "gSEmVmaq1i0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brake Balance SimPrac Hyperparameter Optimization\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV3_3.csv\")\n",
        "\n",
        "# Calculate target: Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0  # Adjust this ratio to control importance (10000/40 = 250)\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    X = df[['Brake Balance', 'Suspension', 'Front Wing', 'Rear Wing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"weight\"]\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights.iloc[train_idx], weights.iloc[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=1000,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=300)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)\n"
      ],
      "metadata": {
        "id": "f_X9K3B-1tm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Brake Balance: \tX = df[['Brake_Balance', 'Suspension', 'Front_Wing', 'Rear_Wing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "Suspension: \tX = df[['Suspension', 'Front_Wing', 'Rear_Wing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "Front Wing: \tX = df[['Front_Wing', 'Rear_Wing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "Rear Wing: \tX = df[['Rear_Wing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "Differential: \tX = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "Engine: \tX = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]"
      ],
      "metadata": {
        "id": "3Q9vSV_E06Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and saving LGBM-Models for each Car-Parameter (SimPrac Combo)"
      ],
      "metadata": {
        "id": "cXZtPpGb2pcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Best Engine trial:\n",
        "{'num_leaves': 46, 'max_depth': 5, 'learning_rate': 0.033632492994555285, 'min_data_in_leaf': 10, 'feature_fraction': 0.9292816238478901, 'bagging_fraction': 0.6029213489987132, 'bagging_freq': 1, 'lambda_l1': 4.9435895890172095, 'lambda_l2': 1.5835241201372334}\n",
        "\n",
        "Best Differential trial:\n",
        "{'num_leaves': 120, 'max_depth': 4, 'learning_rate': 0.06188740190651351, 'min_data_in_leaf': 10, 'feature_fraction': 0.8516027132719757, 'bagging_fraction': 0.7665333052378347, 'bagging_freq': 2, 'lambda_l1': 4.842140375323366, 'lambda_l2': 0.8572408057266877}\n",
        "\n",
        "Best Rear Wing trial:\n",
        "{'num_leaves': 234, 'max_depth': 8, 'learning_rate': 0.009001605992864, 'min_data_in_leaf': 12, 'feature_fraction': 0.7155500136774263, 'bagging_fraction': 0.9782633351035283, 'bagging_freq': 3, 'lambda_l1': 4.0691300429761625, 'lambda_l2': 0.4270601685583596}\n",
        "\n",
        "Best Front Win trail:\n",
        "{'num_leaves': 75, 'max_depth': 15, 'learning_rate': 0.008393206404444, 'min_data_in_leaf': 10, 'feature_fraction': 0.636593633990795, 'bagging_fraction': 0.8763178958731057, 'bagging_freq': 1, 'lambda_l1': 3.37372395608384, 'lambda_l2': 3.5508540427910322}\n",
        "\n",
        "Best Suspension Trail:\n",
        "{'num_leaves': 94, 'max_depth': 7, 'learning_rate': 0.053319349444258306, 'min_data_in_leaf': 11, 'feature_fraction': 0.7588234798454238, 'bagging_fraction': 0.9663253274432543, 'bagging_freq': 1, 'lambda_l1': 4.443326692620447, 'lambda_l2': 2.017979694136418}\n",
        "\n",
        "Best Brake Balance Trail:\n",
        "{'num_leaves': 52, 'max_depth': 14, 'learning_rate': 0.02166755029862027, 'min_data_in_leaf': 10, 'feature_fraction': 0.5221020637935397, 'bagging_fraction': 0.938010385446846, 'bagging_freq': 1, 'lambda_l1': 2.531857885425292, 'lambda_l2': 4.788757729441045}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QnF8gtni22fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine (Hyperparameters and X set) Check\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "prac_df = pd.read_csv(\"practice_data_newnamesV3_2.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0\n",
        "\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Features, target, and weights\n",
        "X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 46,\n",
        "    'max_depth': 5,\n",
        "    'learning_rate': 0.033632492994555285,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.9292816238478901,\n",
        "    'bagging_fraction': 0.6029213489987132,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 4.9435895890172095,\n",
        "    'lambda_l2': 1.5835241201372334\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Engine_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Engine_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "NWgx-I8k22jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Differential (Hyperparameters and X set) check\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "prac_df = pd.read_csv(\"practice_data_newnamesV3_2.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0\n",
        "\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Features, target, and weights\n",
        "X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 120,\n",
        "    'max_depth': 4,\n",
        "    'learning_rate': 0.06188740190651351,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.8516027132719757,\n",
        "    'bagging_fraction': 0.7665333052378347,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 4.842140375323366,\n",
        "    'lambda_l2': 0.8572408057266877\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Differential_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Differential_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "I7YlEQUp22jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rear Wing (Hyperparameters and X set) check\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "prac_df = pd.read_csv(\"practice_data_newnamesV3_2.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0\n",
        "\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Features, target, and weights\n",
        "X = df[['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 234,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.009001605992864,\n",
        "    'min_data_in_leaf': 12,\n",
        "    'feature_fraction': 0.7155500136774263,\n",
        "    'bagging_fraction': 0.9782633351035283,\n",
        "    'bagging_freq': 3,\n",
        "    'lambda_l1': 4.0691300429761625,\n",
        "    'lambda_l2': 0.4270601685583596}\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"RearWing_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'RearWing_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "9oGqRigD22jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Front Wing (hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "prac_df = pd.read_csv(\"practice_data_newnamesV3_2.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0\n",
        "\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Features, target, and weights\n",
        "X = df[['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 75,\n",
        "    'max_depth': 15,\n",
        "    'learning_rate': 0.008393206404444,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.636593633990795,\n",
        "    'bagging_fraction': 0.8763178958731057,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 3.37372395608384,\n",
        "    'lambda_l2': 3.5508540427910322\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"FrontWing_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'FrontWing_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "ieCZQARc22jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Suspension\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "prac_df = pd.read_csv(\"practice_data_newnamesV3_2.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0\n",
        "\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Features, target, and weights\n",
        "X = df[['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 52,\n",
        "    'max_depth': 14,\n",
        "    'learning_rate': 0.02166755029862027,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.5221020637935397,\n",
        "    'bagging_fraction': 0.938010385446846,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 2.531857885425292,\n",
        "    'lambda_l2': 4.788757729441045\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Suspension_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Suspension_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "z34fB37822jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brake Balance (hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "prac_df = pd.read_csv(\"practice_data_newnamesV3_2.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0\n",
        "\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Features, target, and weights\n",
        "X = df[['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 27,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.18696777761444966,\n",
        "    'min_data_in_leaf': 73,\n",
        "    'feature_fraction': 0.964098272670725,\n",
        "    'bagging_fraction': 0.8852821315081366,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 3.486399193949678,\n",
        "    'lambda_l2': 3.7053586457221366\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"BrakeBalance_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'BrakeBalance_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "qqzx4ff922jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and saving LGBM-Models for each Car-Parameter (Not SimPrac Combo)"
      ],
      "metadata": {
        "id": "vdpD1FdhGKsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 281,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.04083240077095524,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9534759457760267,\n",
        "    'bagging_fraction': 0.6637656869307713,\n",
        "    'bagging_freq': 7,\n",
        "    'lambda_l1': 0.48486124811843856,\n",
        "    'lambda_l2': 1.0469435993282974\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Engine_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Engine_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "kBMC0Tg7GCxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Differential (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 126,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.05725939263903659,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9048274481436259,\n",
        "    'bagging_fraction': 0.6871315558035545,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 2.9809700145880162,\n",
        "    'lambda_l2': 2.893530172381254\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Differential_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Differential_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "eUDpQqMbGCxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rear Wing (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 41,\n",
        "    'max_depth': 14,\n",
        "    'learning_rate': 0.07379289439034513,\n",
        "    'min_data_in_leaf': 14,\n",
        "    'feature_fraction': 0.9805583874353474,\n",
        "    'bagging_fraction': 0.757494844528942,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 3.3280185770912905,\n",
        "    'lambda_l2': 4.668868493827121\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"RearWing_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'RearWing_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "yqQMVK1pGCxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Front Wing (hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 228,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.10991909073754202,\n",
        "    'min_data_in_leaf': 94,\n",
        "    'feature_fraction': 0.954670793846081,\n",
        "    'bagging_fraction': 0.7659843231590522,\n",
        "    'bagging_freq': 1, 'lambda_l1': 3.8616883301361553,\n",
        "    'lambda_l2': 4.896984209036385\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"FrontWing_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'FrontWing_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "8oUuJxikGCxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Suspension\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': 10,\n",
        "    'learning_rate': 0.14051851191638579,\n",
        "    'min_data_in_leaf': 75,\n",
        "    'feature_fraction': 0.999888882811791,\n",
        "    'bagging_fraction': 0.7009625088481276,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 4.8851994495913145,\n",
        "    'lambda_l2': 3.6824821872767783\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Suspension_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Suspension_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "hkBpHrzUGCxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brake Balance (hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 27,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.18696777761444966,\n",
        "    'min_data_in_leaf': 73,\n",
        "    'feature_fraction': 0.964098272670725,\n",
        "    'bagging_fraction': 0.8852821315081366,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 3.486399193949678,\n",
        "    'lambda_l2': 3.7053586457221366\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"BrakeBalance_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'BrakeBalance_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "LWq7AZI4GCxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing Models using Practice Data"
      ],
      "metadata": {
        "id": "xRwa1wATpFuc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kO_PDP2UpD-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Model names\n",
        "model_names = [\n",
        "    \"Engine_lgbm_model_R3.txt\",\n",
        "    \"Differential_lgbm_model_R3.txt\",\n",
        "    \"RearWing_lgbm_model_R3.txt\",\n",
        "    \"FrontWing_lgbm_model_R3.txt\",\n",
        "    \"Suspension_lgbm_model_R3.txt\",\n",
        "    \"BrakeBalance_lgbm_model_R3.txt\"\n",
        "]\n",
        "\n",
        "# Corresponding feature sets\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load new data\n",
        "df = pd.read_csv(\"practice_data_newnamesV3_1.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Target variable\n",
        "y_new = df[\"Avg Speed\"]\n",
        "\n",
        "# Sample weight\n",
        "sample_weight = [1000] * len(y_new)\n",
        "\n",
        "# Training each model with its feature set\n",
        "for model_file in model_names:\n",
        "    model_key = model_file.split(\"_\")[0]  # Extract the prefix e.g., \"Engine\"\n",
        "    features = feature_sets[model_key]\n",
        "\n",
        "    print(f\"\\nUpdating {model_file} with features: {features}\")\n",
        "\n",
        "    # Prepare feature matrix\n",
        "    X_new = df[features]\n",
        "\n",
        "    # Load existing model\n",
        "    model = lgb.Booster(model_file=model_file)\n",
        "\n",
        "    # Create dataset\n",
        "    new_data = lgb.Dataset(X_new, label=y_new, weight=sample_weight)\n",
        "\n",
        "    # Continue training\n",
        "    updated_model = lgb.train(\n",
        "        params={},\n",
        "        train_set=new_data,\n",
        "        init_model=model,\n",
        "        num_boost_round=1000\n",
        "    )\n",
        "\n",
        "    # Save updated model\n",
        "    updated_file = model_file.replace(\".txt\", \"_updated.txt\")\n",
        "    updated_model.save_model(updated_file)\n",
        "\n",
        "    print(f\"Model saved to {updated_file}\")\n",
        "\n",
        "print(\"\\nAll models updated successfully.\")\n"
      ],
      "metadata": {
        "id": "Sfes5lvwpEe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequentially optimizing Car Parameters."
      ],
      "metadata": {
        "id": "YNPkq8hpIhzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "# Ordered list of parameters to optimize\n",
        "params_to_optimize = [\"Engine\", \"Differential\", \"RearWing\", \"FrontWing\", \"Suspension\", \"BrakeBalance\"]\n",
        "\n",
        "# Corresponding feature sets for each model\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load static track & weather data\n",
        "base_data = pd.read_csv(\"track_weather_netherlands.csv\")\n",
        "\n",
        "# This will hold the best parameter values as we optimize them\n",
        "optimized_params = {}\n",
        "\n",
        "# Begin sequential optimization\n",
        "for car_part in params_to_optimize:\n",
        "    print(f\"\\n🔧 Optimizing {car_part}...\")\n",
        "\n",
        "    model_path = f\"{car_part}_lgbm_model_R3.txt\"\n",
        "    model = lgb.Booster(model_file=model_path)\n",
        "    features = feature_sets[car_part]\n",
        "\n",
        "    def objective(trial):\n",
        "        # Set the current parameter being optimized\n",
        "\n",
        "        params = {\n",
        "            \"Engine\": trial.suggest_int(\"Engine\", 1, 500),\n",
        "            \"RearWing\": trial.suggest_int(\"RearWing\", 1, 500),\n",
        "            \"FrontWing\": trial.suggest_int(\"FrontWing\", 1, 500),\n",
        "            \"BrakeBalance\": trial.suggest_int(\"BrakeBalance\", 1, 500),\n",
        "            \"Suspension\": trial.suggest_int(\"Suspension\", 1, 500),\n",
        "            \"Differential\": trial.suggest_int(\"Differential\", 1, 500),}\n",
        "\n",
        "        current_value = params[car_part]\n",
        "\n",
        "        # Create a single row input with all needed features\n",
        "        input_data = base_data.copy()\n",
        "\n",
        "        # Add previously optimized params\n",
        "        for param, value in optimized_params.items():\n",
        "            input_data[param] = value\n",
        "\n",
        "        # Add current trial value\n",
        "        input_data[car_part] = current_value\n",
        "\n",
        "        # If any missing feature, fill with 0 or a safe default\n",
        "        for col in features:\n",
        "            if col not in input_data.columns:\n",
        "                input_data[col] = 0\n",
        "\n",
        "        # Ensure correct order of features\n",
        "        X = input_data[features]\n",
        "\n",
        "        # Predict Avg Speed\n",
        "        avg_speed = model.predict(X)[0]\n",
        "        return avg_speed  # Optuna maximizes this\n",
        "\n",
        "    # Run Optuna\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=500, show_progress_bar=True)\n",
        "\n",
        "    best_value = study.best_params[car_part]\n",
        "    optimized_params[car_part] = best_value\n",
        "    print(f\"✅ Best {car_part}: {best_value}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n🎯 Final Optimized Parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "5PvVj0gTIrPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "🎯 Final Optimized Parameters:\n",
        "Engine: 77\n",
        "Differential: 101\n",
        "RearWing: 161\n",
        "FrontWing: 80\n",
        "Suspension: 41\n",
        "BrakeBalance: 68\n",
        "\n",
        "🎯 Final Optimized Parameters:\n",
        "Engine: 45\n",
        "Differential: 35\n",
        "RearWing: 233\n",
        "FrontWing: 72\n",
        "Suspension: 160\n",
        "BrakeBalance: 53"
      ],
      "metadata": {
        "id": "-o_VKmAcknpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "# Ordered list of car parameters to optimize\n",
        "params_to_optimize = [\n",
        "    \"Engine\", \"Differential\", \"RearWing\",\n",
        "    \"FrontWing\", \"Suspension\", \"BrakeBalance\"\n",
        "]\n",
        "\n",
        "# Feature sets used by each model for prediction\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load static base data (track & weather)\n",
        "base_data = pd.read_csv(\"track_weather_netherlands.csv\")\n",
        "\n",
        "# Dictionary to store optimized values\n",
        "optimized_params = {}\n",
        "\n",
        "# Sequential optimization loop\n",
        "for car_part in params_to_optimize:\n",
        "    print(f\"\\n🔧 Optimizing {car_part}...\")\n",
        "\n",
        "    # Load the pretrained LightGBM model\n",
        "    model = lgb.Booster(model_file=f\"{car_part}_lgbm_model_R3.txt\")\n",
        "    features = feature_sets[car_part]\n",
        "\n",
        "    def objective(trial):\n",
        "        # Clone base data for this trial\n",
        "        input_data = base_data.copy()\n",
        "\n",
        "        # Add fixed parameters (already optimized)\n",
        "        for param, value in optimized_params.items():\n",
        "            input_data[param] = value\n",
        "\n",
        "        # Suggest value only for the current parameter\n",
        "        trial_value = trial.suggest_int(car_part, 1, 500)\n",
        "        input_data[car_part] = trial_value\n",
        "\n",
        "        # Fill in missing feature columns with default (0)\n",
        "        for feature in features:\n",
        "            if feature not in input_data.columns:\n",
        "                input_data[feature] = 0\n",
        "\n",
        "        # Predict average speed\n",
        "        X = input_data[features]\n",
        "        avg_speed = model.predict(X)[0]\n",
        "        return avg_speed\n",
        "\n",
        "    # Run Optuna optimization for this parameter\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "    # Save best value\n",
        "    best_value = study.best_params[car_part]\n",
        "    optimized_params[car_part] = best_value\n",
        "    print(f\"✅ Best {car_part}: {best_value}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n🎯 Final Optimized Parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "a665N6bAmqWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AllInOne Optimization"
      ],
      "metadata": {
        "id": "3o5jS5VRDBwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"best_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'best_lgbm_model_R3.txt'\")\n"
      ],
      "metadata": {
        "id": "CsuSmP2-DIbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Directly trained LGBM-Model with Simulator_Data and Practice_data using optm. Hyperparameters\n",
        "#Hyperparameters optm. for Simulator and Practice_data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load both datasets\n",
        "sim_df = pd.read_csv(\"simulator_data.csv\")\n",
        "prac_df = pd.read_csv(\"practice_dataV3_2.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "sim_df[\"Avg Speed\"] = (sim_df[\"Lap Distance\"] / sim_df[\"Lap Time\"]) * 3600\n",
        "prac_df[\"Avg Speed\"] = (prac_df[\"Lap Distance\"] / prac_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Assign weights\n",
        "sim_df[\"weight\"] = 1.0\n",
        "prac_df[\"weight\"] = 250.0\n",
        "\n",
        "# Make sure both have the same columns in the same order\n",
        "required_cols = [\"Lap Distance\", \"Lap Time\", \"Avg Speed\", \"weight\", \"Rear Wing\", \"Front Wing\", \"Engine\", \"Brake Balance\", \"Differential\", \"Suspension\", \"Cornering\", \"Inclines\", \"Camber\", \"Grip\", \"Altitude\", \"Roughness\", \"Width\", \"Temperature\", \"Humidity\", \"Wind (Avg. Speed)\", \"Wind (Gusts)\", \"Air Density\", \"Air Pressure\"] + \\\n",
        "                [col for col in sim_df.columns if col not in [\"Lap Distance\", \"Lap Time\", \"Avg Speed\", \"weight\", \"Rear Wing\", \"Front Wing\", \"Engine\", \"Brake Balance\", \"Differential\", \"Suspension\", \"Cornering\", \"Inclines\", \"Camber\", \"Grip\", \"Altitude\", \"Roughness\", \"Width\", \"Temperature\", \"Humidity\", \"Wind (Avg. Speed)\", \"Wind (Gusts)\", \"Air Density\", \"Air Pressure\"]]\n",
        "\n",
        "sim_df = sim_df[required_cols]\n",
        "prac_df = prac_df[required_cols]\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([sim_df, prac_df], ignore_index=True)\n",
        "\n",
        "# Features, target, and weights\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\", \"weight\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"weight\"]\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (already tuned)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 74,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.028870630893942515,\n",
        "    'min_data_in_leaf': 13,\n",
        "    'feature_fraction': 0.7399238619191133,\n",
        "    'bagging_fraction': 0.9203471068533383,\n",
        "    'bagging_freq': 6,\n",
        "    'lambda_l1': 0.025758523534949052,\n",
        "    'lambda_l2': 0.9073066525703107}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"sim_prac_lgbm_model_R3.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'sim_prac_lgbm_model_R3.txt'\")"
      ],
      "metadata": {
        "id": "cXVlcXoLuwI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the original model\n",
        "model = lgb.Booster(model_file=\"best_lgbm_model_R3.txt\")\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "}\n",
        "# Load the new data\n",
        "new_df = pd.read_csv(\"practice_dataV3_1.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "new_df[\"Avg Speed\"] = (new_df[\"Lap Distance\"] / new_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Prepare features and target\n",
        "X_new = new_df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\",\"Round\", \"Track\", \"Qualifying\", \"Stint\", \"Lap\", \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"])\n",
        "y_new = new_df[\"Avg Speed\"]\n",
        "\n",
        "# Assign higher weights (e.g., 10x more important)\n",
        "sample_weight = [5] * len(y_new)\n",
        "\n",
        "# Create new LightGBM Dataset\n",
        "new_data = lgb.Dataset(X_new, label=y_new, weight=sample_weight)\n",
        "\n",
        "# Continue training from previous model\n",
        "model = lgb.train(\n",
        "    params={},  # Empty here since model already knows them\n",
        "    train_set=new_data,\n",
        "    init_model=model,\n",
        "    num_boost_round=1000,  # You can increase if needed\n",
        ")\n",
        "\n",
        "# Save updated model\n",
        "model.save_model(\"best_lgbm_model_R3_updated.txt\")\n",
        "\n",
        "print(\"Model updated with new data and saved to 'best_lgbm_model_R3_updated.txt'\")\n"
      ],
      "metadata": {
        "id": "OKMKqN_CEjMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "\n",
        "# Load trained model\n",
        "#model = lgb.Booster(model_file=\"best_lgbm_model_R3_updated.txt\")\n",
        "model = lgb.Booster(model_file=\"sim_prac_lgbm_model_R3.txt\")\n",
        "\n",
        "# Load track/weather data (single row)\n",
        "track_weather = pd.read_csv(\"track_weather_netherlands.csv\")\n",
        "\n",
        "# Define the objective function\n",
        "def objective(trial):\n",
        "    # Suggest car parameters\n",
        "    params = {\n",
        "        \"Engine\": trial.suggest_int(\"Engine\", 20, 60),\n",
        "        \"Rear Wing\": trial.suggest_int(\"Rear Wing\", 200, 300),\n",
        "        \"Front Wing\": trial.suggest_int(\"Front Wing\", 48, 88),\n",
        "        \"Brake Balance\": trial.suggest_int(\"Brake Balance\", 18, 118),\n",
        "        \"Suspension\": trial.suggest_int(\"Suspension\", 10, 110),\n",
        "        \"Differential\": trial.suggest_int(\"Differential\", 10, 100),\n",
        "    }\n",
        "\n",
        "#Engine: 45\n",
        "#RearWing: 250\n",
        "#FrontWing: 68\n",
        "#BrakeBalance: 68\n",
        "#Suspension: 60\n",
        "#Differential: 35\n",
        "\n",
        "    # Combine with static track/weather parameters\n",
        "    input_data = pd.concat([track_weather, pd.DataFrame([params])], axis=1)\n",
        "\n",
        "    # Predict average speed\n",
        "    predicted_avg_speed = model.predict(input_data)[0]\n",
        "\n",
        "    # We want to maximize speed\n",
        "    return -predicted_avg_speed\n",
        "\n",
        "# Create study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Output best results\n",
        "best_trial = study.best_trial\n",
        "print(\"\\nBest Parameters:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print(f\"Predicted Avg Speed: {-best_trial.value:.2f}\")"
      ],
      "metadata": {
        "id": "CeWJlFASDbxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Race Strategy"
      ],
      "metadata": {
        "id": "0iREpS-7e4vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the tire parameters and their lap time formulas\n",
        "def lap_time_super_soft(X):\n",
        "    return (78.0565192575555) + 0.114973111111111 * X\n",
        "\n",
        "def lap_time_soft(X):\n",
        "    return (78.8737020908889) + 0.136290277777778 * X\n",
        "\n",
        "def lap_time_medium(X):\n",
        "    return (79.1300868242222) + 0.131553711111111 * X\n",
        "\n",
        "def lap_time_hard(X):\n",
        "    return (79.2600146591429) + 0.137075542857143 * X\n",
        "\n",
        "# Tire data with their lifespan\n",
        "tire_lifespan = {\n",
        "    \"super_soft\": 12,\n",
        "    \"soft\": 24,\n",
        "    \"medium\": 30,\n",
        "    \"hard\": 35\n",
        "}\n",
        "\n",
        "# Pit stop penalty\n",
        "pit_stop_time = 30  # seconds\n",
        "\n",
        "# Function to calculate the total race time for a given strategy\n",
        "def calculate_race_time(laps, strategy):\n",
        "    total_time = 0\n",
        "    total_pit_stops = 0\n",
        "    lap_index = 0\n",
        "    lap_counter = 0\n",
        "\n",
        "    while lap_counter < laps:\n",
        "        tire, stint_laps = strategy[lap_index]\n",
        "\n",
        "        # Ensure we don't exceed the total laps\n",
        "        if lap_counter + stint_laps > laps:\n",
        "            stint_laps = laps - lap_counter\n",
        "\n",
        "        # Calculate the lap times for this stint\n",
        "        lap_times = []\n",
        "        for i in range(stint_laps):\n",
        "            if tire == \"super_soft\":\n",
        "                lap_times.append(lap_time_super_soft(i + 1))\n",
        "            elif tire == \"soft\":\n",
        "                lap_times.append(lap_time_soft(i + 1))\n",
        "            elif tire == \"medium\":\n",
        "                lap_times.append(lap_time_medium(i + 1))\n",
        "            elif tire == \"hard\":\n",
        "                lap_times.append(lap_time_hard(i + 1))\n",
        "\n",
        "        total_time += sum(lap_times)  # Add the lap times of this stint\n",
        "        lap_counter += stint_laps\n",
        "\n",
        "        # If we are not at the last stint, account for a pit stop\n",
        "        if lap_counter < laps:\n",
        "            total_time += pit_stop_time  # Pit stop penalty\n",
        "            total_pit_stops += 1\n",
        "\n",
        "        lap_index += 1\n",
        "        if lap_index >= len(strategy):\n",
        "            break\n",
        "\n",
        "    return total_time, total_pit_stops\n",
        "\n",
        "# Function to generate possible strategies dynamically\n",
        "def generate_strategies(laps):\n",
        "    strategies = []\n",
        "    tire_choices = [\"super_soft\", \"soft\", \"medium\", \"hard\"]\n",
        "\n",
        "    # Generate strategies by breaking the laps into multiple stints\n",
        "    for tire1 in tire_choices:\n",
        "        for tire2 in tire_choices:\n",
        "            for tire3 in tire_choices:\n",
        "                for tire4 in tire_choices:\n",
        "                  for tire5 in tire_choices:\n",
        "                    strategy = []\n",
        "                    remaining_laps = laps\n",
        "\n",
        "                    # Create dynamic stints for each tire\n",
        "                    for tire in [tire1, tire2, tire3, tire4, tire5]:\n",
        "                    #for tire in [tire1, tire2, tire3, tire4]:\n",
        "                    #for tire in [tire1, tire2, tire3]:\n",
        "                    #for tire in [tire1, tire2]:\n",
        "                        stint_laps = tire_lifespan[tire]\n",
        "\n",
        "                        if remaining_laps > stint_laps:\n",
        "                            strategy.append((tire, stint_laps))\n",
        "                            remaining_laps -= stint_laps\n",
        "                        else:\n",
        "                            strategy.append((tire, remaining_laps))\n",
        "                            break\n",
        "\n",
        "                    if sum([stint[1] for stint in strategy]) == laps:\n",
        "                        strategies.append(strategy)\n",
        "\n",
        "    return strategies\n",
        "\n",
        "# Function to find the best strategy\n",
        "def optimize_strategy(laps):\n",
        "    best_time = math.inf\n",
        "    best_strategy = None\n",
        "\n",
        "    strategies = generate_strategies(laps)\n",
        "\n",
        "    for strategy in strategies:\n",
        "        total_time, pit_stops = calculate_race_time(laps, strategy)\n",
        "        if total_time < best_time:\n",
        "            best_time = total_time\n",
        "            best_strategy = strategy\n",
        "            best_pit_stops = pit_stops\n",
        "\n",
        "    return best_strategy, best_time, best_pit_stops\n",
        "\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    race_laps = 75\n",
        "    best_strategy, best_time, total_pit_stops = optimize_strategy(race_laps)\n",
        "    print(f\"Best Strategy: {best_strategy}\")\n",
        "    print(f\"Best Total Time: {best_time} seconds\")\n",
        "    print(f\"Total Pit Stops: {total_pit_stops}\")"
      ],
      "metadata": {
        "id": "dxJNdgFCe4vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFETMTcZQL5T"
      },
      "source": [
        "# **Analytics for Race 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This weeks Focus can be split up into three sections:\n",
        "1.   Tuning weights of Practice_Data     \n",
        "2.   Trying and Tuning Lasso Regression\n",
        "3.   Direct optimization with Selenium\n",
        "\n",
        "**Tuning Weights of Practice_Data**\n",
        "During the Feedback-Session after Race 3 we noticed that the other teams were weighing their practice data differently/lighter within their Models and were getting better results doing this. This lead us to research how the practice_data should be weighed, realising that we had chose to heavy weights in the past. **(Quelle)**\n",
        "\n",
        "Hier einfügen, wo drin steht, wie hoch die Gewichtung maximal sein darf etc.\n",
        "\n",
        "To decide which weights to use with the specific datasets, we build a code, that uses optuna to optimize the model quality based on the weights of the practice_data.\n",
        "\n",
        "Once the weights were tuned, we proceeeded with the optimization process that we already used in the earlier Races.\n",
        "\n",
        "**Trying and Tuning Lasso Regression**\n",
        "Another topic discussed during the feedback session was Lasso Regression, and that Prof. Heitmann had once mentioned that using Lasso Regression could potentially yield good results.  \n",
        "Following this suggestion, we attempted to build a Lasso Regression model and to tune the alpha parameter.  \n",
        "The tuned Lasso Regression model achieved an R² of less than 0.1. Despite this low performance, we proceeded to implement the model into our All-In-One optimizer. Technically, it was possible to integrate, but the suggested parameters did not produce meaningful results.  \n",
        "We did not try to incorporate Lasso Regression into our Sequential Optimizer, as we had low expectations for good outcomes and believed it would be too time-consuming.\n",
        "\n",
        "**Direct Optimization with Selenium**\n",
        "During the analytics for Race 1, we unexpectedly discovered that the practice laps are not limited to 80, as the website suggests. The counter for remaining practice laps continues to count into the negative.  \n",
        "Following this observation, it appears that the practice laps are not actually limited, making it feasible to replace the LightGBM model with direct interaction with the website for optimization.\n",
        "\n",
        "Building on this idea, we discovered Selenium, which allows interaction with the website through a browser window. It then became a matter of writing code that:\n",
        "\n",
        "- Opens the team-analytics.com website,  \n",
        "- Logs in and navigates to the practice page,  \n",
        "- Pastes the car parameters and stint information,  \n",
        "- Clicks the submit practice button,  \n",
        "- Reads the resulting average lap time.  \n",
        "\n",
        "Once these steps were operational, it was straightforward to create callable functions encapsulating this process and integrate them with an optimization routine.\n",
        "\n",
        "This resulted in the code found under \"Direct Optimization with Selenium.\" Please note that this code does not run in Google Colab; it must be executed on a local Python environment.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J3R0pdvnaRhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idO7aGYTwPks"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Paula Kussauer, Cedric Schwandt, Hannes Kock"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Optimization"
      ],
      "metadata": {
        "id": "2BO-gd6doz4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Once again: Train and Save each Model with full Dataset (exactly the same as R2&R3)"
      ],
      "metadata": {
        "id": "km3t2XlQuBRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 281,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.04083240077095524,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9534759457760267,\n",
        "    'bagging_fraction': 0.6637656869307713,\n",
        "    'bagging_freq': 7,\n",
        "    'lambda_l1': 0.48486124811843856,\n",
        "    'lambda_l2': 1.0469435993282974\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Engine_lgbm_model_R4.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Engine_lgbm_model_R4.txt'\")\n"
      ],
      "metadata": {
        "id": "hM4jpfJRpHwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Differential (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 126,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.05725939263903659,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9048274481436259,\n",
        "    'bagging_fraction': 0.6871315558035545,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 2.9809700145880162,\n",
        "    'lambda_l2': 2.893530172381254\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Differential_lgbm_model_R4.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Differential_lgbm_model_R4.txt'\")\n"
      ],
      "metadata": {
        "id": "M31f0dqHpHwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rear Wing (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 41,\n",
        "    'max_depth': 14,\n",
        "    'learning_rate': 0.07379289439034513,\n",
        "    'min_data_in_leaf': 14,\n",
        "    'feature_fraction': 0.9805583874353474,\n",
        "    'bagging_fraction': 0.757494844528942,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 3.3280185770912905,\n",
        "    'lambda_l2': 4.668868493827121\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"RearWing_lgbm_model_R4.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'RearWing_lgbm_model_R4.txt'\")\n"
      ],
      "metadata": {
        "id": "l6InXDlCpHwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Front Wing (hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 228,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.10991909073754202,\n",
        "    'min_data_in_leaf': 94,\n",
        "    'feature_fraction': 0.954670793846081,\n",
        "    'bagging_fraction': 0.7659843231590522,\n",
        "    'bagging_freq': 1, 'lambda_l1': 3.8616883301361553,\n",
        "    'lambda_l2': 4.896984209036385\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"FrontWing_lgbm_model_R4.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'FrontWing_lgbm_model_R4.txt'\")\n"
      ],
      "metadata": {
        "id": "x57b0PjapHwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Suspension\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': 10,\n",
        "    'learning_rate': 0.14051851191638579,\n",
        "    'min_data_in_leaf': 75,\n",
        "    'feature_fraction': 0.999888882811791,\n",
        "    'bagging_fraction': 0.7009625088481276,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 4.8851994495913145,\n",
        "    'lambda_l2': 3.6824821872767783\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"Suspension_lgbm_model_R4.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'Suspension_lgbm_model_R4.txt'\")\n"
      ],
      "metadata": {
        "id": "IjH4YI6HpHwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brake Balance (hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df[['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 27,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.18696777761444966,\n",
        "    'min_data_in_leaf': 73,\n",
        "    'feature_fraction': 0.964098272670725,\n",
        "    'bagging_fraction': 0.8852821315081366,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 3.486399193949678,\n",
        "    'lambda_l2': 3.7053586457221366\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"BrakeBalance_lgbm_model_R4.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'BrakeBalance_lgbm_model_R4.txt'\")\n"
      ],
      "metadata": {
        "id": "qKLyP_UVpHwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimize Models using Practice_Data"
      ],
      "metadata": {
        "id": "5SPkqzOJuT1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Model names\n",
        "model_names = [\n",
        "    \"Engine_lgbm_model_R4.txt\",\n",
        "    \"Differential_lgbm_model_R4.txt\",\n",
        "    \"RearWing_lgbm_model_R4.txt\",\n",
        "    \"FrontWing_lgbm_model_R4.txt\",\n",
        "    \"Suspension_lgbm_model_R4.txt\",\n",
        "    \"BrakeBalance_lgbm_model_R4.txt\"\n",
        "]\n",
        "\n",
        "# Corresponding feature sets\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load new data\n",
        "df = pd.read_csv(\"practice_data_newnamesV4_3.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Target variable\n",
        "y_new = df[\"Avg Speed\"]\n",
        "\n",
        "# Sample weight\n",
        "sample_weight = [22] * len(y_new)\n",
        "\n",
        "# Training each model with its feature set\n",
        "for model_file in model_names:\n",
        "    model_key = model_file.split(\"_\")[0]  # Extract the prefix e.g., \"Engine\"\n",
        "    features = feature_sets[model_key]\n",
        "\n",
        "    print(f\"\\nUpdating {model_file} with features: {features}\")\n",
        "\n",
        "    # Prepare feature matrix\n",
        "    X_new = df[features]\n",
        "\n",
        "    # Load existing model\n",
        "    model = lgb.Booster(model_file=model_file)\n",
        "\n",
        "    # Create dataset\n",
        "    new_data = lgb.Dataset(X_new, label=y_new, weight=sample_weight)\n",
        "\n",
        "    # Continue training\n",
        "    updated_model = lgb.train(\n",
        "        params={},\n",
        "        train_set=new_data,\n",
        "        init_model=model,\n",
        "        num_boost_round=1000\n",
        "    )\n",
        "\n",
        "    # Save updated model to folder\n",
        "    folder_name = 'carpart_lgbm_model_R4'\n",
        "\n",
        "    # Create folder if it deos not exist\n",
        "    if not os.path.exists(folder_name):\n",
        "        os.makedirs(folder_name)\n",
        "\n",
        "    #update filename\n",
        "    updated_file = model_file.replace(\".txt\", \"_updated.txt\")\n",
        "\n",
        "    # complete file path\n",
        "    updated_file_path = os.path.join(folder_name, updated_file)\n",
        "\n",
        "    # save model to path\n",
        "    updated_model.save_model(updated_file_path)\n",
        "\n",
        "    print(f'Model saved to: {updated_file_path}')\n",
        "    print(f\"Model saved as {updated_file}\")\n",
        "\n",
        "print(\"\\nAll models updated successfully.\")"
      ],
      "metadata": {
        "id": "mLh9bTCSui6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequentially optimizing Car-Parameters"
      ],
      "metadata": {
        "id": "EjbfBEZkppMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "# Ordered list of car parameters to optimize\n",
        "params_to_optimize = [\n",
        "    \"Engine\", \"Differential\", \"RearWing\",\n",
        "    \"FrontWing\", \"Suspension\", \"BrakeBalance\"\n",
        "]\n",
        "\n",
        "# Feature sets used by each model for prediction\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load static base data (track & weather)\n",
        "base_data = pd.read_csv(\"track_weather_japan.csv\")\n",
        "\n",
        "# Dictionary to store optimized values\n",
        "optimized_params = {}\n",
        "\n",
        "# Sequential optimization loop\n",
        "for car_part in params_to_optimize:\n",
        "    print(f\"\\n🔧 Optimizing {car_part}...\")\n",
        "\n",
        "    # Load the pretrained LightGBM model from folder\n",
        "    folder_name = 'carpart_lgbm_model_R4'\n",
        "    model_file_name = f\"{car_part}_lgbm_model_R4_updated.txt\"\n",
        "    model_file_path = os.path.join(folder_name, model_file_name)\n",
        "    model = lgb.Booster(model_file=model_file_path)\n",
        "    features = feature_sets[car_part]\n",
        "\n",
        "    def objective(trial):\n",
        "        # Clone base data for this trial\n",
        "        input_data = base_data.copy()\n",
        "\n",
        "        # Add fixed parameters (already optimized)\n",
        "        for param, value in optimized_params.items():\n",
        "            input_data[param] = value\n",
        "\n",
        "        # Suggest value only for the current parameter\n",
        "        trial_value = trial.suggest_int(car_part, 1, 500)\n",
        "        input_data[car_part] = trial_value\n",
        "\n",
        "        # Fill in missing feature columns with default (0)\n",
        "        for feature in features:\n",
        "            if feature not in input_data.columns:\n",
        "                input_data[feature] = 0\n",
        "\n",
        "        # Predict average speed\n",
        "        X = input_data[features]\n",
        "        avg_speed = model.predict(X)[0]\n",
        "        return avg_speed\n",
        "\n",
        "    # Run Optuna optimization for this parameter\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "    # Save best value\n",
        "    best_value = study.best_params[car_part]\n",
        "    optimized_params[car_part] = best_value\n",
        "    print(f\"✅ Best {car_part}: {best_value}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n🎯 Final Optimized Parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "s3bWy7d8qBZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "# Ordered list of parameters to optimize\n",
        "params_to_optimize = [\"Engine\", \"Differential\", \"RearWing\", \"FrontWing\", \"Suspension\", \"BrakeBalance\"]\n",
        "\n",
        "# Corresponding feature sets for each model\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load static track & weather data\n",
        "base_data = pd.read_csv(\"track_weather_japan.csv\")\n",
        "\n",
        "# This will hold the best parameter values as we optimize them\n",
        "optimized_params = {}\n",
        "\n",
        "# Begin sequential optimization\n",
        "for car_part in params_to_optimize:\n",
        "    print(f\"\\n🔧 Optimizing {car_part}...\")\n",
        "\n",
        "    model_path = f\"{car_part}_lgbm_model_R4.txt\"\n",
        "    model = lgb.Booster(model_file=model_path)\n",
        "    features = feature_sets[car_part]\n",
        "\n",
        "    def objective(trial):\n",
        "        # Set the current parameter being optimized\n",
        "\n",
        "        params = {\n",
        "            \"Engine\": trial.suggest_int(\"Engine\", 1, 500),\n",
        "            \"RearWing\": trial.suggest_int(\"RearWing\", 1, 500),\n",
        "            \"FrontWing\": trial.suggest_int(\"FrontWing\", 1, 500),\n",
        "            \"BrakeBalance\": trial.suggest_int(\"BrakeBalance\", 1, 500),\n",
        "            \"Suspension\": trial.suggest_int(\"Suspension\", 1, 500),\n",
        "            \"Differential\": trial.suggest_int(\"Differential\", 1, 500),}\n",
        "\n",
        "        current_value = params[car_part]\n",
        "\n",
        "        # Create a single row input with all needed features\n",
        "        input_data = base_data.copy()\n",
        "\n",
        "        # Add previously optimized params\n",
        "        for param, value in optimized_params.items():\n",
        "            input_data[param] = value\n",
        "\n",
        "        # Add current trial value\n",
        "        input_data[car_part] = current_value\n",
        "\n",
        "        # If any missing feature, fill with 0 or a safe default\n",
        "        for col in features:\n",
        "            if col not in input_data.columns:\n",
        "                input_data[col] = 0\n",
        "\n",
        "        # Ensure correct order of features\n",
        "        X = input_data[features]\n",
        "\n",
        "        # Predict Avg Speed\n",
        "        avg_speed = model.predict(X)[0]\n",
        "        return avg_speed  # Optuna maximizes this\n",
        "\n",
        "    # Run Optuna\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=250, show_progress_bar=True)\n",
        "\n",
        "    best_value = study.best_params[car_part]\n",
        "    optimized_params[car_part] = best_value\n",
        "    print(f\"✅ Best {car_part}: {best_value}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n🎯 Final Optimized Parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "RalfPs7tp0dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning weights"
      ],
      "metadata": {
        "id": "AoqLLYVPmQ9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code first builds and saves a LightGBM regression model for predicting average speed based on simulator driving data.\n",
        "In the second part, it loads this trained model and applies it to the practice_data, using cross-validation and Optuna hyperparameter optimization to find the optimal sample weight scaling factor that minimizes prediction error on the new data."
      ],
      "metadata": {
        "id": "sYkWxHGy9WV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"optm_weights_lgbm_model_R4.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'optm_weights_lgbm_model_R4.txt'\")"
      ],
      "metadata": {
        "id": "6xGrl4Qq8iHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import FALSE\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the original model\n",
        "model = lgb.Booster(model_file=\"optm_weights_lgbm_model_R4.txt\")\n",
        "\n",
        "# Load the new data\n",
        "new_df = pd.read_csv(\"practice_dataV4_3.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "new_df[\"Avg Speed\"] = (new_df[\"Lap Distance\"] / new_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Prepare features and target\n",
        "X_new = new_df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\", \"Round\", \"Track\", \"Qualifying\",\n",
        "                             \"Stint\", \"Lap\", \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"])\n",
        "y_new = new_df[\"Avg Speed\"]\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest a scaling factor for sample weights\n",
        "    weight_scaling_factor = trial.suggest_int('weight_scaling_factor', 1, 100, log = True )\n",
        "    sample_weight = [weight_scaling_factor] * len(y_new)\n",
        "\n",
        "    # Use k-fold cross-validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    rmse_list = []\n",
        "\n",
        "    # Train and evaluate using cross-validation\n",
        "    for train_index, valid_index in kf.split(X_new):\n",
        "        X_train, X_valid = X_new.iloc[train_index], X_new.iloc[valid_index]\n",
        "        y_train, y_valid = y_new.iloc[train_index], y_new.iloc[valid_index]\n",
        "        train_weight = np.array(sample_weight)[train_index]\n",
        "\n",
        "        train_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n",
        "        valid_data = lgb.Dataset(X_valid, label=y_valid, weight=np.array(sample_weight)[valid_index], reference=train_data)\n",
        "\n",
        "        # Train the model\n",
        "        model_tmp = model\n",
        "        model_tmp = lgb.train(\n",
        "            params={},  # Use default params unless specified\n",
        "            train_set=train_data,\n",
        "            init_model=model,\n",
        "            num_boost_round=1000,  # You can increase if needed\n",
        "        )\n",
        "\n",
        "        # Predict and calculate RMSE for this fold\n",
        "        y_pred = model_tmp.predict(X_valid, num_iteration=model_tmp.best_iteration)\n",
        "        #rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "        rmse = np.sqrt(((y_valid - y_pred)**2).mean())\n",
        "        rmse_list.append(rmse)\n",
        "\n",
        "    # Return the average RMSE over all folds\n",
        "    mean_rmse = np.mean(rmse_list)\n",
        "    return mean_rmse\n",
        "\n",
        "# Create Optuna study to minimize RMSE\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
        "\n",
        "best_weight_factor = study.best_params['weight_scaling_factor']\n",
        "print(f\"Best weight scaling factor: {best_weight_factor}\")\n"
      ],
      "metadata": {
        "id": "w_-JMlwsv1HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso Regression"
      ],
      "metadata": {
        "id": "aBFPsk1-aoww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "import optuna\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define the Optuna objective\n",
        "def objective(trial):\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)\n",
        "\n",
        "    # Create pipeline\n",
        "    model = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"lasso\", Lasso(alpha=alpha, max_iter=10000))\n",
        "    ])\n",
        "\n",
        "    # K-Fold CV\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(model, X, y, cv=kf, scoring=\"neg_root_mean_squared_error\")\n",
        "\n",
        "    return -np.mean(scores)\n",
        "\n",
        "# Run Optuna study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=150)\n",
        "\n",
        "# Print best params and score\n",
        "print(\"Best RMSE:\", study.best_value)\n",
        "print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "# Train final model on full data\n",
        "best_alpha = study.best_params[\"alpha\"]\n",
        "final_model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lasso\", Lasso(alpha=best_alpha, max_iter=10000))\n",
        "])\n",
        "final_model.fit(X, y)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ytUkRtmXbcPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, r2_score\n",
        "import optuna\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define Optuna objective function\n",
        "def objective(trial):\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"lasso\", Lasso(alpha=alpha, max_iter=10000))\n",
        "    ])\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Negative RMSE\n",
        "    rmse_scores = cross_val_score(pipeline, X, y, cv=kf, scoring=\"neg_root_mean_squared_error\")\n",
        "    mean_rmse = -np.mean(rmse_scores)\n",
        "\n",
        "    # R^2 Score (logged for info)\n",
        "    r2_scores = cross_val_score(pipeline, X, y, cv=kf, scoring=\"r2\")\n",
        "    mean_r2 = np.mean(r2_scores)\n",
        "\n",
        "    # Log R²\n",
        "    trial.set_user_attr(\"mean_r2\", mean_r2)\n",
        "\n",
        "    return mean_rmse\n",
        "\n",
        "# Run Optuna tuning\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Best result\n",
        "print(\"Best RMSE:\", study.best_value)\n",
        "print(\"Best alpha:\", study.best_params[\"alpha\"])\n",
        "print(\"R² for best trial:\", study.best_trial.user_attrs[\"mean_r2\"])\n"
      ],
      "metadata": {
        "id": "oWtRIM7mkBB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"simulator_data.csv\")\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "X = df[[\"Cornering\", \"Inclines\", \"Camber\", \"Grip\", \"Wind (Avg. Speed)\",\n",
        "          \"Temperature\", \"Humidity\", \"Air Density\", \"Air Pressure\", \"Wind (Gusts)\",\n",
        "          \"Altitude\", \"Roughness\", \"Width\", \"Rear Wing\", \"Engine\", \"Front Wing\",\n",
        "          \"Brake Balance\", \"Differential\", \"Suspension\"]]\n",
        "        #X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Manually set best alpha\n",
        "best_alpha = 0.009431544737855992\n",
        "\n",
        "# Build pipeline\n",
        "model_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lasso\", Lasso(alpha=best_alpha, max_iter=10000))\n",
        "])\n",
        "\n",
        "# Train model\n",
        "model_pipeline.fit(X, y)\n",
        "\n",
        "# Save model to file\n",
        "joblib.dump(model_pipeline, \"lasso_model.pkl\")\n",
        "\n",
        "print(\"Model saved as 'lasso_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "1fDbNxANdesn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import joblib\n",
        "\n",
        "feature_order = [\n",
        "    \"Cornering\", \"Inclines\", \"Camber\", \"Grip\", \"Wind (Avg. Speed)\",\n",
        "    \"Temperature\", \"Humidity\", \"Air Density\", \"Air Pressure\", \"Wind (Gusts)\",\n",
        "    \"Altitude\", \"Roughness\", \"Width\", \"Rear Wing\", \"Engine\", \"Front Wing\",\n",
        "    \"Brake Balance\", \"Differential\", \"Suspension\"]\n",
        "\n",
        "\n",
        "# Load the saved model\n",
        "model = joblib.load(\"lasso_model.pkl\")\n",
        "\n",
        "# Load constant track and weather data\n",
        "#track_weather = pd.read_csv(\"track_weather_japan.csv\")\n",
        "track_weather_data = track_weather.iloc[0]  # Use first row (assumed to be constant)\n",
        "\n",
        "# Define Optuna optimization function\n",
        "def objective(trial):\n",
        "    # Car setup parameters (suggested by Optuna)\n",
        "    params = {\n",
        "        \"Rear Wing\": trial.suggest_int(\"Rear Wing\", 1, 500),\n",
        "        \"Engine\": trial.suggest_int(\"Engine\", 1, 500),\n",
        "        \"Front Wing\": trial.suggest_int(\"Front Wing\", 1, 500),\n",
        "        \"Brake Balance\": trial.suggest_int(\"Brake Balance\", 1, 500),\n",
        "        \"Differential\": trial.suggest_int(\"Differential\", 1, 500),\n",
        "        \"Suspension\": trial.suggest_int(\"Suspension\", 1, 500),\n",
        "\n",
        "    }\n",
        "     # Merge with constant track/weather data\n",
        "    input_dict = {**track_weather_data.to_dict(), **params}\n",
        "\n",
        "   # input_data = pd.concat([track_weather, pd.DataFrame([params])], axis=1)\n",
        "\n",
        "    # Combine car params and constant track/weather data into a single DataFrame\n",
        "    input_data = pd.DataFrame([[input_dict[col] for col in feature_order]], columns=feature_order)\n",
        "\n",
        "    # Predict Avg Speed (Lasso was trained with standardized inputs)\n",
        "    predicted_speed = model.predict(input_data)[0]\n",
        "\n",
        "    # Since Optuna minimizes, return negative speed to maximize it\n",
        "    return -predicted_speed\n",
        "\n",
        "# Run the Optuna study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=250)\n",
        "\n",
        "# Show best result\n",
        "print(\"Best Avg Speed:\", -study.best_value)\n",
        "print(\"Best Parameters:\", study.best_params)\n"
      ],
      "metadata": {
        "id": "ZDpc0ER7faN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Direct Optimization with Selenium"
      ],
      "metadata": {
        "id": "nBtVUnjorojh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code does not run in Google Colab, it must be run in a local instance of python."
      ],
      "metadata": {
        "id": "WBcrMSdJEFwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import optuna\n",
        "\n",
        "USERNAME = 'xxx@studium.uni-hamburg.de'\n",
        "PASSWORD = 'xxx'\n",
        "STINT_LENGTH = 5\n",
        "FUEL_LOAD = 16\n",
        "COUNTER = 11 #set to last xpath tr value +1 or last stint +2\n",
        "N_TRAILS = 1\n",
        "\n",
        "# Function to set up the browser\n",
        "def setup_browser():\n",
        "\n",
        "    chrome_options = Options()\n",
        "\n",
        "    # Open the browser\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    return driver\n",
        "\n",
        "\n",
        "# Function to login and navigate to the page with the form\n",
        "def login_and_navigate(driver, username, password):\n",
        "    # Open the login page\n",
        "    driver.get(\"https://team-analytics.com/f1/\")  # Replace with your login URL\n",
        "\n",
        "    # Fill in the login credentials\n",
        "    driver.find_element(By.NAME, 'email').send_keys(username)  # Replace 'username' with actual element ID\n",
        "    driver.find_element(By.NAME, 'password').send_keys(password)  # Replace 'password' with actual element ID\n",
        "\n",
        "    # Click the login button\n",
        "    driver.find_element(By.NAME, 'login_user_btn').click()  # Replace 'login-button' with the correct button ID\n",
        "\n",
        "    # Wait for the login to complete (adjust sleep time as necessary)\n",
        "    time.sleep(3)\n",
        "\n",
        "     #Click the button that takes you to the next page\n",
        "    next_page_button = driver.find_element(By.NAME, 'practice_round')  # Adjust the ID of the button\n",
        "    next_page_button.click()\n",
        "\n",
        "    # Wait for the page to load (adjust as necessary)\n",
        "    time.sleep(3)\n",
        "\n",
        "def parse_time_string(time_str):\n",
        "    \"\"\"Parses a time string in the format 'MM:SS:MS' into total seconds.\"\"\"\n",
        "    minutes, seconds, milliseconds = map(int, time_str.split(':'))\n",
        "    total_seconds = minutes * 60 + seconds + milliseconds / 1000.0\n",
        "    return total_seconds\n",
        "\n",
        "# Function to fill in the form and get the result (reuse the open driver session)\n",
        "def get_avg_time(driver, params):\n",
        "\n",
        "\n",
        "    # Locate and fill the input fields (replace 'paramX' with actual field IDs)\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"rearwing\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[0]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"engine\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[1]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"frontwing\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[2]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"brake\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[3]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"differential\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[4]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"suspension\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[5]))\n",
        "\n",
        "    stintlenght = 3\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"stint_length\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(STINT_LENGTH))\n",
        "\n",
        "    fuelload = 12\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"fuel_load\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(FUEL_LOAD))\n",
        "\n",
        "\n",
        "\n",
        "    # Submit the form\n",
        "    driver.find_element(By.NAME, 'submit_practice_stint').click()\n",
        "\n",
        "    # Wait for the result to load\n",
        "    time.sleep(3)\n",
        "\n",
        "    if not hasattr(get_avg_time, \"counter\"):\n",
        "        get_avg_time.counter = COUNTER #set to last xpath tr value +1 / last stint +2\n",
        "\n",
        "    # Extract the avg. time result\n",
        "    xpath = f'//*[@id=\"submit_practice\"]/table[2]/tbody/tr[{get_avg_time.counter}]/td[12]'\n",
        "    time_str_element = driver.find_element(By.XPATH, xpath)\n",
        "    time_str = time_str_element.text\n",
        "    avg_time = parse_time_string(time_str)\n",
        "\n",
        "    # Increment the counter\n",
        "    get_avg_time.counter += 1\n",
        "\n",
        "    return avg_time\n",
        "\n",
        "\n",
        "# Function to close the browser\n",
        "def close_browser(driver):\n",
        "    driver.quit()\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial, driver):\n",
        "    # Sample values for the 6 parameters\n",
        "    param1 = trial.suggest_int('param1', low=350, high=500) #RearWing\n",
        "    param2 = trial.suggest_int('param2', low=20, high=150)    #Engine\n",
        "    param3 = trial.suggest_int('param3', low=300, high=450)  #FrontWing\n",
        "    param4 = trial.suggest_int('param4', low=100, high=250)   #Brake\n",
        "    param5 = trial.suggest_int('param5', low=35, high=250)  #Differential\n",
        "    param6 = trial.suggest_int('param6', low=50, high=200)  #Suspension\n",
        "\n",
        "    # Bundle the parameters into a list\n",
        "    params = [param1, param2, param3, param4, param5, param6]\n",
        "\n",
        "    # Get the avg time from the website (reuse the same driver)\n",
        "    avg_time = get_avg_time(driver, params)\n",
        "\n",
        "    return avg_time\n",
        "\n",
        "# Set up the browser and login\n",
        "driver = setup_browser()\n",
        "login_and_navigate(driver, USERNAME, PASSWORD)\n",
        "\n",
        "# Create an Optuna study to optimize the objective function\n",
        "storage = \"sqlite:///DirectOptStudyOneR4.db\"\n",
        "study = optuna.create_study(direction='minimize', study_name=\"DirectOptStudyOneR5\", storage=storage, load_if_exists=True)\n",
        "study.optimize(lambda trial: objective(trial, driver), n_trials=N_TRAILS)\n",
        "\n",
        "# Print the best parameters and corresponding result\n",
        "print(\"Best parameters found:\", study.best_params)\n",
        "print(\"Best avg. time:\", study.best_value)\n",
        "\n",
        "# Close the browser after optimization\n",
        "close_browser(driver)\n",
        "\n"
      ],
      "metadata": {
        "id": "aG23y8bLzLHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ajs8np_QL5T"
      },
      "source": [
        "\n",
        "# **Analytics for Race 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goals of this weeks Analytics were twofold.\n",
        "\n",
        "1. We wanted to further look into weighing our Data, not only weighing the whole set of Practice_data, but also weighig each Line of Simulator_Data based on its distance to the Races Track/Weather Conditions.\n",
        "2. We wanted to integrate the \"Direct Optimization with Selenium\" into our optimizatin Workflow and use it to vary the parameters in a defindes space to create more Data.\n",
        "\n",
        "**Weighing Simulator and Pracice Data**\n",
        "This week we wanted to further look into weighing the Simulator_data and Practice_Data. We want to weigh each line of Simulator_Data depending on its closeness to the actual Track_Data. For weighing the Practice_Data we want to use the same optimization as last week.\n",
        "In addition to this,we want to be able, to weigh the Practice_Data from the last races different to the Practice_Data from this race. (e.g. prior practice_data weight = 2, current practice_data weight = 5). The code found in \"Weighing Each Line of Simulator_Data\" weights every line of Simulator_Data and then combines the weighed Simulator_Data with the the two Practice_Data sets. The weights, that are assinged to the two practice data sets can be set manually within the code and are optimized beforehand.\n",
        "\n",
        "This week, we focused on further refining the weighting of our Simulator_Data and Practice_Data. Our goal is to assign weights to each line of Simulator_Data based on its proximity to the actual Track_Data, giving more importance to data that closely matches the real track conditions. For Practice_Data, we plan to use the same optimization approach as last week to determine appropriate weights.\n",
        "\n",
        "Additionally, we want the flexibility to assign different weights to Practice_Data from previous races compared to the Practice_Data from the current race, for example, setting a weight of 2 for prior practice_data and 5 for practice_data from the current race. The code in \"Weighing Each Line of Simulator_Data\" handles the weighting of each Simulator_Data line and then combines it with the two Practice_Data sets. The weights assigned to the practice datasets can be manually set within the code and are optimized beforehand to find the best combination.\n",
        "\n",
        "The weighing of the Simulator_Data lines works as follows:\n",
        "\n",
        "**Hier einfügen, wie die gewichtung der einzelnen datenzeilen funktioniert, gerne in einem Stil wie ich  ihn zum beispiel auch bei Race 1 oder 4 genutzt habe um logisches vorgehen des codes zu beschreiben.**\n",
        "\n",
        "Once the code for weighting the data was developed, we modified our hyperparameter optimization, model training, and sequential optimization routines to incorporate the new, weighted datasets. This ensured that all processes could utilize the adjusted data effectively. We then proceeded as described, leveraging the weighted Simulator_Data and Practice_Data to refine our models and optimization strategies, ultimately aiming for improved predictive accuracy and more realistic simulation results.\n",
        "\n",
        "**Implementing the direct Optimization**\n",
        "\n",
        "We aimed to combine our direct optimization approach with the sequential optimization method. To achieve this, we first optimized the car parameters using the weighted data and our sequential model. Subsequently, we configured the direct optimization to vary the parameters within an interval around the values obtained from the sequential model. This approach is similar to the parameter variation we performed with our all-in-one optimizer, as described in \"Analytics for Race 2.\" The generated practice laps from this process could then be fed back into the sequential model for further refinement. We repeated this process for multiple iterations, and after each iteration, the parameter ranges within the direct optimization were redefined based on the results from the sequential optimizer.\n",
        "\n",
        "We tried doing this for 15 trails, using fuel=10 and stint_lenght=1, so 15 Laps per iteration. But due to some hickups and user error we were not able to acurately do this, but were forced to do three iterations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OAsfd-Hwh7Wb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hidcQoG0wLLK"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Paula Kussauer, Cedric Schwandt, Hannes Kock"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighing Simulator_Data and Practice_Data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hu6SNdpmjjLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighing Practice_Data vs Simulator_Data"
      ],
      "metadata": {
        "id": "3JbBLqYbv3VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The part in this Notebook following this one weighs each Lap from the Simulator_Data and then merges the Simulator_Data and Practice_Data.\n",
        "\n",
        "When it merges the two datasets, it also assigns weights to the Practice_Data. We use these two Codecells to figure out what that weight sould be.\n",
        "\n",
        "Its the same as the one from last week, it just uses different Data."
      ],
      "metadata": {
        "id": "41Sjcdnr7rOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"optm_weights_lgbm_model_R5.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'optm_weights_lgbm_model_R5.txt'\")\n"
      ],
      "metadata": {
        "id": "_ZsnK4hqxZpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import FALSE\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the original model\n",
        "model = lgb.Booster(model_file=\"optm_weights_lgbm_model_R5.txt\")\n",
        "\n",
        "# Load the new data\n",
        "new_df = pd.read_csv(\"practice_data_newnamesV5_1.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "new_df[\"Avg Speed\"] = (new_df[\"Lap Distance\"] / new_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Prepare features and target\n",
        "X_new = new_df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\", \"Round\", \"Track\", \"Qualifying\",\n",
        "                             \"Stint\", \"Lap\", \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"])\n",
        "y_new = new_df[\"Avg Speed\"]\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest a scaling factor for sample weights\n",
        "    weight_scaling_factor = trial.suggest_int('weight_scaling_factor', 1, 15, log = True )\n",
        "    sample_weight = [weight_scaling_factor] * len(y_new)\n",
        "\n",
        "    # Use k-fold cross-validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    rmse_list = []\n",
        "\n",
        "    # Train and evaluate using cross-validation\n",
        "    for train_index, valid_index in kf.split(X_new):\n",
        "        X_train, X_valid = X_new.iloc[train_index], X_new.iloc[valid_index]\n",
        "        y_train, y_valid = y_new.iloc[train_index], y_new.iloc[valid_index]\n",
        "        train_weight = np.array(sample_weight)[train_index]\n",
        "\n",
        "        train_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n",
        "        valid_data = lgb.Dataset(X_valid, label=y_valid, weight=np.array(sample_weight)[valid_index], reference=train_data)\n",
        "\n",
        "        # Train the model\n",
        "        model_tmp = model\n",
        "        model_tmp = lgb.train(\n",
        "            params={},  # Use default params unless specified\n",
        "            train_set=train_data,\n",
        "            init_model=model,\n",
        "            num_boost_round=1000,  # You can increase if needed\n",
        "        )\n",
        "\n",
        "        # Predict and calculate RMSE for this fold\n",
        "        y_pred = model_tmp.predict(X_valid, num_iteration=model_tmp.best_iteration)\n",
        "        #rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "        rmse = np.sqrt(((y_valid - y_pred)**2).mean())\n",
        "        rmse_list.append(rmse)\n",
        "\n",
        "    # Return the average RMSE over all folds\n",
        "    mean_rmse = np.mean(rmse_list)\n",
        "    return mean_rmse\n",
        "\n",
        "# Create Optuna study to minimize RMSE\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=30)  # You can adjust the number of trials\n",
        "\n",
        "best_weight_factor = study.best_params['weight_scaling_factor']\n",
        "print(f\"Best weight scaling factor: {best_weight_factor}\")\n"
      ],
      "metadata": {
        "id": "aVZ0LiUTv2ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighing Each Line of Simulator_Data"
      ],
      "metadata": {
        "id": "mROz43M2znV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script merges simulated and real racing data, calculates weights (higher if simulated data resembles real conditions), and outputs the consolidated data for further machine learning, giving real/practice laps greater emphasis.\n",
        "\n",
        "The weights of the SImulation_Data Laps are calculated based on Gaussian RBF kernel weights, based on how \"close\" each simulated lap's track/weather features are to the centroid of the practice laps (in Mahalanobis distance)."
      ],
      "metadata": {
        "id": "P5oeP-1K3YZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rbf_weights_auto.py  ︳  nur EINMAL pro Race ausführen\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# A) Dateipfade anpassen\n",
        "# -----------------------------------------------------------\n",
        "SIM_CSV      = \"simulator_data_newnames.csv\"   # 10 000 Sim-Laps\n",
        "PRACTICEOLD_CSV = \"practice_data_newnamesV5_1.csv\"             # 80 echte Laps\n",
        "PRACTICENEW_CSV = \"practice_data_newnamesV5_3.csv\"\n",
        "ENV_CSV      = \"track_weather_USA.csv\" # 1 Zeile Track & Weather\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# B) Daten laden\n",
        "# -----------------------------------------------------------\n",
        "df_sim = pd.read_csv(SIM_CSV)\n",
        "df_pro  = pd.read_csv(PRACTICEOLD_CSV)\n",
        "df_prn  = pd.read_csv(PRACTICENEW_CSV)\n",
        "env_df = pd.read_csv(ENV_CSV)                  # eine Zeile\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# C) Meta-Spalten automatisch ableiten\n",
        "# -----------------------------------------------------------\n",
        "# 1. Alle Car-Parameter (= Optimierungs­kandidaten) sammeln\n",
        "car_param_cols = {\n",
        "    \"Engine\", \"Differential\", \"RearWing\", \"FrontWing\",\n",
        "    \"Suspension\", \"BrakeBalance\"\n",
        "}\n",
        "\n",
        "# 2. Alles, was in der ENV-Datei steht, ist ein Track/Weather-Feature\n",
        "env_cols = env_df.columns.tolist()             # z.B. Air Pressure, Grip, …\n",
        "\n",
        "# 3. Sicherheitshalber nur die Spalten behalten,\n",
        "#    die auch wirklich in den Sim/Practice-CSVs stehen\n",
        "meta_cols = [c for c in env_cols if c in df_sim.columns]\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# D) RBF-Kernel-Gewichte berechnen\n",
        "# -----------------------------------------------------------\n",
        "tau = 2.0  # Bandbreite\n",
        "X_sim_meta = df_sim[meta_cols].values\n",
        "\n",
        "# Use target track/weather parameters from ENV_CSV instead of practice centroid\n",
        "env_point = env_df[meta_cols].values.flatten()  # 1D array of target conditions\n",
        "\n",
        "# Covariance matrix (use simulator data for shape/spread)\n",
        "Sigma_inv = np.linalg.inv(np.cov(X_sim_meta, rowvar=False))\n",
        "\n",
        "k_sim = np.exp([\n",
        "    -mahalanobis(x, env_point, Sigma_inv) / tau\n",
        "    for x in X_sim_meta\n",
        "])  # k_i = exp(-d_i/τ)\n",
        "\n",
        "w_pro = np.full(len(df_pro), 1)  # konstant für Practice, wie gehabt\n",
        "w_prn = np.full(len(df_prn), 6)  # konstant für Practice, wie gehabt\n",
        "weights = np.concatenate([k_sim, w_pro, w_prn])\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# E) Gemeinsames DataFrame für alle weiteren Modelle\n",
        "# -----------------------------------------------------------\n",
        "df_all = pd.concat([df_sim, df_pro, df_prn], ignore_index=True)\n",
        "\n",
        "df_all[\"Avg Speed\"] = (df_all[\"Lap Distance\"] / df_all[\"Lap Time\"]) * 3600\n",
        "\n",
        "\n",
        "df_all[\"sample_weight\"] = weights     # Add weights as a new column\n",
        "\n",
        "# List of columns to drop\n",
        "drop_cols = [\n",
        "    \"Lap Distance\", \"Lap Time\", \"Round\", \"Track\", \"Qualifying\", \"Stint\", \"Lap\",\n",
        "    \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"\n",
        "]\n",
        "\n",
        "# Drop the columns (ignore errors if column missing just in case)\n",
        "df_all_dropped = df_all.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "OUTPUT_CSV = \"all_data_with_weights.csv\"\n",
        "df_all_dropped.to_csv(OUTPUT_CSV, index=False)\n"
      ],
      "metadata": {
        "id": "VeFeLMyRjrgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying Weighed Data on AllInOne Optimization code"
      ],
      "metadata": {
        "id": "8BQaaeTJuQZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")  # <--- new combined data with weights included!\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "    # Drop unwanted columns\n",
        "    X = df.drop(columns=['Avg Speed', 'sample_weight'])  # ensure these are not in X\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"sample_weight\"].values\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        train_weights, val_weights = weights[train_idx], weights[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=train_weights)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=val_weights)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds) ** 2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ypuv_1bVoJ5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{'num_leaves': 78, 'max_depth': 15, 'learning_rate': 0.11424671210900976, 'min_data_in_leaf': 48, 'feature_fraction': 0.7696371152792187, 'bagging_fraction': 0.9622430850853232, 'bagging_freq': 10, 'lambda_l1': 0.4732893195494387, 'lambda_l2': 2.047352465618619}"
      ],
      "metadata": {
        "id": "leX4OzdCtCvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the weighed data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"sample_weight\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset with weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 78,\n",
        "    'max_depth': 15,\n",
        "    'learning_rate': 0.11424671210900976,\n",
        "    'min_data_in_leaf': 48,\n",
        "    'feature_fraction': 0.7696371152792187,\n",
        "    'bagging_fraction': 0.9622430850853232,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 0.4732893195494387,\n",
        "    'lambda_l2': 2.047352465618619}\n",
        "\n",
        "# Train on full dataset using sample weights\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"best_lgbm_model_R5.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'best_lgbm_model_R5.txt'\")"
      ],
      "metadata": {
        "id": "XWASXgBPtaBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "\n",
        "# Load trained model\n",
        "#model = lgb.Booster(model_file=\"best_lgbm_model_updated.txt\")\n",
        "model = lgb.Booster(model_file=\"best_lgbm_model_R5.txt\")\n",
        "\n",
        "# Load track/weather data (single row)\n",
        "track_weather = pd.read_csv(\"track_weather_USA.csv\")\n",
        "\n",
        "# Define the objective function\n",
        "def objective(trial):\n",
        "    # Suggest car parameters\n",
        "    params = {\n",
        "        \"Engine\": trial.suggest_int(\"Engine\", 1, 500),\n",
        "        \"Rear Wing\": trial.suggest_int(\"Rear Wing\", 1, 500),\n",
        "        \"Front Wing\": trial.suggest_int(\"Front Wing\", 1, 500),\n",
        "        \"Brake Balance\": trial.suggest_int(\"Brake Balance\", 1, 500),\n",
        "        \"Suspension\": trial.suggest_int(\"Suspension\", 1, 500),\n",
        "        \"Differential\": trial.suggest_int(\"Differential\", 1, 500),\n",
        "    }\n",
        "#Engine: 91\n",
        "#Differential: 101\n",
        "#RearWing: 489\n",
        "#FrontWing: 274\n",
        "#Suspension: 45\n",
        "#BrakeBalance: 101\n",
        "\n",
        "    # Combine with static track/weather parameters\n",
        "    input_data = pd.concat([track_weather, pd.DataFrame([params])], axis=1)\n",
        "\n",
        "    # Predict average speed\n",
        "    predicted_avg_speed = model.predict(input_data)[0]\n",
        "\n",
        "    # We want to maximize speed\n",
        "    return -predicted_avg_speed\n",
        "\n",
        "# Create study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize\n",
        "study.optimize(objective, n_trials=1000)\n",
        "\n",
        "# Output best results\n",
        "best_trial = study.best_trial\n",
        "print(\"\\nBest Parameters:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print(f\"Predicted Avg Speed: {-best_trial.value:.2f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5zv1xh4CtaBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighed Data and Sequential Optimization"
      ],
      "metadata": {
        "id": "KFL33p1fuXyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "a8kmjWhju-iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"sample_weight\"].values\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights[train_idx], weights[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7on7gZLEueie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Engine:\n",
        "{'num_leaves': 228, 'max_depth': 9, 'learning_rate': 0.019795034976830574, 'min_data_in_leaf': 45, 'feature_fraction': 0.6769051793180653, 'bagging_fraction': 0.5679615916585826, 'bagging_freq': 5, 'lambda_l1': 0.2140082876917294, 'lambda_l2': 0.778813758195442}"
      ],
      "metadata": {
        "id": "a_iOFPeRJqzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Differential\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"sample_weight\"].values\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights[train_idx], weights[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-JMGNyRjvsnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Differential:\n",
        "{'num_leaves': 95, 'max_depth': 14, 'learning_rate': 0.020430708599380454, 'min_data_in_leaf': 11, 'feature_fraction': 0.8289782961700536, 'bagging_fraction': 0.6054739109884507, 'bagging_freq': 6, 'lambda_l1': 0.164978854742107, 'lambda_l2': 0.09387958914404965}"
      ],
      "metadata": {
        "id": "hDfDgG3-JkNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Rear Wing\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    X = df[['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"sample_weight\"].values\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights[train_idx], weights[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kKCqkZ87vsnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Rear Wing:\n",
        "{'num_leaves': 263, 'max_depth': 8, 'learning_rate': 0.033545204540268374, 'min_data_in_leaf': 10, 'feature_fraction': 0.67904602486583, 'bagging_fraction': 0.9010766438515055, 'bagging_freq': 3, 'lambda_l1': 0.00361547478377304, 'lambda_l2': 0.00947809994264175}"
      ],
      "metadata": {
        "id": "YP9GE3mrJclh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Front Wing\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    X = df[['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"sample_weight\"].values\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights[train_idx], weights[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)"
      ],
      "metadata": {
        "id": "WfPqNfYmvsnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Front Wing:\n",
        "{'num_leaves': 192, 'max_depth': 12, 'learning_rate': 0.062393011943707284, 'min_data_in_leaf': 10, 'feature_fraction': 0.8418876970992247, 'bagging_fraction': 0.8723398679038107, 'bagging_freq': 10, 'lambda_l1': 0.3441273621643392, 'lambda_l2': 0.5476995416360311}"
      ],
      "metadata": {
        "id": "CkrN6zjbJVkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Suspension\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    X = df[['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"sample_weight\"].values\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights[train_idx], weights[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)"
      ],
      "metadata": {
        "id": "PMH7jVeevsnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Optimization Brake Balance\n",
        "#https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Example: Regression task\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "        'force_col_wise': True  # Optional but often speeds things up\n",
        "    }\n",
        "\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    X = df[['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "    y = df[\"Avg Speed\"]\n",
        "    weights = df[\"sample_weight\"].values\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_train, w_val = weights[train_idx], weights[val_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n",
        "\n",
        "        model = lgb.train(param, dtrain, num_boost_round=100,\n",
        "                          valid_sets=[dvalid],\n",
        "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)])\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(((y_val - preds)**2).mean())\n",
        "        scores.append(rmse)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=500)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)"
      ],
      "metadata": {
        "id": "DB_Tazw8vsnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Engine:\n",
        "{'num_leaves': 228, 'max_depth': 9, 'learning_rate': 0.019795034976830574, 'min_data_in_leaf': 45, 'feature_fraction': 0.6769051793180653, 'bagging_fraction': 0.5679615916585826, 'bagging_freq': 5, 'lambda_l1': 0.2140082876917294, 'lambda_l2': 0.778813758195442}"
      ],
      "metadata": {
        "id": "H85SBOG3KAkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Differential:\n",
        "{'num_leaves': 95, 'max_depth': 14, 'learning_rate': 0.020430708599380454, 'min_data_in_leaf': 11, 'feature_fraction': 0.8289782961700536, 'bagging_fraction': 0.6054739109884507, 'bagging_freq': 6, 'lambda_l1': 0.164978854742107, 'lambda_l2': 0.09387958914404965}"
      ],
      "metadata": {
        "id": "gKJ4lW4LKAkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Rear Wing:\n",
        "{'num_leaves': 263, 'max_depth': 8, 'learning_rate': 0.033545204540268374, 'min_data_in_leaf': 10, 'feature_fraction': 0.67904602486583, 'bagging_fraction': 0.9010766438515055, 'bagging_freq': 3, 'lambda_l1': 0.00361547478377304, 'lambda_l2': 0.00947809994264175}"
      ],
      "metadata": {
        "id": "Oz7hsgFzKAkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Front Wing:\n",
        "{'num_leaves': 192, 'max_depth': 12, 'learning_rate': 0.062393011943707284, 'min_data_in_leaf': 10, 'feature_fraction': 0.8418876970992247, 'bagging_fraction': 0.8723398679038107, 'bagging_freq': 10, 'lambda_l1': 0.3441273621643392, 'lambda_l2': 0.5476995416360311}"
      ],
      "metadata": {
        "id": "z0uQoknoKAkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial Suspension:\n",
        "{'num_leaves': 273, 'max_depth': 11, 'learning_rate': 0.04451045225106865, 'min_data_in_leaf': 12, 'feature_fraction': 0.7765092601043706, 'bagging_fraction': 0.6301841215843957, 'bagging_freq': 1, 'lambda_l1': 0.01670808736808642, 'lambda_l2': 0.12825953049230684}"
      ],
      "metadata": {
        "id": "K2kODLeDQbTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best trial:\n",
        "{'num_leaves': 191, 'max_depth': 14, 'learning_rate': 0.024681809936626296, 'min_data_in_leaf': 10, 'feature_fraction': 0.8101056533251277, 'bagging_fraction': 0.5448991038428275, 'bagging_freq': 2, 'lambda_l1': 0.09124351826015425, 'lambda_l2': 0.0008864299246597318}"
      ],
      "metadata": {
        "id": "__n8xB0LQfmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Models for Seq. Optimization"
      ],
      "metadata": {
        "id": "MMFIIFlmDfRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "basically the same as before, adjusted to use the weighed data and to save to a Folder."
      ],
      "metadata": {
        "id": "X0KIrtjxDmIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the weighed data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Features and target\n",
        "X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 228,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.019795034976830574,\n",
        "    'min_data_in_leaf': 45,\n",
        "    'feature_fraction': 0.6769051793180653,\n",
        "    'bagging_fraction': 0.5679615916585826,\n",
        "    'bagging_freq': 5,\n",
        "    'lambda_l1': 0.2140082876917294,\n",
        "    'lambda_l2': 0.778813758195442\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"Engine_lgbm_model_R5.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "-E6-Pq1pDjOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Differential (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "# Features and target\n",
        "X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 95,\n",
        "    'max_depth': 14,\n",
        "    'learning_rate': 0.020430708599380454,\n",
        "    'min_data_in_leaf': 11,\n",
        "    'feature_fraction': 0.8289782961700536,\n",
        "    'bagging_fraction': 0.6054739109884507,\n",
        "    'bagging_freq': 6,\n",
        "    'lambda_l1': 0.164978854742107,\n",
        "    'lambda_l2': 0.09387958914404965\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"Differential_lgbm_model_R5.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "YNxFgi1XDjON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rear Wing (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "\n",
        "# Features and target\n",
        "X = df[['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 263,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.033545204540268374,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.67904602486583,\n",
        "    'bagging_fraction': 0.9010766438515055,\n",
        "    'bagging_freq': 3,\n",
        "    'lambda_l1': 0.00361547478377304,\n",
        "    'lambda_l2': 0.00947809994264175\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"RearWing_lgbm_model_R5.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "0uBz10u1DjON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Front Wing (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "\n",
        "# Features and target\n",
        "X = df[['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 192,\n",
        "    'max_depth': 12,\n",
        "    'learning_rate': 0.062393011943707284,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.8418876970992247,\n",
        "    'bagging_fraction': 0.8723398679038107,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 0.3441273621643392,\n",
        "    'lambda_l2': 0.5476995416360311\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"FrontWing_lgbm_model_R5.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "YSkLXfS_DjON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Suspension (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "\n",
        "# Features and target\n",
        "X = df[['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 191,\n",
        "    'max_depth': 14,\n",
        "    'learning_rate': 0.024681809936626296,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.8101056533251277,\n",
        "    'bagging_fraction': 0.5448991038428275,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.09124351826015425,\n",
        "    'lambda_l2': 0.0008864299246597318\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"Suspension_lgbm_model_R5.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "YZUnFHXQDjON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brake Balance ( and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights.csv\")\n",
        "\n",
        "\n",
        "# Features and target\n",
        "X = df[['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 27,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.18696777761444966,\n",
        "    'min_data_in_leaf': 73,\n",
        "    'feature_fraction': 0.964098272670725,\n",
        "    'bagging_fraction': 0.8852821315081366,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 3.486399193949678,\n",
        "    'lambda_l2': 3.7053586457221366\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"BrakeBalance_lgbm_model_R5.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "aMDQ9A1BDjON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq. optimizing Car-Parameters"
      ],
      "metadata": {
        "id": "X9E3LzqPDxfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "# Ordered list of car parameters to optimize\n",
        "params_to_optimize = [\n",
        "    \"Engine\", \"Differential\", \"RearWing\",\n",
        "    \"FrontWing\", \"Suspension\", \"BrakeBalance\"\n",
        "]\n",
        "\n",
        "# Feature sets used by each model for prediction\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load static base data (track & weather)\n",
        "base_data = pd.read_csv(\"track_weather_USA.csv\")\n",
        "\n",
        "# Dictionary to store optimized values\n",
        "optimized_params = {}\n",
        "\n",
        "# Sequential optimization loop\n",
        "for car_part in params_to_optimize:\n",
        "    print(f\"\\n🔧 Optimizing {car_part}...\")\n",
        "\n",
        "    # Load the pretrained LightGBM model from folder\n",
        "    folder_name = 'SeqOptmModels'\n",
        "    model_file_name = f\"{car_part}_lgbm_model_R5.txt\"\n",
        "    model_file_path = os.path.join(folder_name, model_file_name)\n",
        "    model = lgb.Booster(model_file=model_file_path)\n",
        "    features = feature_sets[car_part]\n",
        "\n",
        "    def objective(trial):\n",
        "        # Clone base data for this trial\n",
        "        input_data = base_data.copy()\n",
        "\n",
        "        # Add fixed parameters (already optimized)\n",
        "        for param, value in optimized_params.items():\n",
        "            input_data[param] = value\n",
        "\n",
        "        # Suggest value only for the current parameter\n",
        "        trial_value = trial.suggest_int(car_part, 1, 500)\n",
        "        input_data[car_part] = trial_value\n",
        "\n",
        "        # Fill in missing feature columns with default (0)\n",
        "        for feature in features:\n",
        "            if feature not in input_data.columns:\n",
        "                input_data[feature] = 0\n",
        "\n",
        "        # Predict average speed\n",
        "        X = input_data[features]\n",
        "        avg_speed = model.predict(X)[0]\n",
        "        return avg_speed\n",
        "\n",
        "    # Run Optuna optimization for this parameter\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=500, show_progress_bar=True)\n",
        "\n",
        "    # Save best value\n",
        "    best_value = study.best_params[car_part]\n",
        "    optimized_params[car_part] = best_value\n",
        "    print(f\"✅ Best {car_part}: {best_value}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n🎯 Final Optimized Parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "ewDYTtVwD9AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Direct Optimization using Selenium"
      ],
      "metadata": {
        "id": "r4qZneHcCFyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code does not run in Google Colab, it must be run in a local instance of python."
      ],
      "metadata": {
        "id": "68786QDgEEa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import optuna\n",
        "\n",
        "USERNAME = 'xxx@studium.uni-hamburg.de'\n",
        "PASSWORD = 'xxx'\n",
        "STINT_LENGTH = 1\n",
        "FUEL_LOAD = 10\n",
        "COUNTER = 25 #set to last xpath tr value +1 or last stint +2\n",
        "N_TRAILS = 15\n",
        "\n",
        "# Function to set up the browser\n",
        "def setup_browser():\n",
        "\n",
        "    chrome_options = Options()\n",
        "\n",
        "    # Open the browser\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    return driver\n",
        "\n",
        "\n",
        "# Function to login and navigate to the page with the form\n",
        "def login_and_navigate(driver, username, password):\n",
        "    # Open the login page\n",
        "    driver.get(\"https://team-analytics.com/f1/\")  # Replace with your login URL\n",
        "\n",
        "    # Fill in the login credentials\n",
        "    driver.find_element(By.NAME, 'email').send_keys(username)  # Replace 'username' with actual element ID\n",
        "    driver.find_element(By.NAME, 'password').send_keys(password)  # Replace 'password' with actual element ID\n",
        "\n",
        "    # Click the login button\n",
        "    driver.find_element(By.NAME, 'login_user_btn').click()  # Replace 'login-button' with the correct button ID\n",
        "\n",
        "    # Wait for the login to complete (adjust sleep time as necessary)\n",
        "    time.sleep(3)\n",
        "\n",
        "     #Click the button that takes you to the next page\n",
        "    next_page_button = driver.find_element(By.NAME, 'practice_round')  # Adjust the ID of the button\n",
        "    next_page_button.click()\n",
        "\n",
        "    # Wait for the page to load (adjust as necessary)\n",
        "    time.sleep(3)\n",
        "\n",
        "def parse_time_string(time_str):\n",
        "    \"\"\"Parses a time string in the format 'MM:SS:MS' into total seconds.\"\"\"\n",
        "    minutes, seconds, milliseconds = map(int, time_str.split(':'))\n",
        "    total_seconds = minutes * 60 + seconds + milliseconds / 1000.0\n",
        "    return total_seconds\n",
        "\n",
        "# Function to fill in the form and get the result (reuse the open driver session)\n",
        "def get_avg_time(driver, params):\n",
        "\n",
        "\n",
        "    # Locate and fill the input fields (replace 'paramX' with actual field IDs)\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"rearwing\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[0]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"engine\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[1]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"frontwing\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[2]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"brake\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[3]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"differential\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[4]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"suspension\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[5]))\n",
        "\n",
        "    stintlenght = 3\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"stint_length\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(STINT_LENGTH))\n",
        "\n",
        "    fuelload = 12\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"fuel_load\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(FUEL_LOAD))\n",
        "\n",
        "\n",
        "\n",
        "    # Submit the form\n",
        "    driver.find_element(By.NAME, 'submit_practice_stint').click()\n",
        "\n",
        "    # Wait for the result to load\n",
        "    time.sleep(3)\n",
        "\n",
        "    if not hasattr(get_avg_time, \"counter\"):\n",
        "        get_avg_time.counter = COUNTER #set to last xpath tr value +1 / last stint +2\n",
        "\n",
        "    # Extract the avg. time result\n",
        "    xpath = f'//*[@id=\"submit_practice\"]/table[2]/tbody/tr[{get_avg_time.counter}]/td[12]'\n",
        "    time_str_element = driver.find_element(By.XPATH, xpath)\n",
        "    time_str = time_str_element.text\n",
        "    avg_time = parse_time_string(time_str)\n",
        "\n",
        "    # Increment the counter\n",
        "    get_avg_time.counter += 1\n",
        "\n",
        "    return avg_time\n",
        "\n",
        "\n",
        "# Function to close the browser\n",
        "def close_browser(driver):\n",
        "    driver.quit()\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial, driver):\n",
        "    # Sample values for the 6 parameters\n",
        "    param1 = trial.suggest_int('param1', low=350, high=500) #RearWing\n",
        "    param2 = trial.suggest_int('param2', low=20, high=150)    #Engine\n",
        "    param3 = trial.suggest_int('param3', low=300, high=450)  #FrontWing\n",
        "    param4 = trial.suggest_int('param4', low=100, high=250)   #Brake\n",
        "    param5 = trial.suggest_int('param5', low=35, high=250)  #Differential\n",
        "    param6 = trial.suggest_int('param6', low=50, high=200)  #Suspension\n",
        "\n",
        "    # Bundle the parameters into a list\n",
        "    params = [param1, param2, param3, param4, param5, param6]\n",
        "\n",
        "    # Get the avg time from the website (reuse the same driver)\n",
        "    avg_time = get_avg_time(driver, params)\n",
        "\n",
        "    return avg_time\n",
        "\n",
        "# Set up the browser and login\n",
        "driver = setup_browser()\n",
        "login_and_navigate(driver, USERNAME, PASSWORD)\n",
        "\n",
        "# Create an Optuna study to optimize the objective function\n",
        "storage = \"sqlite:///DirectOptStudyOneR5.db\"\n",
        "study = optuna.create_study(direction='minimize', study_name=\"DirectOptStudyOneR5\", storage=storage, load_if_exists=True)\n",
        "study.optimize(lambda trial: objective(trial, driver), n_trials=N_TRAILS)\n",
        "\n",
        "# Print the best parameters and corresponding result\n",
        "print(\"Best parameters found:\", study.best_params)\n",
        "print(\"Best avg. time:\", study.best_value)\n",
        "\n",
        "# Close the browser after optimization\n",
        "close_browser(driver)\n",
        "\n"
      ],
      "metadata": {
        "id": "sYdicbXACRuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Race Strategy\n"
      ],
      "metadata": {
        "id": "PzjkaHdzJdtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the tire parameters and their lap time formulas\n",
        "def lap_time_super_soft(X):\n",
        "    return (92.3006197773077) + 0.282958307692307 * X\n",
        "\n",
        "def lap_time_soft(X):\n",
        "    return (93.4651746647101) + 0.246570086956521 * X\n",
        "\n",
        "def lap_time_medium(X):\n",
        "    return (94.2806098566049) + 0.211755061728395 * X\n",
        "\n",
        "def lap_time_hard(X):\n",
        "    return (93.4813606710215) + 0.259557913978495 * X\n",
        "\n",
        "# Tire data with their lifespan\n",
        "tire_lifespan = {\n",
        "    \"super_soft\": 13,\n",
        "    \"soft\": 23,\n",
        "    \"medium\": 27,\n",
        "    \"hard\": 31\n",
        "}\n",
        "\n",
        "# Pit stop penalty\n",
        "pit_stop_time = 30  # seconds\n",
        "\n",
        "# Function to calculate the total race time for a given strategy\n",
        "def calculate_race_time(laps, strategy):\n",
        "    total_time = 0\n",
        "    total_pit_stops = 0\n",
        "    lap_index = 0\n",
        "    lap_counter = 0\n",
        "\n",
        "    while lap_counter < laps:\n",
        "        tire, stint_laps = strategy[lap_index]\n",
        "\n",
        "        # Ensure we don't exceed the total laps\n",
        "        if lap_counter + stint_laps > laps:\n",
        "            stint_laps = laps - lap_counter\n",
        "\n",
        "        # Calculate the lap times for this stint\n",
        "        lap_times = []\n",
        "        for i in range(stint_laps):\n",
        "            if tire == \"super_soft\":\n",
        "                lap_times.append(lap_time_super_soft(i + 1))\n",
        "            elif tire == \"soft\":\n",
        "                lap_times.append(lap_time_soft(i + 1))\n",
        "            elif tire == \"medium\":\n",
        "                lap_times.append(lap_time_medium(i + 1))\n",
        "            elif tire == \"hard\":\n",
        "                lap_times.append(lap_time_hard(i + 1))\n",
        "\n",
        "        total_time += sum(lap_times)  # Add the lap times of this stint\n",
        "        lap_counter += stint_laps\n",
        "\n",
        "        # If we are not at the last stint, account for a pit stop\n",
        "        if lap_counter < laps:\n",
        "            total_time += pit_stop_time  # Pit stop penalty\n",
        "            total_pit_stops += 1\n",
        "\n",
        "        lap_index += 1\n",
        "        if lap_index >= len(strategy):\n",
        "            break\n",
        "\n",
        "    return total_time, total_pit_stops\n",
        "\n",
        "# Function to generate possible strategies dynamically\n",
        "def generate_strategies(laps):\n",
        "    strategies = []\n",
        "    tire_choices = [\"super_soft\", \"soft\", \"medium\", \"hard\"]\n",
        "\n",
        "    # Generate strategies by breaking the laps into multiple stints\n",
        "    for tire1 in tire_choices:\n",
        "        for tire2 in tire_choices:\n",
        "            for tire3 in tire_choices:\n",
        "                for tire4 in tire_choices:\n",
        "                  for tire5 in tire_choices:\n",
        "                    strategy = []\n",
        "                    remaining_laps = laps\n",
        "\n",
        "                    # Create dynamic stints for each tire\n",
        "                    for tire in [tire1, tire2, tire3, tire4, tire5]:\n",
        "                    #for tire in [tire1, tire2, tire3, tire4]:\n",
        "                    #for tire in [tire1, tire2, tire3]:\n",
        "                    #for tire in [tire1, tire2]:\n",
        "                        stint_laps = tire_lifespan[tire]\n",
        "\n",
        "                        if remaining_laps > stint_laps:\n",
        "                            strategy.append((tire, stint_laps))\n",
        "                            remaining_laps -= stint_laps\n",
        "                        else:\n",
        "                            strategy.append((tire, remaining_laps))\n",
        "                            break\n",
        "\n",
        "                    if sum([stint[1] for stint in strategy]) == laps:\n",
        "                        strategies.append(strategy)\n",
        "\n",
        "    return strategies\n",
        "\n",
        "# Function to find the best strategy\n",
        "def optimize_strategy(laps):\n",
        "    best_time = math.inf\n",
        "    best_strategy = None\n",
        "\n",
        "    strategies = generate_strategies(laps)\n",
        "\n",
        "    for strategy in strategies:\n",
        "        total_time, pit_stops = calculate_race_time(laps, strategy)\n",
        "        if total_time < best_time:\n",
        "            best_time = total_time\n",
        "            best_strategy = strategy\n",
        "            best_pit_stops = pit_stops\n",
        "\n",
        "    return best_strategy, best_time, best_pit_stops\n",
        "\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    race_laps = 63\n",
        "    best_strategy, best_time, total_pit_stops = optimize_strategy(race_laps)\n",
        "    print(f\"Best Strategy: {best_strategy}\")\n",
        "    print(f\"Best Total Time: {best_time} seconds\")\n",
        "    print(f\"Total Pit Stops: {total_pit_stops}\")"
      ],
      "metadata": {
        "id": "LXa79b9EJhyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFvVYCufQL5T"
      },
      "source": [
        "# **Debrief Race Calendar before Final Race**\n",
        "\n",
        "*Write a longer text (200-500 words) reflecting on what were the main ideas you started the seminar with, how you improved your models to achieve better performance and what strategy and analytics you want to use for your final race during seminar day*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1t67p2kQL5T"
      },
      "source": [
        "# **Analytics for Final Race**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this weeks Race we plan on fine tuning what we did for Race 5.\n",
        "Therefore we want to:\n",
        "1. weigh and combine the Datasets\n",
        "2. Train LGBM Models for sequential optimization\n",
        "3. Run sequential optimization and validate Parameters using 1 Practice Lap\n",
        "4. Set up the direct optimization with correct parameter ranges and run for 15 trails, using fuel=10 and stint_lenght=1\n",
        "5. Include new Data into the Training data\n",
        "Repeat Steps 1-4 four times.\n",
        "This leaves 16 Laps for strategy optimization, which we will do afterwords.\n",
        "\n",
        "The weiging part can be split up into three parts:\n",
        "- 1.1 being the one where we figure out what weights to use with our old (1.1.1 and new practice_data (1.1.2) practice data within part 1.2 and 1.3\n",
        "- 1.2 is where we weigh the simulator data and combine with the old practice data\n",
        "- 1.3 is where we weigh the simulator data and combine with the old practice_data and the new  practice_data\n",
        "\n",
        "\n",
        "After using our regular 80 Practice Laps we plan on doing a run of 225 Trails using our direct optimizer without any ML Models inbetween with the hope that this might lead to even better car-parameters. This leaves 25 Laps to validate possible results. As the counter stops once it hits -251 Laps."
      ],
      "metadata": {
        "id": "iZHWuS5JoFcY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4hVvuL2wG1j"
      },
      "outputs": [],
      "source": [
        "# Team members working on this code: Paula Kussauer, Cedric Schwandt, Hannes Kock"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Weigh and combine the Datasets"
      ],
      "metadata": {
        "id": "mg0U_usJrrGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1.1"
      ],
      "metadata": {
        "id": "RH37hQ_V1F_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.1.1"
      ],
      "metadata": {
        "id": "LPNFqqWF-_fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"simulator_data_newnames.csv\")\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"optm_weights_lgbm_model_R6.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'optm_weights_lgbm_model_R6.txt'\")\n"
      ],
      "metadata": {
        "id": "2Wt4DChgrxoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import FALSE\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the original model\n",
        "model = lgb.Booster(model_file=\"optm_weights_lgbm_model_R6.txt\")\n",
        "\n",
        "# Load the new data\n",
        "new_df = pd.read_csv(\"practice_data_newnamesV6_1.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "new_df[\"Avg Speed\"] = (new_df[\"Lap Distance\"] / new_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Prepare features and target\n",
        "X_new = new_df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\", \"Round\", \"Track\", \"Qualifying\",\n",
        "                             \"Stint\", \"Lap\", \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"])\n",
        "y_new = new_df[\"Avg Speed\"]\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest a scaling factor for sample weights\n",
        "    weight_scaling_factor = trial.suggest_int('weight_scaling_factor', 1, 10, log = True )\n",
        "    sample_weight = [weight_scaling_factor] * len(y_new)\n",
        "\n",
        "    # Use k-fold cross-validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse_list = []\n",
        "\n",
        "    # Train and evaluate using cross-validation\n",
        "    for train_index, valid_index in kf.split(X_new):\n",
        "        X_train, X_valid = X_new.iloc[train_index], X_new.iloc[valid_index]\n",
        "        y_train, y_valid = y_new.iloc[train_index], y_new.iloc[valid_index]\n",
        "        train_weight = np.array(sample_weight)[train_index]\n",
        "\n",
        "        train_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n",
        "        valid_data = lgb.Dataset(X_valid, label=y_valid, weight=np.array(sample_weight)[valid_index], reference=train_data)\n",
        "\n",
        "        # Train the model\n",
        "        model_tmp = model\n",
        "        model_tmp = lgb.train(\n",
        "            params={},  # Use default params unless specified\n",
        "            train_set=train_data,\n",
        "            init_model=model,\n",
        "            num_boost_round=1000,  # You can increase if needed\n",
        "        )\n",
        "\n",
        "        # Predict and calculate RMSE for this fold\n",
        "        y_pred = model_tmp.predict(X_valid, num_iteration=model_tmp.best_iteration)\n",
        "        #rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "        rmse = np.sqrt(((y_valid - y_pred)**2).mean())\n",
        "        rmse_list.append(rmse)\n",
        "\n",
        "    # Return the average RMSE over all folds\n",
        "    mean_rmse = np.mean(rmse_list)\n",
        "    return mean_rmse\n",
        "\n",
        "# Create Optuna study to minimize RMSE\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=30)  # You can adjust the number of trials\n",
        "\n",
        "best_weight_factor = study.best_params['weight_scaling_factor']\n",
        "print(f\"Best weight scaling factor: {best_weight_factor}\")\n"
      ],
      "metadata": {
        "id": "YZICbrsXrxoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Best weight scaling factor: 9"
      ],
      "metadata": {
        "id": "fq6IqsYut9Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.1.2"
      ],
      "metadata": {
        "id": "lW5iLDh-_EiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To figure out what weight to use for the new Practice_Data, we first merge the Simulator_Data and old Practice_Data together with the weights that we determined in 1.1.1 and then just follow the known weights optimization procedure with the modification of weights."
      ],
      "metadata": {
        "id": "hjG7Kebj_Iw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read in both CSV files\n",
        "simulator_df = pd.read_csv('simulator_data_newnames.csv')\n",
        "practice_df = pd.read_csv('practice_data_newnamesV6_1.csv')\n",
        "\n",
        "# Define your weights\n",
        "SIMULATOR_WEIGHT = 1\n",
        "PRACTICE_WEIGHT = 9\n",
        "\n",
        "# Assign weights as a new column\n",
        "simulator_df['dataset_weight'] = SIMULATOR_WEIGHT\n",
        "practice_df['dataset_weight'] = PRACTICE_WEIGHT\n",
        "\n",
        "# Keep only columns of simulator_df in practice_df (excluding the new 'dataset_weight' column)\n",
        "cols_to_match = simulator_df.columns.intersection(practice_df.columns)\n",
        "practice_df_matched = practice_df[cols_to_match]\n",
        "\n",
        "# Reindex columns to match simulator_df order (excluding 'dataset_weight' in reindex)\n",
        "practice_df_matched = practice_df_matched.reindex(columns=simulator_df.columns.difference(['dataset_weight']))\n",
        "\n",
        "# Concatenate with 'dataset_weight' column included\n",
        "merged_df = pd.concat([\n",
        "    simulator_df,\n",
        "    practice_df_matched.assign(dataset_weight=PRACTICE_WEIGHT)\n",
        "], ignore_index=True)\n",
        "\n",
        "# Write the merged DataFrame to a new CSV file\n",
        "merged_df.to_csv('merged_data_R6.csv', index=False)\n",
        "\n",
        "print(\"Merge completed, saved as 'merged_data_R6.csv'\")"
      ],
      "metadata": {
        "id": "Eu4ODK2Z_tRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"merged_data_R6.csv\")  # Use merged file\n",
        "\n",
        "# Compute Avg Speed\n",
        "df[\"Avg Speed\"] = (df[\"Lap Distance\"] / df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\"])\n",
        "y = df[\"Avg Speed\"]\n",
        "\n",
        "# Use weights from the merged file\n",
        "weights = df[\"dataset_weight\"]\n",
        "\n",
        "# Remove 'dataset_weight' from features (as it's not a feature but a weight)\n",
        "X = X.drop(columns=[\"dataset_weight\"])\n",
        "\n",
        "# Define LightGBM dataset using weights\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 242,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.0645474493988489,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.9427185636727835,\n",
        "    'bagging_fraction': 0.785711983453578,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.6006835353397709,\n",
        "    'lambda_l2': 1.4905819957883624,\n",
        "}\n",
        "\n",
        "# Train on full dataset using weights\n",
        "model = lgb.train(params, lgb_data, num_boost_round=1000)\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_model(\"optm_weights_lgbm_model_R6_merged.txt\")\n",
        "\n",
        "print(\"Model training complete and saved to 'optm_weights_lgbm_model_R6_merged.txt'\")"
      ],
      "metadata": {
        "id": "50kzAFLeALWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import FALSE\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the original model\n",
        "model = lgb.Booster(model_file=\"optm_weights_lgbm_model_R6_merged.txt\")\n",
        "\n",
        "# Load the new data\n",
        "new_df = pd.read_csv(\"practice_data_newnamesV6_2.csv\")\n",
        "\n",
        "# Compute Avg Speed for new data\n",
        "new_df[\"Avg Speed\"] = (new_df[\"Lap Distance\"] / new_df[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Prepare features and target\n",
        "X_new = new_df.drop(columns=[\"Avg Speed\", \"Lap Distance\", \"Lap Time\", \"Round\", \"Track\", \"Qualifying\",\n",
        "                             \"Stint\", \"Lap\", \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"])\n",
        "y_new = new_df[\"Avg Speed\"]\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest a scaling factor for sample weights\n",
        "    weight_scaling_factor = trial.suggest_int('weight_scaling_factor', 1, 100, log = True )\n",
        "    sample_weight = [weight_scaling_factor] * len(y_new)\n",
        "\n",
        "    # Use k-fold cross-validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse_list = []\n",
        "\n",
        "    # Train and evaluate using cross-validation\n",
        "    for train_index, valid_index in kf.split(X_new):\n",
        "        X_train, X_valid = X_new.iloc[train_index], X_new.iloc[valid_index]\n",
        "        y_train, y_valid = y_new.iloc[train_index], y_new.iloc[valid_index]\n",
        "        train_weight = np.array(sample_weight)[train_index]\n",
        "\n",
        "        train_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n",
        "        valid_data = lgb.Dataset(X_valid, label=y_valid, weight=np.array(sample_weight)[valid_index], reference=train_data)\n",
        "\n",
        "        # Train the model\n",
        "        model_tmp = model\n",
        "        model_tmp = lgb.train(\n",
        "            params={},  # Use default params unless specified\n",
        "            train_set=train_data,\n",
        "            init_model=model,\n",
        "            num_boost_round=1000,  # You can increase if needed\n",
        "        )\n",
        "\n",
        "        # Predict and calculate RMSE for this fold\n",
        "        y_pred = model_tmp.predict(X_valid, num_iteration=model_tmp.best_iteration)\n",
        "        #rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "        rmse = np.sqrt(((y_valid - y_pred)**2).mean())\n",
        "        rmse_list.append(rmse)\n",
        "\n",
        "    # Return the average RMSE over all folds\n",
        "    mean_rmse = np.mean(rmse_list)\n",
        "    return mean_rmse\n",
        "\n",
        "# Create Optuna study to minimize RMSE\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=30)  # You can adjust the number of trials\n",
        "\n",
        "best_weight_factor = study.best_params['weight_scaling_factor']\n",
        "print(f\"Best weight scaling factor: {best_weight_factor}\")\n"
      ],
      "metadata": {
        "id": "nGaUu0AoAT8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1.2"
      ],
      "metadata": {
        "id": "QCKFs5rB1MMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "\n",
        "# Adjust file paths to your current setup\n",
        "\n",
        "SIM_CSV         = \"simulator_data_newnames.csv\"     # 10,000 simulated laps\n",
        "PRACTICEOLD_CSV = \"practice_data_newnamesV6_1.csv\"\n",
        "ENV_CSV         = \"track_weather_italy.csv\"           # 1 row: track & weather conditions\n",
        "\n",
        "\n",
        "# Load all relevant data\n",
        "\n",
        "df_sim = pd.read_csv(SIM_CSV)\n",
        "df_pro = pd.read_csv(PRACTICEOLD_CSV)\n",
        "env_df = pd.read_csv(ENV_CSV)  # track/weather parameters (single row)\n",
        "\n",
        "\n",
        "# Automatically infer meta feature columns\n",
        "\n",
        "# 1. Collect all car setup parameters relevant for optimization\n",
        "car_param_cols = {\n",
        "    \"Engine\", \"Differential\", \"RearWing\", \"FrontWing\",\n",
        "    \"Suspension\", \"BrakeBalance\"\n",
        "}\n",
        "\n",
        "# 2. All columns in the ENV file are considered track/weather features\n",
        "env_cols = env_df.columns.tolist()\n",
        "\n",
        "# 3. Only keep those features that are present in both ENV and Sim/Practice datasets (for consistency)\n",
        "meta_cols = [c for c in env_cols if c in df_sim.columns]\n",
        "\n",
        "\n",
        "# D) Compute RBF kernel weights for simulator data\n",
        "tau = 2.0  # Bandwidth for kernel function\n",
        "X_sim_meta = df_sim[meta_cols].values\n",
        "\n",
        "# Use the specific target track/weather conditions from ENV_CSV\n",
        "env_point = env_df[meta_cols].values.flatten()\n",
        "\n",
        "# Calculate covariance matrix (using simulator data for variance and correlation structure)\n",
        "Sigma_inv = np.linalg.inv(np.cov(X_sim_meta, rowvar=False))\n",
        "\n",
        "# Calculate RBF kernel weights for each simulator sample (measuring similarity to actual track/weather)\n",
        "k_sim = np.exp([\n",
        "    -mahalanobis(x, env_point, Sigma_inv) / tau\n",
        "    for x in X_sim_meta\n",
        "])\n",
        "\n",
        "# Assign constant weights for old and new practice laps (as previously defined)\n",
        "w_pro = np.full(len(df_pro), 9)  # All old practice laps\n",
        "\n",
        "# Combine all weights into a single array for further use\n",
        "weights = np.concatenate([k_sim, w_pro]) #without new practice data data\n",
        "\n",
        "\n",
        "\n",
        "# Create a unified DataFrame for downstream modeling\n",
        "# Combine simulator, old practice, and new practice datasets\n",
        "df_all = pd.concat([df_sim, df_pro], ignore_index=True) #without new practice data data\n",
        "\n",
        "\n",
        "\n",
        "# Add a column for average speed in km/h (distance in km divided by time in hours)\n",
        "df_all[\"Avg Speed\"] = (df_all[\"Lap Distance\"] / df_all[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Append the computed weights as a new column named 'sample_weight'\n",
        "df_all[\"sample_weight\"] = weights\n",
        "\n",
        "# Specify columns that should be removed from the final dataset\n",
        "drop_cols = [\n",
        "    \"Lap Distance\", \"Lap Time\", \"Round\", \"Track\", \"Qualifying\", \"Stint\", \"Lap\",\n",
        "    \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"\n",
        "]\n",
        "\n",
        "# Safely drop unwanted columns, ignoring any missing columns just in case\n",
        "df_all_dropped = df_all.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "# Export the cleaned, combined, and weighted dataset to CSV\n",
        "OUTPUT_CSV = \"all_data_with_weights_R6.csv\"\n",
        "df_all_dropped.to_csv(OUTPUT_CSV, index=False)"
      ],
      "metadata": {
        "id": "TJTwfKHAt-pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1.3"
      ],
      "metadata": {
        "id": "mexVZTof1UID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "\n",
        "# Adjust file paths to your current setup\n",
        "\n",
        "SIM_CSV         = \"simulator_data_newnames.csv\"     # 10,000 simulated laps\n",
        "PRACTICEOLD_CSV = \"practice_data_newnamesV6_1.csv\"\n",
        "PRACTICENEW_CSV = \"practice_data_newnamesV6_2.csv\"\n",
        "ENV_CSV         = \"track_weather_italy.csv\"           # 1 row: track & weather conditions\n",
        "\n",
        "\n",
        "# Load all relevant data\n",
        "\n",
        "df_sim = pd.read_csv(SIM_CSV)\n",
        "df_pro = pd.read_csv(PRACTICEOLD_CSV)\n",
        "df_prn = pd.read_csv(PRACTICENEW_CSV)\n",
        "env_df = pd.read_csv(ENV_CSV)  # track/weather parameters (single row)\n",
        "\n",
        "\n",
        "# Automatically infer meta feature columns\n",
        "\n",
        "# 1. Collect all car setup parameters relevant for optimization\n",
        "car_param_cols = {\n",
        "    \"Engine\", \"Differential\", \"RearWing\", \"FrontWing\",\n",
        "    \"Suspension\", \"BrakeBalance\"\n",
        "}\n",
        "\n",
        "# 2. All columns in the ENV file are considered track/weather features\n",
        "env_cols = env_df.columns.tolist()\n",
        "\n",
        "# 3. Only keep those features that are present in both ENV and Sim/Practice datasets (for consistency)\n",
        "meta_cols = [c for c in env_cols if c in df_sim.columns]\n",
        "\n",
        "\n",
        "# D) Compute RBF kernel weights for simulator data\n",
        "tau = 2.0  # Bandwidth for kernel function\n",
        "X_sim_meta = df_sim[meta_cols].values\n",
        "\n",
        "# Use the specific target track/weather conditions from ENV_CSV\n",
        "env_point = env_df[meta_cols].values.flatten()\n",
        "\n",
        "# Calculate covariance matrix (using simulator data for variance and correlation structure)\n",
        "Sigma_inv = np.linalg.inv(np.cov(X_sim_meta, rowvar=False))\n",
        "\n",
        "# Calculate RBF kernel weights for each simulator sample (measuring similarity to actual track/weather)\n",
        "k_sim = np.exp([\n",
        "    -mahalanobis(x, env_point, Sigma_inv) / tau\n",
        "    for x in X_sim_meta\n",
        "])\n",
        "\n",
        "# Assign constant weights for old and new practice laps (as previously defined)\n",
        "w_prn = np.full(len(df_pro), 9)  # All old practice laps\n",
        "w_prn = np.full(len(df_prn), 15)  # All new practice laps\n",
        "\n",
        "# Combine all weights into a single array for further use\n",
        "weights = np.concatenate([k_sim, w_pro, w_prn]) #with new practice data data\n",
        "\n",
        "\n",
        "# Create a unified DataFrame for downstream modeling\n",
        "# Combine simulator, old practice, and new practice datasets\n",
        "df_all = pd.concat([df_sim, df_pro, df_prn], ignore_index=True) #with new practice data data\n",
        "\n",
        "\n",
        "# Add a column for average speed in km/h (distance in km divided by time in hours)\n",
        "df_all[\"Avg Speed\"] = (df_all[\"Lap Distance\"] / df_all[\"Lap Time\"]) * 3600\n",
        "\n",
        "# Append the computed weights as a new column named 'sample_weight'\n",
        "df_all[\"sample_weight\"] = weights\n",
        "\n",
        "# Specify columns that should be removed from the final dataset\n",
        "drop_cols = [\n",
        "    \"Lap Distance\", \"Lap Time\", \"Round\", \"Track\", \"Qualifying\", \"Stint\", \"Lap\",\n",
        "    \"Fuel\", \"Tyre Remaining\", \"Tyre Choice\"\n",
        "]\n",
        "\n",
        "# Safely drop unwanted columns, ignoring any missing columns just in case\n",
        "df_all_dropped = df_all.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "# Export the cleaned, combined, and weighted dataset to CSV\n",
        "OUTPUT_CSV = \"all_data_with_weights_R6.csv\"\n",
        "df_all_dropped.to_csv(OUTPUT_CSV, index=False)"
      ],
      "metadata": {
        "id": "3YEUyo2GvlF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Train LightGBM Models for sequenial optimization"
      ],
      "metadata": {
        "id": "IQEjTYX5vl_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Engine (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the weighed data\n",
        "df = pd.read_csv(\"all_data_with_weights_R6.csv\")\n",
        "\n",
        "# Features and target\n",
        "X = df[['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure' , 'Cornering']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 228,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.019795034976830574,\n",
        "    'min_data_in_leaf': 45,\n",
        "    'feature_fraction': 0.6769051793180653,\n",
        "    'bagging_fraction': 0.5679615916585826,\n",
        "    'bagging_freq': 5,\n",
        "    'lambda_l1': 0.2140082876917294,\n",
        "    'lambda_l2': 0.778813758195442\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels_R6\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"Engine_lgbm_model_R6.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "008KXL2tv6z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Differential (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights_R6.csv\")\n",
        "\n",
        "# Features and target\n",
        "X = df[['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 95,\n",
        "    'max_depth': 14,\n",
        "    'learning_rate': 0.020430708599380454,\n",
        "    'min_data_in_leaf': 11,\n",
        "    'feature_fraction': 0.8289782961700536,\n",
        "    'bagging_fraction': 0.6054739109884507,\n",
        "    'bagging_freq': 6,\n",
        "    'lambda_l1': 0.164978854742107,\n",
        "    'lambda_l2': 0.09387958914404965\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels_R6\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"Differential_lgbm_model_R6.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "prixY-KOv6z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rear Wing (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights_R6.csv\")\n",
        "\n",
        "\n",
        "# Features and target\n",
        "X = df[['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure' , 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 263,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.033545204540268374,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.67904602486583,\n",
        "    'bagging_fraction': 0.9010766438515055,\n",
        "    'bagging_freq': 3,\n",
        "    'lambda_l1': 0.00361547478377304,\n",
        "    'lambda_l2': 0.00947809994264175\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels_R6\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"RearWing_lgbm_model_R6.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "VMi23AExv6z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Front Wing (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights_R6.csv\")\n",
        "\n",
        "\n",
        "# Features and target\n",
        "X = df[['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 192,\n",
        "    'max_depth': 12,\n",
        "    'learning_rate': 0.062393011943707284,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.8418876970992247,\n",
        "    'bagging_fraction': 0.8723398679038107,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 0.3441273621643392,\n",
        "    'lambda_l2': 0.5476995416360311\n",
        "    }\n",
        "\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels_R6\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"FrontWing_lgbm_model_R6.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "hxnS8Ao0v6z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Suspension (Hyperparameters and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights_R6.csv\")\n",
        "\n",
        "\n",
        "# Features and target\n",
        "X = df[['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 191,\n",
        "    'max_depth': 14,\n",
        "    'learning_rate': 0.024681809936626296,\n",
        "    'min_data_in_leaf': 10,\n",
        "    'feature_fraction': 0.8101056533251277,\n",
        "    'bagging_fraction': 0.5448991038428275,\n",
        "    'bagging_freq': 2,\n",
        "    'lambda_l1': 0.09124351826015425,\n",
        "    'lambda_l2': 0.0008864299246597318\n",
        "    }\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels_R6\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"Suspension_lgbm_model_R6.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "80fcTsB_v6z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brake Balance ( and X set)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"all_data_with_weights_R6.csv\")\n",
        "\n",
        "\n",
        "# Features and target\n",
        "X = df[['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine','Cornering', 'Width', 'Roughness', 'Temperature']]\n",
        "y = df[\"Avg Speed\"]\n",
        "weights = df[\"sample_weight\"].values\n",
        "\n",
        "# Define LightGBM dataset\n",
        "lgb_data = lgb.Dataset(X, label=y, weight=weights)\n",
        "\n",
        "# Parameters (tuned already)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 27,\n",
        "    'max_depth': 11,\n",
        "    'learning_rate': 0.18696777761444966,\n",
        "    'min_data_in_leaf': 73,\n",
        "    'feature_fraction': 0.964098272670725,\n",
        "    'bagging_fraction': 0.8852821315081366,\n",
        "    'bagging_freq': 10,\n",
        "    'lambda_l1': 3.486399193949678,\n",
        "    'lambda_l2': 3.7053586457221366\n",
        "}\n",
        "\n",
        "# Train on full dataset\n",
        "model = lgb.train(params, lgb_data, num_boost_round=100)\n",
        "\n",
        "# Save the model to a file\n",
        "folder = \"SeqOptmModels_R6\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(folder, \"BrakeBalance_lgbm_model_R6.txt\")\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(f\"Model training complete and saved to '{model_path}'\")\n"
      ],
      "metadata": {
        "id": "eUoYZBQ4v6z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Run sequential optimization"
      ],
      "metadata": {
        "id": "iSlUVXrgxjI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "# Ordered list of car parameters to optimize\n",
        "params_to_optimize = [\n",
        "    \"Engine\", \"Differential\", \"RearWing\",\n",
        "    \"FrontWing\", \"Suspension\", \"BrakeBalance\"\n",
        "]\n",
        "\n",
        "# Feature sets used by each model for prediction\n",
        "feature_sets = {\n",
        "    \"Engine\": ['Engine', 'Grip', 'Humidity', 'Air Density', 'Altitude', 'Temperature', 'Inclines', 'Air Pressure', 'Cornering'],\n",
        "    \"Differential\": ['Differential', 'Engine', 'Cornering', 'Width', 'Inclines', 'Grip', 'Temperature', 'Air Density'],\n",
        "    \"RearWing\": ['RearWing', 'Differential', 'Engine', 'Air Density', 'Cornering', 'Air Pressure', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Roughness'],\n",
        "    \"FrontWing\": ['FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Air Pressure', 'Air Density', 'Inclines', 'Wind (Avg. Speed)', 'Humidity', 'Wind (Gusts)'],\n",
        "    \"Suspension\": ['Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Grip', 'Inclines', 'Cornering', 'Camber', 'Roughness', 'Width'],\n",
        "    \"BrakeBalance\": ['BrakeBalance', 'Suspension', 'FrontWing', 'RearWing', 'Differential', 'Engine', 'Cornering', 'Width', 'Roughness', 'Temperature']\n",
        "}\n",
        "\n",
        "# Load static base data (track & weather)\n",
        "base_data = pd.read_csv(\"track_weather_italy.csv\")\n",
        "\n",
        "# Dictionary to store optimized values\n",
        "optimized_params = {}\n",
        "\n",
        "# Sequential optimization loop\n",
        "for car_part in params_to_optimize:\n",
        "    print(f\"\\n Optimizing {car_part}...\")\n",
        "\n",
        "    # Load the pretrained LightGBM model from folder\n",
        "    folder_name = 'SeqOptmModels_R6'\n",
        "    model_file_name = f\"{car_part}_lgbm_model_R6.txt\"\n",
        "    model_file_path = os.path.join(folder_name, model_file_name)\n",
        "    model = lgb.Booster(model_file=model_file_path)\n",
        "    features = feature_sets[car_part]\n",
        "\n",
        "    def objective(trial):\n",
        "        # Clone base data for this trial\n",
        "        input_data = base_data.copy()\n",
        "\n",
        "        # Add fixed parameters (already optimized)\n",
        "        for param, value in optimized_params.items():\n",
        "            input_data[param] = value\n",
        "\n",
        "        # Suggest value only for the current parameter\n",
        "        trial_value = trial.suggest_int(car_part, 1, 500)\n",
        "        input_data[car_part] = trial_value\n",
        "\n",
        "        # Fill in missing feature columns with default (0)\n",
        "        for feature in features:\n",
        "            if feature not in input_data.columns:\n",
        "                input_data[feature] = 0\n",
        "\n",
        "        # Predict average speed\n",
        "        X = input_data[features]\n",
        "        avg_speed = model.predict(X)[0]\n",
        "        return avg_speed\n",
        "\n",
        "    # Run Optuna optimization for this parameter\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=500, show_progress_bar=True)\n",
        "\n",
        "    # Save best value\n",
        "    best_value = study.best_params[car_part]\n",
        "    optimized_params[car_part] = best_value\n",
        "    print(f\" Best {car_part}: {best_value}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n Final Optimized Parameters:\")\n",
        "for k, v in optimized_params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "GxbiigzUxgAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ranges for direct Direct optimization run 1:\n",
        "\n",
        "Engine: 18 - 38 (+-10)\n",
        "Differential: 21 - 51 (+-15)\n",
        "RearWing: 325 - 365 (+-20)\n",
        "FrontWing: 55 - 105 (+-25)\n",
        "Suspension: 106 - 166 (+-30)\n",
        "BrakeBalance: 40 - 110 (+-35)"
      ],
      "metadata": {
        "id": "OZvCW8EfEZRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ranges for direct Direct optimization run 2:\n",
        "\n",
        "Engine: 18 - 38\n",
        "Differential: 28 - 58\n",
        "RearWing: 325 - 365\n",
        "FrontWing: 55 - 105\n",
        "Suspension: 106 - 166\n",
        "BrakeBalance: 40 - 110"
      ],
      "metadata": {
        "id": "N5f39w05lohs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Direct Optimization using Selenium"
      ],
      "metadata": {
        "id": "1dnvL0d3Dyw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code does not run in Google Colab, it must be run in a local instance of python."
      ],
      "metadata": {
        "id": "cq9JohspD4LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import optuna\n",
        "\n",
        "USERNAME = 'xxx@studium.uni-hamburg.de'\n",
        "PASSWORD = 'xxx'\n",
        "STINT_LENGTH = 1\n",
        "FUEL_LOAD = 10\n",
        "COUNTER = 3 #set to last xpath tr value +1 or last stint +2\n",
        "N_TRAILS = 15\n",
        "\n",
        "# Function to set up the browser\n",
        "def setup_browser():\n",
        "\n",
        "    chrome_options = Options()\n",
        "\n",
        "    # Open the browser\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    return driver\n",
        "\n",
        "\n",
        "# Function to login and navigate to the page with the form\n",
        "def login_and_navigate(driver, username, password):\n",
        "    # Open the login page\n",
        "    driver.get(\"https://team-analytics.com/f1/\")  # Replace with your login URL\n",
        "\n",
        "    # Fill in the login credentials\n",
        "    driver.find_element(By.NAME, 'email').send_keys(username)  # Replace 'username' with actual element ID\n",
        "    driver.find_element(By.NAME, 'password').send_keys(password)  # Replace 'password' with actual element ID\n",
        "\n",
        "    # Click the login button\n",
        "    driver.find_element(By.NAME, 'login_user_btn').click()  # Replace 'login-button' with the correct button ID\n",
        "\n",
        "    # Wait for the login to complete (adjust sleep time as necessary)\n",
        "    time.sleep(3)\n",
        "\n",
        "     #Click the button that takes you to the next page\n",
        "    next_page_button = driver.find_element(By.NAME, 'practice_round')  # Adjust the ID of the button\n",
        "    next_page_button.click()\n",
        "\n",
        "    # Wait for the page to load (adjust as necessary)\n",
        "    time.sleep(3)\n",
        "\n",
        "def parse_time_string(time_str):\n",
        "    \"\"\"Parses a time string in the format 'MM:SS:MS' into total seconds.\"\"\"\n",
        "    minutes, seconds, milliseconds = map(int, time_str.split(':'))\n",
        "    total_seconds = minutes * 60 + seconds + milliseconds / 1000.0\n",
        "    return total_seconds\n",
        "\n",
        "# Function to fill in the form and get the result (reuse the open driver session)\n",
        "def get_avg_time(driver, params):\n",
        "\n",
        "\n",
        "    # Locate and fill the input fields (replace 'paramX' with actual field IDs)\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"rearwing\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[0]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"engine\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[1]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"frontwing\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[2]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"brake\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[3]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"differential\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[4]))\n",
        "\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"suspension\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(params[5]))\n",
        "\n",
        "    stintlenght = 3\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"stint_length\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(STINT_LENGTH))\n",
        "\n",
        "    fuelload = 12\n",
        "    driver.execute_script(\"\"\"\n",
        "    const input = document.querySelector('[name=\"fuel_load\"]');\n",
        "    input.value = arguments[0];\n",
        "    input.dispatchEvent(new Event('input', { bubbles: true }));\n",
        "    \"\"\", str(FUEL_LOAD))\n",
        "\n",
        "\n",
        "\n",
        "    # Submit the form\n",
        "    driver.find_element(By.NAME, 'submit_practice_stint').click()\n",
        "\n",
        "    # Wait for the result to load\n",
        "    time.sleep(3)\n",
        "\n",
        "    if not hasattr(get_avg_time, \"counter\"):\n",
        "        get_avg_time.counter = COUNTER #set to last xpath tr value +1 / last stint +2\n",
        "\n",
        "    # Extract the avg. time result\n",
        "    xpath = f'//*[@id=\"submit_practice\"]/table[2]/tbody/tr[{get_avg_time.counter}]/td[12]'\n",
        "    time_str_element = driver.find_element(By.XPATH, xpath)\n",
        "    time_str = time_str_element.text\n",
        "    avg_time = parse_time_string(time_str)\n",
        "\n",
        "    # Increment the counter\n",
        "    get_avg_time.counter += 1\n",
        "\n",
        "    return avg_time\n",
        "\n",
        "\n",
        "# Function to close the browser\n",
        "def close_browser(driver):\n",
        "    driver.quit()\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial, driver):\n",
        "    # Sample values for the 6 parameters\n",
        "    param1 = trial.suggest_int('param1', low=350, high=500) #RearWing\n",
        "    param2 = trial.suggest_int('param2', low=20, high=150)    #Engine\n",
        "    param3 = trial.suggest_int('param3', low=300, high=450)  #FrontWing\n",
        "    param4 = trial.suggest_int('param4', low=100, high=250)   #Brake\n",
        "    param5 = trial.suggest_int('param5', low=35, high=250)  #Differential\n",
        "    param6 = trial.suggest_int('param6', low=50, high=200)  #Suspension\n",
        "\n",
        "    # Bundle the parameters into a list\n",
        "    params = [param1, param2, param3, param4, param5, param6]\n",
        "\n",
        "    # Get the avg time from the website (reuse the same driver)\n",
        "    avg_time = get_avg_time(driver, params)\n",
        "\n",
        "    return avg_time\n",
        "\n",
        "# Set up the browser and login\n",
        "driver = setup_browser()\n",
        "login_and_navigate(driver, USERNAME, PASSWORD)\n",
        "\n",
        "# Create an Optuna study to optimize the objective function\n",
        "storage = \"sqlite:///DirectOptStudyOneR6.db\"\n",
        "study = optuna.create_study(direction='minimize', study_name=\"DirectOptStudyOneR6\", storage=storage, load_if_exists=True)\n",
        "study.optimize(lambda trial: objective(trial, driver), n_trials=N_TRAILS)\n",
        "\n",
        "# Print the best parameters and corresponding result\n",
        "print(\"Best parameters found:\", study.best_params)\n",
        "print(\"Best avg. time:\", study.best_value)\n",
        "\n",
        "# Close the browser after optimization\n",
        "close_browser(driver)\n",
        "\n"
      ],
      "metadata": {
        "id": "8iQdbBvdD2F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Race Strategy"
      ],
      "metadata": {
        "id": "DovfKmoFiQ7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the tire parameters and their lap time formulas\n",
        "def lap_time_super_soft(X):\n",
        "    return (92.3006197773077) + 0.282958307692307 * X\n",
        "\n",
        "def lap_time_soft(X):\n",
        "    return (93.4651746647101) + 0.246570086956521 * X\n",
        "\n",
        "def lap_time_medium(X):\n",
        "    return (94.2806098566049) + 0.211755061728395 * X\n",
        "\n",
        "def lap_time_hard(X):\n",
        "    return (93.4813606710215) + 0.259557913978495 * X\n",
        "\n",
        "# Tire data with their lifespan\n",
        "tire_lifespan = {\n",
        "    \"super_soft\": 13,\n",
        "    \"soft\": 23,\n",
        "    \"medium\": 27,\n",
        "    \"hard\": 31\n",
        "}\n",
        "\n",
        "# Pit stop penalty\n",
        "pit_stop_time = 30  # seconds\n",
        "\n",
        "# Function to calculate the total race time for a given strategy\n",
        "def calculate_race_time(laps, strategy):\n",
        "    total_time = 0\n",
        "    total_pit_stops = 0\n",
        "    lap_index = 0\n",
        "    lap_counter = 0\n",
        "\n",
        "    while lap_counter < laps:\n",
        "        tire, stint_laps = strategy[lap_index]\n",
        "\n",
        "        # Ensure we don't exceed the total laps\n",
        "        if lap_counter + stint_laps > laps:\n",
        "            stint_laps = laps - lap_counter\n",
        "\n",
        "        # Calculate the lap times for this stint\n",
        "        lap_times = []\n",
        "        for i in range(stint_laps):\n",
        "            if tire == \"super_soft\":\n",
        "                lap_times.append(lap_time_super_soft(i + 1))\n",
        "            elif tire == \"soft\":\n",
        "                lap_times.append(lap_time_soft(i + 1))\n",
        "            elif tire == \"medium\":\n",
        "                lap_times.append(lap_time_medium(i + 1))\n",
        "            elif tire == \"hard\":\n",
        "                lap_times.append(lap_time_hard(i + 1))\n",
        "\n",
        "        total_time += sum(lap_times)  # Add the lap times of this stint\n",
        "        lap_counter += stint_laps\n",
        "\n",
        "        # If we are not at the last stint, account for a pit stop\n",
        "        if lap_counter < laps:\n",
        "            total_time += pit_stop_time  # Pit stop penalty\n",
        "            total_pit_stops += 1\n",
        "\n",
        "        lap_index += 1\n",
        "        if lap_index >= len(strategy):\n",
        "            break\n",
        "\n",
        "    return total_time, total_pit_stops\n",
        "\n",
        "# Function to generate possible strategies dynamically\n",
        "def generate_strategies(laps):\n",
        "    strategies = []\n",
        "    tire_choices = [\"super_soft\", \"soft\", \"medium\", \"hard\"]\n",
        "\n",
        "    # Generate strategies by breaking the laps into multiple stints\n",
        "    for tire1 in tire_choices:\n",
        "        for tire2 in tire_choices:\n",
        "            for tire3 in tire_choices:\n",
        "                for tire4 in tire_choices:\n",
        "                  for tire5 in tire_choices:\n",
        "                    strategy = []\n",
        "                    remaining_laps = laps\n",
        "\n",
        "                    # Create dynamic stints for each tire\n",
        "                    for tire in [tire1, tire2, tire3, tire4, tire5]:\n",
        "                    #for tire in [tire1, tire2, tire3, tire4]:\n",
        "                    #for tire in [tire1, tire2, tire3]:\n",
        "                    #for tire in [tire1, tire2]:\n",
        "                        stint_laps = tire_lifespan[tire]\n",
        "\n",
        "                        if remaining_laps > stint_laps:\n",
        "                            strategy.append((tire, stint_laps))\n",
        "                            remaining_laps -= stint_laps\n",
        "                        else:\n",
        "                            strategy.append((tire, remaining_laps))\n",
        "                            break\n",
        "\n",
        "                    if sum([stint[1] for stint in strategy]) == laps:\n",
        "                        strategies.append(strategy)\n",
        "\n",
        "    return strategies\n",
        "\n",
        "# Function to find the best strategy\n",
        "def optimize_strategy(laps):\n",
        "    best_time = math.inf\n",
        "    best_strategy = None\n",
        "\n",
        "    strategies = generate_strategies(laps)\n",
        "\n",
        "    for strategy in strategies:\n",
        "        total_time, pit_stops = calculate_race_time(laps, strategy)\n",
        "        if total_time < best_time:\n",
        "            best_time = total_time\n",
        "            best_strategy = strategy\n",
        "            best_pit_stops = pit_stops\n",
        "\n",
        "    return best_strategy, best_time, best_pit_stops\n",
        "\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    race_laps = 79\n",
        "    best_strategy, best_time, total_pit_stops = optimize_strategy(race_laps)\n",
        "    print(f\"Best Strategy: {best_strategy}\")\n",
        "    print(f\"Best Total Time: {best_time} seconds\")\n",
        "    print(f\"Total Pit Stops: {total_pit_stops}\")"
      ],
      "metadata": {
        "id": "Cgc4PwJIiWNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kulv8cpKcq9_"
      },
      "source": [
        "## **References**\n",
        "- Cite all references you need according to chair guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OLrMDxWc8oM"
      },
      "source": [
        "Liu, Xuan; Shi, Savannah Wei; Teixeira, Thales; Wedel, Michel (2018): Video Content Marketing: The Making of Clips, Journal of Marketing, Vol. 82, 86-101."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}